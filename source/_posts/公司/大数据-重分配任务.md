---
title: 大数据任务-重分配任务
date: 2022-05-27 18:52:40
categories: [大数据]
tags: [大数据, 公司]
---

## 前言

总结一下公司大数据的任务ETL离线工作流 - 重分配任务

<!-- more -->

公司的ETL服务，目前采用的是组件为：

- 数仓 (hadoop)
- 任务调度器 (azkaban)
- 出仓 (gp)
- 数仓查询 (hue)(hive)(spark)

## job_tk_reassign（重分配任务）

- schedule: `30 10 * * *`

> 按 `天` 为一个周期

天级指标：

- `isready`
- `isready_success`
- `showfailed`

所以在查看报表数据的时候，这3个指标是`天级`的。

除了上面所说的，代码也包含了`source_type = 2`，即实时的数据。但是目前来说，已经没有`source_type=2`的数据了，主要是业务导向。

![重分配任务](/images/公司/tk_reassign.png)

### sync_mysql_unit_and_placement

> 拉取最新的广告位数据

```shell
stat_source='placement'

tmpfilename="tmp/uparpu_mysql_placement.log"

mkdir -p tmp

if [ -f "${tmpfilename}" ]; then

  rm -f ${tmpfilename}
  echo "delete ${tmpfilename}"

fi

mysql -u${DB_USER} -P${DB_PORT} -p${DB_PWD} -h${DB_HOST} -e "
    select 
        id,
        uuid,        
        publisher_id,
        app_id,      
        name,        
        format,      
        remark,      
        create_time, 
        update_time 
    from 
        ${DB_NAME}.${stat_source}
    ;
" --skip-column-names | sed 's/\t/|/g' >${tmpfilename}

if [ -f "${tmpfilename}" ]; then

  target="${HIVE_DB_PATH}/${T_UPARPU_PLACEMENT}/yyyy=${yyyy}/mm=${mm}/dd=${dd}/"

  hadoop fs -rm -r ${target}

  ${FILE_COMMAND_CP} ${tmpfilename} ${target}

  hive -e "
        use ${DB_UPARPU};
        alter table ${T_UPARPU_PLACEMENT} drop partition (yyyy='${yyyy}',mm='${mm}',dd='${dd}');
        alter table ${T_UPARPU_PLACEMENT} add partition (yyyy='${yyyy}',mm='${mm}',dd='${dd}') location 'yyyy=${yyyy}/mm=${mm}/dd=${dd}';
    "

  echo "sync ${tmpfilename} to ${target}"

fi
```

1. 从mysql把出最新的广告位数据，然后同步到对应的hive表中

### merge_isready

> 统计isready的数据

```shell
#清洗isready数据，写入report_tk,dt&dimen=15
hourGap=0
date_time_timeStamp=$(date -d "${yyyy}-${mm}-${dd} 00:00:00" +%s)
next_date_time_timeStamp=$(expr ${date_time_timeStamp} + 86400)
next_date_time=$(date -d @${next_date_time_timeStamp} "+%Y-%m-%d")

where_dt="dt='${yyyy}-${mm}-${dd}'"
if [[ ${run_type} = 'utc0' ]]; then
  hourGap=8
  where_dt="dt in ('${yyyy}-${mm}-${dd}','${next_date_time}')"
else
  if [[ ${run_type} = 'utcw8' ]]; then
    hourGap=16
    where_dt="dt in ('${yyyy}-${mm}-${dd}','${next_date_time}')"
  else
    hourGap=0
  fi
fi

#转换成utc8时间
day_start_timeStamp=$(date -d "${yyyy}-${mm}-${dd} 00:00:00" +%s)
day_end_timeStamp=$(date -d "${yyyy}-${mm}-${dd} 23:00:00" +%s)
while_run_stamp=${day_start_timeStamp}
where_hour='('

while [ "${while_run_stamp}" -le "${day_end_timeStamp}" ]; do
  tmp_run_stamp=$(expr ${while_run_stamp} + ${hourGap} \* 3600)
  tmp_date_time=$(date -d @${tmp_run_stamp} "+%Y-%m-%d-%H")
  tmp_yyyy=$(echo ${tmp_date_time} | awk -F- '{print $1}')
  tmp_mm=$(echo ${tmp_date_time} | awk -F- '{print $2}')
  tmp_dd=$(echo ${tmp_date_time} | awk -F- '{print $3}')
  tmp_hh=$(echo ${tmp_date_time} | awk -F- '{print $4}')
  if [ "${while_run_stamp}" == "${day_start_timeStamp}" ]; then
    where_hour="${where_hour}dt='${tmp_yyyy}-${tmp_mm}-${tmp_dd}' and hour='${tmp_hh}'"
  else
    where_hour="${where_hour} or dt='${tmp_yyyy}-${tmp_mm}-${tmp_dd}' and hour='${tmp_hh}'"
  fi
  while_run_stamp=$(expr ${while_run_stamp} + 3600)
done
where_hour="${where_hour})"

hql="
      select
          '${yyyy}${mm}${dd}',
          case  when (c.nw_firm_id is null) then '0' else c.nw_firm_id end,
          case  when (a.group_id is null or a.group_id='' or cast(a.group_id as int) is null) then '0' else a.group_id end,
          case c.nw_firm_id when '35' then '1' else case  when (a.unit_id is null or a.unit_id='' or cast(a.unit_id as int) is null) then '0' else a.unit_id end end,
          1,
          a.sdk_version,
          a.app_vn,
          a.os_platform,
          a.geo_short,
          a.publisher_id,
          a.app_raw_id,
          b.id,
          b.format,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          case  when (a.channel is null) then '' else a.channel end,
          case  when (a.sub_channel is null) then '' else a.sub_channel end,
          0,
          0,
          0,
          0,
          case  when (c.network_id is null) then '0' else c.network_id end,
          case  when (a.traffic_group_id is null or a.traffic_group_id='') then '0' else a.traffic_group_id end,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          '',
          0,
          0,
          0,
          case when (a.is_cn_sdk is null or  cast(a.is_cn_sdk as int) is null) then '0' else a.is_cn_sdk end,
          cast(sum(case  when (a.key_count is null or a.key_count='') then 0 else a.key_count end) as bigint),
          0,
          0,
          case when device_type is null then 1 else device_type end,
          0,
          0,
          0,
          0,
          0,
          0,
          case when abtest_id is null then '' else abtest_id end,
          0
      from 
          (
            select 
                  nw_firm_id,
                  group_id,
                  unit_id,
                  placement_id,
                  sdk_version,
                  app_vn,
                  os_platform,
                  geo_short,
                  publisher_id,
                  app_raw_id,
                  channel,
                  sub_channel,
                  traffic_group_id,
                  is_cn_sdk,
                  device_type,
                  abtest_id,
                  cast(sum(case  when (key_count is null or key_count='') then 0 else key_count end) as bigint) key_count
            from ${DB_UPARPU}.${T_UPARPU_EVENT_ANALYSIS_COUNT}
            where
                 ${where_dt}
                 and key='1004632'
                 and ${where_hour}
                 ${whereInPublisherList}
            group by 
                  nw_firm_id,
                  group_id,
                  unit_id,
                  placement_id,
                  sdk_version,
                  app_vn,
                  os_platform,
                  geo_short,
                  publisher_id,
                  app_raw_id,
                  channel,
                  sub_channel,
                  traffic_group_id,
                  is_cn_sdk,
                  device_type,
                  abtest_id
          ) as a
      left outer join
          (
                select id,uuid,app_id,format
                from ${DB_UPARPU}.${T_UPARPU_PLACEMENT}
                where yyyy='${yyyy}' and mm='${mm}' and dd='${dd}' ${whereInPublisherList}
                group by id,uuid,app_id,format
          ) as b
      on 
          a.placement_id=b.uuid and a.app_raw_id=b.app_id
      left outer join
          (
                select id,network_id,nw_firm_id
                from ${DB_UPARPU}.${T_UPARPU_UNIT}
                where yyyy='${yyyy}' and mm='${mm}' and dd='${dd}' ${whereInPublisherList}
          ) as c
      on 
          a.unit_id=c.id
      where
         cast(a.group_id as bigint)<=2147483647
         and b.uuid is not null
      group by 
          case  when (c.nw_firm_id is null) then '0' else c.nw_firm_id end,
          case  when (a.group_id is null or a.group_id='' or cast(a.group_id as int) is null) then '0' else a.group_id end,
          case c.nw_firm_id when '35' then '1' else case  when (a.unit_id is null or a.unit_id='' or cast(a.unit_id as int) is null) then '0' else a.unit_id end end,
          a.sdk_version,
          a.app_vn,
          a.os_platform,
          a.geo_short,
          a.publisher_id,
          a.app_raw_id,
          b.id,
          b.format,
          case  when (a.channel is null) then '' else a.channel end,
          case  when (a.sub_channel is null) then '' else a.sub_channel end,
          case  when (c.network_id is null) then '0' else c.network_id end,
          case  when (a.traffic_group_id is null or a.traffic_group_id='') then '0' else a.traffic_group_id end,
          case when (a.is_cn_sdk is null or  cast(a.is_cn_sdk as int) is null) then '0' else a.is_cn_sdk end,
          case when device_type is null then 1 else device_type end,
          case when abtest_id is null then '' else abtest_id end
"

spark-submit --class com.topon.spark.jobs.common.CommonSparkDateTimeJob \
  --name "TopOn_CommonSparkDateTimeJob_report_isready_dt${yyyy_mm_dd}" \
  --master yarn \
  --deploy-mode cluster \
  --executor-memory "${SPARK_EXECUTOR_MEMORY}" \
  --driver-memory "${SPARK_DRIVER_MEMORY}" \
  --executor-cores 2 \
  --num-executors "${SPARK_NUM_EXECUTORS}" \
  --conf spark.dynamicAllocation.enabled=false \
  --conf spark.dynamicAllocation.minExecutors=32 \
  --conf spark.dynamicAllocation.maxExecutors=64 \
  --conf spark.core.connection.ack.wait.timeout=300 \
  ${SPARK_SQL_JAR} ${CLIENT_TMP_LOG_PATH} "${yyyy_mm_dd}" "${hql}" "${DB_UPARPU}.${T_RUN_REASSIGN_REPORT_TK}" "dt='${ymd}', dimen='15'" "overwrite"
```

1. `key='1004632'` 的数据为`isready`的数据
2. 由于原始数据已经有统计，所以`sum(key_count)`的总和就是`isready`的`对应维度`的总数
3. 数据写入到 `dimen = '15'` 的分区

### merge_isready_success

> 统计isready_success的数据

```shell
#清洗isready_success数据，写入report_tk,dt&dimen=16
hourGap=0
date_time_timeStamp=`date -d "${yyyy}-${mm}-${dd} 00:00:00" +%s`
next_date_time_timeStamp=`expr ${date_time_timeStamp} + 86400`
next_date_time=`date -d @${next_date_time_timeStamp} "+%Y-%m-%d"`

where_dt="dt='${yyyy}-${mm}-${dd}'"
if [[ ${run_type} = 'utc0' ]]
  then
     hourGap=8
     where_dt="dt in ('${yyyy}-${mm}-${dd}','${next_date_time}')"
  else
      if [[ ${run_type} = 'utcw8' ]] 
      then
          hourGap=16
          where_dt="dt in ('${yyyy}-${mm}-${dd}','${next_date_time}')"
      else
          hourGap=0
      fi
fi


#转换成utc8时间
day_start_timeStamp=`date -d "${yyyy}-${mm}-${dd} 00:00:00" +%s`
day_end_timeStamp=`date -d "${yyyy}-${mm}-${dd} 23:00:00" +%s`
while_run_stamp=${day_start_timeStamp}
where_hour='('

while [ "${while_run_stamp}" -le "${day_end_timeStamp}" ]
do
      tmp_run_stamp=`expr ${while_run_stamp} + ${hourGap} \* 3600`
      tmp_date_time=`date -d @${tmp_run_stamp} "+%Y-%m-%d-%H"`
      tmp_yyyy=`echo ${tmp_date_time}|awk -F- '{print $1}'`
      tmp_mm=`echo ${tmp_date_time}|awk -F- '{print $2}'`
      tmp_dd=`echo ${tmp_date_time}|awk -F- '{print $3}'`
      tmp_hh=`echo ${tmp_date_time}|awk -F- '{print $4}'`
      if [ "${while_run_stamp}" == "${day_start_timeStamp}" ]
        then
           where_hour="${where_hour}dt='${tmp_yyyy}-${tmp_mm}-${tmp_dd}' and hour='${tmp_hh}'"
        else
           where_hour="${where_hour} or dt='${tmp_yyyy}-${tmp_mm}-${tmp_dd}' and hour='${tmp_hh}'"
        fi
        while_run_stamp=`expr ${while_run_stamp} + 3600`
done
where_hour="${where_hour})"

hql="
      select
          '${yyyy}${mm}${dd}',
          case  when (c.nw_firm_id is null) then '0' else c.nw_firm_id end,
          case  when (a.group_id is null or a.group_id='' or cast(a.group_id as int) is null) then '0' else a.group_id end,
          case c.nw_firm_id when '35' then '1' else case  when (a.unit_id is null or a.unit_id='' or cast(a.unit_id as int) is null) then '0' else a.unit_id end end,
          1,
          a.sdk_version,
          a.app_vn,
          a.os_platform,
          a.geo_short,
          a.publisher_id,
          a.app_raw_id,
          b.id,
          b.format,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          case  when (a.channel is null) then '' else a.channel end,
          case  when (a.sub_channel is null) then '' else a.sub_channel end,
          0,
          0,
          0,
          0,
          case  when (c.network_id is null) then '0' else c.network_id end,
          case  when (a.traffic_group_id is null or a.traffic_group_id='') then '0' else a.traffic_group_id end,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          '',
          0,
          0,
          0,
          case when (a.is_cn_sdk is null or  cast(a.is_cn_sdk as int) is null) then '0' else a.is_cn_sdk end,
          0,
          cast(sum(case  when (a.key_count is null or a.key_count='') then 0 else a.key_count end) as bigint),
          0,
          case when device_type is null then 1 else device_type end,
          0,
          0,
          0,
          0,
          0,
          0,
          case when abtest_id is null then '' else abtest_id end,
          0
      from 
          (
            select 
                  nw_firm_id,
                  group_id,
                  unit_id,
                  placement_id,
                  sdk_version,
                  app_vn,
                  os_platform,
                  geo_short,
                  publisher_id,
                  app_raw_id,
                  channel,
                  sub_channel,
                  traffic_group_id,
                  is_cn_sdk,
                  device_type,
                  abtest_id,
                  cast(sum(case  when (key_count is null or key_count='') then 0 else key_count end) as bigint) key_count
            from ${DB_UPARPU}.${T_UPARPU_EVENT_ANALYSIS_COUNT}
            where
                 ${where_dt}
                 and key='1004632'
                 and extra3='1'
                 and ${where_hour}
                 ${whereInPublisherList}
            group by 
                  nw_firm_id,
                  group_id,
                  unit_id,
                  placement_id,
                  sdk_version,
                  app_vn,
                  os_platform,
                  geo_short,
                  publisher_id,
                  app_raw_id,
                  channel,
                  sub_channel,
                  traffic_group_id,
                  is_cn_sdk,
                  device_type,
                  abtest_id
          ) as a
      left outer join
          (
                select id,uuid,app_id,format
                from ${DB_UPARPU}.${T_UPARPU_PLACEMENT}
                where yyyy='${yyyy}' and mm='${mm}' and dd='${dd}' ${whereInPublisherList}
                group by id,uuid,app_id,format
          ) as b
      on 
          a.placement_id=b.uuid and a.app_raw_id=b.app_id
      left outer join
          (
                select id,network_id,nw_firm_id
                from ${DB_UPARPU}.${T_UPARPU_UNIT}
                where yyyy='${yyyy}' and mm='${mm}' and dd='${dd}' ${whereInPublisherList}
          ) as c
      on 
          a.unit_id=c.id
      where
         cast(a.group_id as bigint)<=2147483647
         and b.uuid is not null
      group by 
          case  when (c.nw_firm_id is null) then '0' else c.nw_firm_id end,
          case  when (a.group_id is null or a.group_id='' or cast(a.group_id as int) is null) then '0' else a.group_id end,
          case c.nw_firm_id when '35' then '1' else case  when (a.unit_id is null or a.unit_id='' or cast(a.unit_id as int) is null) then '0' else a.unit_id end end,
          a.sdk_version,
          a.app_vn,
          a.os_platform,
          a.geo_short,
          a.publisher_id,
          a.app_raw_id,
          b.id,
          b.format,
          case  when (a.channel is null) then '' else a.channel end,
          case  when (a.sub_channel is null) then '' else a.sub_channel end,
          case  when (c.network_id is null) then '0' else c.network_id end,
          case  when (a.traffic_group_id is null or a.traffic_group_id='') then '0' else a.traffic_group_id end,
          case when (a.is_cn_sdk is null or  cast(a.is_cn_sdk as int) is null) then '0' else a.is_cn_sdk end,
          case when device_type is null then 1 else device_type end,
          case when abtest_id is null then '' else abtest_id end
"

spark-submit --class com.topon.spark.jobs.common.CommonSparkDateTimeJob \
  --name "TopOn_CommonSparkDateTimeJob_report_isready_success_dt${yyyy_mm_dd}" \
  --master yarn \
  --deploy-mode cluster \
  --executor-memory "${SPARK_EXECUTOR_MEMORY}" \
  --driver-memory "${SPARK_DRIVER_MEMORY}" \
  --executor-cores 2 \
  --num-executors "${SPARK_NUM_EXECUTORS}" \
  --conf spark.dynamicAllocation.enabled=false \
  --conf spark.dynamicAllocation.minExecutors=32 \
  --conf spark.dynamicAllocation.maxExecutors=64 \
  --conf spark.core.connection.ack.wait.timeout=300 \
  ${SPARK_SQL_JAR} ${CLIENT_TMP_LOG_PATH} "${yyyy_mm_dd}" "${hql}" "${DB_UPARPU}.${T_RUN_REASSIGN_REPORT_TK}" "dt='${ymd}', dimen='16'" "overwrite"
```

1. `key='1004632' and extra3='1'` 代表 isready 下，并且相应成功的数据
2. 由于原始数据已经有统计，所以`sum(key_count)`的总和就是`isready_success`的`对应维度`的总数
3. 写入到 `dimen = '16'` 的分区

### merge_showfailed

> 统计广告展示失败

```shell
#清洗isready数据，写入report_tk,dt&dimen=15
hourGap=0
date_time_timeStamp=`date -d "${yyyy}-${mm}-${dd} 00:00:00" +%s`
next_date_time_timeStamp=`expr ${date_time_timeStamp} + 86400`
next_date_time=`date -d @${next_date_time_timeStamp} "+%Y-%m-%d"`

where_dt="dt='${yyyy}-${mm}-${dd}'"
if [[ ${run_type} = 'utc0' ]]
  then
     hourGap=8
     where_dt="dt in ('${yyyy}-${mm}-${dd}','${next_date_time}')"
  else
      if [[ ${run_type} = 'utcw8' ]] 
      then
          hourGap=16
          where_dt="dt in ('${yyyy}-${mm}-${dd}','${next_date_time}')"
      else
          hourGap=0
      fi
fi


#转换成utc8时间
day_start_timeStamp=`date -d "${yyyy}-${mm}-${dd} 00:00:00" +%s`
day_end_timeStamp=`date -d "${yyyy}-${mm}-${dd} 23:00:00" +%s`
while_run_stamp=${day_start_timeStamp}
where_hour='('

while [ "${while_run_stamp}" -le "${day_end_timeStamp}" ]
do
      tmp_run_stamp=`expr ${while_run_stamp} + ${hourGap} \* 3600`
      tmp_date_time=`date -d @${tmp_run_stamp} "+%Y-%m-%d-%H"`
      tmp_yyyy=`echo ${tmp_date_time}|awk -F- '{print $1}'`
      tmp_mm=`echo ${tmp_date_time}|awk -F- '{print $2}'`
      tmp_dd=`echo ${tmp_date_time}|awk -F- '{print $3}'`
      tmp_hh=`echo ${tmp_date_time}|awk -F- '{print $4}'`
      if [ "${while_run_stamp}" == "${day_start_timeStamp}" ]
        then
           where_hour="${where_hour}dt='${tmp_yyyy}-${tmp_mm}-${tmp_dd}' and hour='${tmp_hh}'"
        else
           where_hour="${where_hour} or dt='${tmp_yyyy}-${tmp_mm}-${tmp_dd}' and hour='${tmp_hh}'"
        fi
        while_run_stamp=`expr ${while_run_stamp} + 3600`
done
where_hour="${where_hour})"

hql="
      select
          '${yyyy}${mm}${dd}',
          case  when (c.nw_firm_id is null) then '0' else c.nw_firm_id end,
          case  when (a.group_id is null or a.group_id='' or cast(a.group_id as int) is null) then '0' else a.group_id end,
          case c.nw_firm_id when '35' then '1' else case  when (a.unit_id is null or a.unit_id='' or cast(a.unit_id as int) is null) then '0' else a.unit_id end end,
          1,
          a.sdk_version,
          a.app_vn,
          a.os_platform,
          a.geo_short,
          a.publisher_id,
          a.app_raw_id,
          b.id,
          b.format,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          case  when (a.channel is null) then '' else a.channel end,
          case  when (a.sub_channel is null) then '' else a.sub_channel end,
          0,
          0,
          0,
          0,
          case  when (c.network_id is null) then '0' else c.network_id end,
          case  when (a.traffic_group_id is null or a.traffic_group_id='') then '0' else a.traffic_group_id end,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          '',
          0,
          0,
          0,
          case when (a.is_cn_sdk is null or  cast(a.is_cn_sdk as int) is null) then '0' else a.is_cn_sdk end,
          0,
          0,
          cast(sum(case  when (a.key_count is null or a.key_count='') then 0 else a.key_count end) as bigint),
          case when device_type is null then 1 else device_type end,
          0,
          0,
          0,
          0,
          0,
          0,
          case when abtest_id is null then '' else abtest_id end,
          0
      from 
          (
            select 
                  group_id,
                  unit_id,
                  placement_id,
                  sdk_version,
                  app_vn,
                  os_platform,
                  geo_short,
                  publisher_id,
                  app_raw_id,
                  channel,
                  sub_channel,
                  traffic_group_id,
                  is_cn_sdk,
                  device_type,
                  abtest_id,
                  cast(sum(case  when (key_count is null or key_count='') then 0 else key_count end) as bigint) key_count
            from ${DB_UPARPU}.${T_UPARPU_EVENT_ANALYSIS_COUNT}
            where
                 ${where_dt}
                 and key='1004633'
                 and ${where_hour}
                 ${whereInPublisherList}
            group by 
                  group_id,
                  unit_id,
                  placement_id,
                  sdk_version,
                  app_vn,
                  os_platform,
                  geo_short,
                  publisher_id,
                  app_raw_id,
                  channel,
                  sub_channel,
                  traffic_group_id,
                  is_cn_sdk,
                  device_type,
                  abtest_id
          ) as a
      left outer join
          (
                select id,uuid,app_id,format
                from ${DB_UPARPU}.${T_UPARPU_PLACEMENT}
                where yyyy='${yyyy}' and mm='${mm}' and dd='${dd}' ${whereInPublisherList}
                group by id,uuid,app_id,format
          ) as b
      on 
          a.placement_id=b.uuid and a.app_raw_id=b.app_id
      left outer join
          (
                select id,network_id,nw_firm_id
                from ${DB_UPARPU}.${T_UPARPU_UNIT}
                where yyyy='${yyyy}' and mm='${mm}' and dd='${dd}' ${whereInPublisherList}
          ) as c
      on 
          a.unit_id=c.id
      where
         cast(a.group_id as bigint)<=2147483647
         and b.uuid is not null
      group by 
          case  when (c.nw_firm_id is null) then '0' else c.nw_firm_id end,
          case  when (a.group_id is null or a.group_id='' or cast(a.group_id as int) is null) then '0' else a.group_id end,
          case c.nw_firm_id when '35' then '1' else case  when (a.unit_id is null or a.unit_id='' or cast(a.unit_id as int) is null) then '0' else a.unit_id end end,
          a.sdk_version,
          a.app_vn,
          a.os_platform,
          a.geo_short,
          a.publisher_id,
          a.app_raw_id,
          b.id,
          b.format,
          case  when (a.channel is null) then '' else a.channel end,
          case  when (a.sub_channel is null) then '' else a.sub_channel end,
          case  when (c.network_id is null) then '0' else c.network_id end,
          case  when (a.traffic_group_id is null or a.traffic_group_id='') then '0' else a.traffic_group_id end,
          case when (a.is_cn_sdk is null or  cast(a.is_cn_sdk as int) is null) then '0' else a.is_cn_sdk end,
          case when device_type is null then 1 else device_type end,
          case when abtest_id is null then '' else abtest_id end
"

spark-submit --class com.topon.spark.jobs.common.CommonSparkDateTimeJob \
  --name "TopOn_CommonSparkDateTimeJob_report_showfailed_dt${yyyy_mm_dd}" \
  --master yarn \
  --deploy-mode cluster \
  --executor-memory "${SPARK_EXECUTOR_MEMORY}" \
  --driver-memory "${SPARK_DRIVER_MEMORY}" \
  --executor-cores 2 \
  --num-executors "${SPARK_NUM_EXECUTORS}" \
  --conf spark.dynamicAllocation.enabled=false \
  --conf spark.dynamicAllocation.minExecutors=32 \
  --conf spark.dynamicAllocation.maxExecutors=64 \
  --conf spark.core.connection.ack.wait.timeout=300 \
  ${SPARK_SQL_JAR} ${CLIENT_TMP_LOG_PATH} "${yyyy_mm_dd}" "${hql}" "${DB_UPARPU}.${T_RUN_REASSIGN_REPORT_TK}" "dt='${ymd}', dimen='17'" "overwrite"
```

1. `key='1004633'` 代表 `showfailed` 的数据
2. 由于原始数据已经有统计，所以`sum(key_count)`的总和就是`showfailed`的`对应维度`的总数
3. 写入到 `dimen = '17'` 的分区

### merge_revenue

> 统计收益数据

```shell

hql="
        select
            '${yyyy}${mm}${dd}',
            case  when (a.nw_firm_id is null or a.nw_firm_id='' or a.nw_firm_id not rlike '^\\\\\\d+$') then '0' else a.nw_firm_id end,
            case  when (a.group_id is null or a.group_id='' or a.group_id not rlike '^\\\\\\d+$') then '0' else a.group_id end,
            case  when (a.unit_id is null or a.unit_id='' or a.unit_id not rlike '^\\\\\\d+$') then '0' else a.unit_id end,
            a.system_type,
            a.sdk_version,
            case a.app_vn when 'null' then '0' else regexp_replace(a.app_vn,'\\\\\\\\0000','') end,
            a.os_platform,
            a.geo_short,
            a.publisher_id,
            a.app_id,
            a.placement_id,
            a.format,
            a.sc_type,
            cast(sum(a.request) as bigint),
            cast(sum(a.filled_request) as bigint),
            cast(sum(a.impression) as bigint),
            cast(sum(a.click) as bigint),
            cast(sum(a.load) as bigint),
            cast(sum(a.filled_load) as bigint),
            cast(sum(a.rv_play_start) as bigint),
            cast(sum(a.rv_play_complete) as bigint),
            a.channel,
            a.sub_channel,
            cast(sum(a.app_request) as bigint),
            cast(sum(a.placement_request) as bigint),
            cast(sum(a.show) as bigint),
            cast(sum(a.impression_optimize) as bigint),
            a.network_id,
            case  when (a.traffic_group_id is null or a.traffic_group_id='') then '0' else a.traffic_group_id end,
            case  when (a.bidtype is null or a.bidtype='') then '0' else a.bidtype end,
            cast(sum(case  when (a.bid_request is null or a.bid_request='') then '0' else a.bid_request end) as bigint),
            cast(sum(case  when (a.bid_response is null or a.bid_response='') then '0' else a.bid_response end) as bigint),
            cast(sum(case  when (a.estimated_revenue is null or a.estimated_revenue='') then '0' else a.estimated_revenue end) as float),
            case 
                when (sum(case when a.fake_impression_optimize is not null then a.fake_impression_optimize else a.impression_optimize end)* sum(b.total_revenue) / sum(b.total_impression)) is null then '0' 
                else cast((sum(case when a.fake_impression_optimize is not null then a.fake_impression_optimize else a.impression_optimize end) * sum(b.total_revenue) / sum(b.total_impression)) as float) 
            end,
            case 
                when (sum(case when a.fake_impression_optimize is not null then a.fake_impression_optimize else a.impression_optimize end)* sum(b.total_currency_revenue) / sum(b.total_impression)) is null then '0' 
                else cast((sum(case when a.fake_impression_optimize is not null then a.fake_impression_optimize else a.impression_optimize end) * sum(b.total_currency_revenue) / sum(b.total_impression)) as float) 
            end,
            case when (scenario is null or scenario='') then '1' else scenario end,
            case when (error_type is null or error_type='') then '0' else error_type end,
            case when (error_msg is null or error_msg='') then '' else error_msg end,
            cast(sum(case  when (a.fake_impression_optimize is null or a.fake_impression_optimize='') then a.impression_optimize else a.fake_impression_optimize end) as bigint),
            cast(sum(case  when (a.fake_filled_load is null or a.fake_filled_load='') then a.filled_load else a.fake_filled_load end) as bigint),
            cast(sum(case  when (a.fake_filled_request is null or a.fake_filled_request='') then a.filled_request else a.fake_filled_request end) as bigint),
            case when (a.is_cn_sdk is null or a.is_cn_sdk not rlike '^\\\\\\d+$') then '0' else a.is_cn_sdk end,
            cast(sum(case  when (a.ready_request is null or a.ready_request='' or a.ready_request='(null)') then 0 else a.ready_request end) as bigint),
            cast(sum(case  when (a.ready_success is null or a.ready_success='' or a.ready_success='(null)') then 0 else a.ready_success end) as bigint),
            cast(sum(case  when (a.show_failed is null or a.show_failed='' or a.show_failed='(null)') then 0 else a.show_failed end) as bigint),
            case when device_type is null then 1 else device_type end,
            cast(sum(case when load_cost_time is null or load_cost_time<0 then 0 else load_cost_time end) as bigint),
            cast(sum(case when request_cost_time is null or request_cost_time<0 then 0 else request_cost_time end) as bigint),
            case when idfa_exist_tag is null then 0 else idfa_exist_tag end,
            case coalesce(sum(bid_response),0) when 0  then 0.0 else cast(sum(bid_response * coalesce(bid_response_ecpm,0)) / sum(bid_response) as float) end,
            cast(coalesce(sum(scenario_entry),0) as bigint),
            cast(coalesce(sum(scenario_entry_ready),0) as bigint),
            case when abtest_id is null then '' else abtest_id end,
            case when ofl is null then 0 else ofl end
        from 
            (select *
            from ${DB_UPARPU}.${T_RUN_REPORT_TK}
            where
                dt = '${ymd}'
                and dimen in ('0','15','16','17','18')
                and os_platform is not null
                and length(sdk_version) < 15
                and bidtype<=20
                and bidtype>=0
                and format>=0
                ${whereInPublisherList}
            ) as a
        left join
            (
           select 
              app_id,
              placement_id,
              geo_short,
              unit_id,
              tk_impression as total_impression,
              revenue as total_revenue,
              currency_revenue as total_currency_revenue,
              dt
            from
              ${DB_UPARPU}.${T_UPARPU_TK_UNIT_ECPM}
            where
              dt = '${ymd}'
              ${whereInPublisherList}
            ) as b
        on 
            a.app_id = b.app_id and
            a.placement_id=b.placement_id and
            a.geo_short = b.geo_short and
            a.unit_id = b.unit_id
            and a.dt = '${ymd}'
            and b.dt = '${ymd}'
            and a.dimen in ('0','15','16','17','18')
        where 
            a.dt = '${ymd}'
            and a.dimen in ('0','15','16','17','18')
        group by 
            case  when (a.nw_firm_id is null or a.nw_firm_id='' or a.nw_firm_id not rlike '^\\\\\\d+$') then '0' else a.nw_firm_id end,
            case  when (a.group_id is null or a.group_id='' or a.group_id not rlike '^\\\\\\d+$') then '0' else a.group_id end,
            case  when (a.unit_id is null or a.unit_id='' or a.unit_id not rlike '^\\\\\\d+$') then '0' else a.unit_id end,
            a.system_type,
            a.sdk_version,
            case a.app_vn when 'null' then '0' else regexp_replace(a.app_vn,'\\\\\\\\0000','') end,
            a.os_platform,
            a.geo_short,
            a.publisher_id,
            a.app_id,
            a.placement_id,
            a.format,
            a.sc_type,
            a.channel,
            a.sub_channel,
            a.network_id,
            case  when (a.traffic_group_id is null or a.traffic_group_id='') then '0' else a.traffic_group_id end,
            case  when (a.bidtype is null or a.bidtype='') then '0' else a.bidtype end,
            case when (scenario is null or scenario='') then '1' else scenario end,
            case when (error_type is null or error_type='') then '0' else error_type end,
            case when (error_msg is null or error_msg='') then '' else error_msg end,
            case when (a.is_cn_sdk is null or a.is_cn_sdk not rlike '^\\\\\\d+$') then '0' else a.is_cn_sdk end,
            case when device_type is null then 1 else device_type end,
            case when idfa_exist_tag is null then 0 else idfa_exist_tag end,
            case when abtest_id is null then '' else abtest_id end,
            case when ofl is null then 0 else ofl end
"

spark-submit --class com.topon.spark.jobs.common.CommonSparkDateTimeJob \
  --name "TopOn_CommonSparkDateTimeJob_ltv_tk_reassign_revenue_dt${yyyy_mm_dd}" \
  --master yarn \
  --deploy-mode cluster \
  --executor-memory "${SPARK_EXECUTOR_MEMORY}" \
  --driver-memory "${SPARK_DRIVER_MEMORY}" \
  --executor-cores 2 \
  --num-executors "${SPARK_NUM_EXECUTORS}" \
  --conf spark.dynamicAllocation.enabled=false \
  --conf spark.dynamicAllocation.minExecutors=32 \
  --conf spark.dynamicAllocation.maxExecutors=64 \
  --conf spark.core.connection.ack.wait.timeout=300 \
  ${SPARK_SQL_JAR} ${CLIENT_TMP_LOG_PATH} "${ymd}" "${hql}" "${DB_UPARPU}.${T_RUN_REASSIGN_REPORT_TK}" "dt='${ymd}', dimen='00'" "overwrite"
```

1. `dimen in ('0','15','16','17','18')`，分别是`0=原来的小时任务统计出来的数据，15=isready数据, 16=isready_success数据,17=showfailed数据, 18=source_type=2的实时数据`，拿到所有的数据，然后进行重新写入
2. 重新写入到 `dimen = '00'` 的分区中

### to_db

> 数据出仓到gp

```shell
source ./export_tk_util.sh
export_tk_func "${DB_UPARPU}.${T_RUN_REASSIGN_REPORT_TK}" "${T_RUN_REPORT_TK_SOURCE}" "dt='${ymd}' AND dimen='00'" " date_time =${yyyy}${mm}${dd}  ${whereInPublisherList} "
```

1. 把当天的数据delete掉
2. 把所有数据重新写入到gp


重分配到任务做的事情处理完毕。注意这里涉及到了`收益`数据，由于业务导向是聚合平台，所以会收益是从多个平台拉取回来的数据，可能存在拉取收益数据失败的情况，所以需要有重试机制，所以这里的的任务有分为`跑前1天`，`跑前2天`，`跑前3天`的数据，如果超过3天，都拉取失败，那么这部分数据我们需要手动重跑。