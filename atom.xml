<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>白菜君の技术库</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://blog.crazylaw.cn/"/>
  <updated>2021-03-31T07:52:48.306Z</updated>
  <id>http://blog.crazylaw.cn/</id>
  
  <author>
    <name>白菜(whiteCcinn)</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>【数据库开发】msource</title>
    <link href="http://blog.crazylaw.cn/2021/03/27/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%BC%80%E5%8F%91%E7%9F%A5%E8%AF%86/msource/"/>
    <id>http://blog.crazylaw.cn/2021/03/27/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%BC%80%E5%8F%91%E7%9F%A5%E8%AF%86/msource/</id>
    <published>2021-03-26T17:16:43.000Z</published>
    <updated>2021-03-31T07:52:48.306Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p><code>CAP</code>原则又称CAP定理，指的是在一个分布式系统中，<code>一致性（Consistency）</code>、<code>可用性（Availability）</code>、<code>分区容错性（Partition tolerance）</code>。CAP 原则指的是，这三个要素最多只能同时实现两点，不可能三者兼顾。</p><p><code>msource</code> 是我们的一个 <code>数据源组件</code>，我们所有的大数据ETL服务都构建在此之上，所以我们msource可以说是所有业务系统的核心。他维护着一个稳定，可靠，高性能的数据传输机制。让我们 <code>业务层</code> 中可以做各种操作，同步，异步等等。</p><p>msource 的角色我大体分为了2种：</p><ul><li>spout （数据推送组件)</li><li>db （数据存储组件）</li></ul><a id="more"></a><p><img src="/images/%E6%95%B0%E6%8D%AE%E5%BA%93/spout_db_relation.png" alt=" spout和db的关系 "></p><p>在这个图中，我们可以看到，db可以独立出来应用，他不依赖于spout。spout默认的传输机制是我们golang中的channel模式，但是它可以选择使用db模式。</p><h1 id="MSOURCE-DB"><a href="#MSOURCE-DB" class="headerlink" title="MSOURCE_DB"></a>MSOURCE_DB</h1><h2 id="RockesDB的基础知识"><a href="#RockesDB的基础知识" class="headerlink" title="RockesDB的基础知识"></a>RockesDB的基础知识</h2><p>rocksdb 我们知道他是支持WAL（Write Ahead Log）的，一般的log文件中通常包括 <code>redo log</code> 和 <code>undo log</code>。其实这不仅仅是rocksdb独有的，这是一种可靠性的保证，像mysql一样有这种机制，也是分为<code>redo log</code>, <code>undo log</code>, <code>binlog</code>，区别就在于 <code>binlog</code> 属于逻辑日志，<code>redo log</code>和<code>undo log</code>属于物理日志。</p><p>rocksdb是facebook开发的一个<code>kv存储引擎</code>。他的机构模式是基于<code>LSM</code>的。基于LSM的架构都需要经过一个叫 <code>Compaction</code>  过程，通常Compaction涉及到三个放大因子。</p><p>Compaction需要在三者之间做取舍。</p><ul><li>写放大 （Write Amplification）</li><li>读放大（Read Amplification）</li><li>空间放大 （Space Amplification）</li></ul><p>后台的 compaction 来减少读放大（减少 SST 文件数量）和空间放大（清理过期数据），但也因此带来了写放大（Write Amplification）的问题。</p><h3 id="Compaction"><a href="#Compaction" class="headerlink" title="Compaction"></a>Compaction</h3><h4 id="写放大"><a href="#写放大" class="headerlink" title="写放大"></a>写放大</h4><p>假设每秒写入10MB的数据，但观察到硬盘的写入是30MB/s，那么写放大就是3。写分为<code>立即写</code>和<code>延迟写</code>，比如<code>redo log</code>是立即写，传统基于B-Tree数据库<code>刷脏页</code>和<code>LSM Compaction</code>是延迟写。<code>redo log</code>使用<code>direct IO</code>写时至少以512字节对齐，假如log记录为100字节，磁盘需要写入512字节，写放大为5。</p><blockquote><p>DirectIO是直接操作IO，不经过BufferIO。<br>BufferIO也称为标准IO，两个系统调用实现的：read() 和 write()。BufferIO用了操作系统内核的页缓存，保护了磁盘，减少读盘的次数，提高了读取速度。但是由于使用了页缓存，它是处于内核空间的，无法被用户直接操作，所以需要经历一次数据拷贝复制。<br>DirectIO 数据均直接在用户地址空间的缓冲区和磁盘之间直接进行传输，中间少了页缓存的支持。读写数据的时候获得更好的性能。使用直接 I/O 读写数据必须要注意缓冲区对齐。</p></blockquote><h4 id="读放大"><a href="#读放大" class="headerlink" title="读放大"></a>读放大</h4><p>对应于一个简单query需要读取硬盘的次数。比如一个简单query读取了5个页面，发生了5次IO，那么读放大就是 5。假如B-Tree的非叶子节点都缓存在内存中，point read-amp 为1，一次磁盘读取就可以获取到Leaf Block；short range read-amp 为1<del>2，1</del>2次磁盘读取可以获取到所需的Leaf Block。</p><p>操作需要从新到旧（从上到下）一层一层查找，直到找到想要的数据。这个过程可能需要<code>不止一次 I/O</code>。特别是 range query 的情况，影响很明显。</p><h4 id="空间放大"><a href="#空间放大" class="headerlink" title="空间放大"></a>空间放大</h4><p>假设我需要存储10MB数据，但实际硬盘占用了30MB，那么空间放大就是3。有比较多的因素会影响空间放大，比如在Compaction过程中需要临时存储空间，空间碎片，Block中有效数据的比例小，旧版本数据未及时删除等等。</p><p>所有的写入都是顺序写 <code>append-only</code> 的，不是 <code>in-place update</code>，所以过期数据不会马上被清理掉。</p><h3 id="LSM-树"><a href="#LSM-树" class="headerlink" title="LSM 树"></a>LSM 树</h3><p>LSM 树的设计思想非常朴素, 它的原理是把一颗大树拆分成N棵小树， 它首先写入到内存中（内存没有寻道速度的问题，随机写的性能得到大幅提升），在内存中构建一颗有序小树，随着小树越来越大，内存的小树会flush到磁盘上。磁盘中的树定期可以做 merge 操作，合并成一棵大树，以优化读性能【读数据的过程可能需要从内存 memtable 到磁盘 sstfile 读取多次，称之为读放大】。RocksDB 的 LSM 体现在多 level 文件格式上，最热最新的数据尽在 L0 层，数据在内存中，最冷最老的数据尽在 LN 层，数据在磁盘或者固态盘上。</p><p><img src="/images/%E6%95%B0%E6%8D%AE%E5%BA%93/log_structured_merge_tree.png" alt=" LSM合并过程 "></p><h3 id="Rocksdb"><a href="#Rocksdb" class="headerlink" title="Rocksdb"></a>Rocksdb</h3><p>RocksDB的三种基本文件格式是 <code>memtable</code> / <code>sstfile</code> / <code>logfile</code>，<code>memtable</code> 是一种内存文件数据系统，新写数据会被写进 <code>memtable</code>，部分请求内容会被写进 <code>logfile</code>。<code>logfile</code> 是一种有利于顺序写的文件系统。<code>memtable</code> 的内存空间被填满之后，会有一部分老数据被转移到 <code>sstfile</code> 里面，这些数据对应的 <code>logfile</code> 里的 <code>log</code> 就会被安全删除</p><p>单独的 Get/Put/Delete 是原子操作，要么成功要么失败，不存在中间状态。</p><p>如果需要进行批量的 Get/Put/Delete 操作且需要操作保持原子属性，则可以使用 WriteBatch。</p><p><img src="/images/%E6%95%B0%E6%8D%AE%E5%BA%93/L0.png" alt=" LSM合并过程0 "></p><p>L0 -&gt; L1</p><p><img src="/images/%E6%95%B0%E6%8D%AE%E5%BA%93/L1.png" alt=" LSM合并过程1 "></p><p>L1 -&gt; L2</p><p><img src="/images/%E6%95%B0%E6%8D%AE%E5%BA%93/L2.png" alt=" LSM合并过程2 "></p><p>L1 -&gt; L2</p><p><img src="/images/%E6%95%B0%E6%8D%AE%E5%BA%93/write_process.png" alt=" 写入过程 "></p><p>可以看到主要的三个组成部分，内存结构<code>memtable</code>，类似事务日志角色的<code>WAL文件</code>，持久化的<code>SST文件</code>。</p><p>数据会放到内存结构<code>memtable</code>，当<code>memtable</code>的数据大小超过阈值(write_buffer_size)后，会<code>新生成一个memtable</code>继续写，将前一个memtable保存为<code>只读memtable</code>。当只读memtable的数量超过阈值后，会将<code>所有的只读memtable</code>合并并flush到磁盘生成一个<code>SST文件</code>。</p><p>这里的SST属于level0， level0中的每个SST有序，可能会有交叉。写入WAL文件是<code>可选的</code>，用来<code>恢复未写入到磁盘的memtable</code>。</p><p>memtable如其名为一种内存的数据结构。通过设置memtable的大小、总大小来控制何时flush到SST文件。大部分格式的memtable不支持并发写入，并发调用依然会依次写入。目前仅支持<code>skiplist</code>。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">rocksdb:</span></span><br><span class="line">  <span class="attr">options:</span></span><br><span class="line">    <span class="comment"># 如果数据库不存在是否自动建立</span></span><br><span class="line">    <span class="comment"># default: false</span></span><br><span class="line">    <span class="attr">create.if.missing:</span> <span class="literal">true</span></span><br><span class="line">    <span class="comment"># 如果数据库已经存在是否直接抛出异常</span></span><br><span class="line">    <span class="comment"># default: false</span></span><br><span class="line">    <span class="attr">error.if.exists:</span> <span class="literal">false</span></span><br><span class="line">    <span class="comment"># default: false</span></span><br><span class="line">    <span class="attr">paranoid.checks:</span> <span class="literal">false</span></span><br><span class="line">    <span class="comment"># 日志等级</span></span><br><span class="line">    <span class="comment"># Debug = 0/Info  = 1/Warn  = 2/Error = 3/Fatal = 4</span></span><br><span class="line">    <span class="attr">info.log.level:</span> <span class="number">3</span></span><br><span class="line">    <span class="comment"># 最佳值是内核（cpu）数</span></span><br><span class="line">    <span class="comment"># 默认RocksDB只使用一个后台线程进行flush和compaction</span></span><br><span class="line">    <span class="comment"># default: 1</span></span><br><span class="line">    <span class="attr">increase.parallelism:</span> <span class="number">4</span></span><br><span class="line">    <span class="comment"># 是否允许并发写入memtabe，目前仅支持skiplist</span></span><br><span class="line">    <span class="comment"># default: false</span></span><br><span class="line">    <span class="attr">allow.concurrent.memtable.writes:</span> <span class="literal">false</span></span><br><span class="line">    <span class="comment"># 更大的值可以提高性能，特别是在批量加载时</span></span><br><span class="line">    <span class="comment"># 此外，更大的写缓冲区在下次打开数据库时将导致更长的恢复时间</span></span><br><span class="line">    <span class="attr">write.buffer.size:</span> <span class="number">64</span> <span class="string">*</span> <span class="number">1024</span> <span class="string">*</span> <span class="number">1024</span></span><br><span class="line">    <span class="comment"># 最大写缓冲区数, 当一个写缓冲区被刷新到存储时，新的写操作可以继续到另一个写缓冲区</span></span><br><span class="line">    <span class="comment"># default: 2</span></span><br><span class="line">    <span class="attr">max.write.buffer.number:</span> <span class="number">4</span></span><br><span class="line">    <span class="attr">min.write.buffer.number.to.merge:</span> <span class="number">1</span></span><br><span class="line">    <span class="attr">max.open.files:</span> <span class="number">1000</span></span><br><span class="line">    <span class="attr">max.file.opening.threads:</span> <span class="number">16</span></span><br><span class="line">    <span class="comment"># 数据压缩方式</span></span><br><span class="line">    <span class="comment"># No = 0/ Snappy = 1 / ZLib = 2 / Bz2 = 3 / LZ4 = 4 / LZ4HC = 5 / Xpress = 6 / ZSTD = 7</span></span><br><span class="line">    <span class="attr">compression:</span> <span class="number">1</span></span><br><span class="line">    <span class="comment"># 设置数据库的level的数量</span></span><br><span class="line">    <span class="comment"># 默认值为7层</span></span><br><span class="line">    <span class="attr">num.levels:</span> <span class="number">7</span></span><br><span class="line">    <span class="comment"># level-0 触发合并的的文件数条件</span></span><br><span class="line">    <span class="attr">level0.file.num.compaction.trigger:</span> <span class="number">4</span></span><br><span class="line">    <span class="comment"># level-0 放慢写入速度的文件数条件</span></span><br><span class="line">    <span class="attr">level0.slowdown.writes.trigger:</span> <span class="number">8</span></span><br><span class="line">    <span class="comment"># level-0 停止写入的文件数条件</span></span><br><span class="line">    <span class="attr">level0.stop.writes.trigger:</span> <span class="number">12</span></span><br><span class="line">    <span class="comment"># 尝试从mem到sst的最大级别</span></span><br><span class="line">    <span class="attr">max.mem.compaction.level:</span> <span class="number">2</span></span><br><span class="line">    <span class="comment"># 目标文件基础大小</span></span><br><span class="line">    <span class="comment"># 如果 target_file_size_base是2MB,</span></span><br><span class="line">    <span class="comment"># target_file_size_multiplier是10，</span></span><br><span class="line">    <span class="comment"># 那么第1级的每个文件都是2MB</span></span><br><span class="line">    <span class="comment"># 第2级的每个文件是20MB</span></span><br><span class="line">    <span class="comment"># 第3级的每个文件是200MB</span></span><br><span class="line">    <span class="attr">target.file.size.base:</span> <span class="string">&amp;target_file_size_base</span> <span class="number">2</span> <span class="string">*</span> <span class="number">1024</span> <span class="string">*</span> <span class="number">1024</span></span><br><span class="line">    <span class="comment"># 目标基础文件倍数</span></span><br><span class="line">    <span class="attr">target.file.size.multiplier:</span> <span class="number">1</span></span><br><span class="line">    <span class="comment"># 所在level所有文件总大小</span></span><br><span class="line">    <span class="comment"># 例如，max_bytes_for_level_base为20MB,</span></span><br><span class="line">    <span class="comment"># max_bytes_for_level_multiplier为10，则第1级的总数据大小为20MB</span></span><br><span class="line">    <span class="comment"># 第2级的总文件大小为200MB</span></span><br><span class="line">    <span class="comment"># 第3级的总文件大小为2GB</span></span><br><span class="line">    <span class="comment"># default: 10M</span></span><br><span class="line">    <span class="attr">max.bytes.for.level.base:</span> <span class="number">10</span> <span class="string">*</span> <span class="number">1024</span> <span class="string">*</span> <span class="number">1024</span></span><br><span class="line">    <span class="comment"># 目标所在level总文件大小</span></span><br><span class="line">    <span class="comment"># default: 10</span></span><br><span class="line">    <span class="attr">max.bytes.for.level.multiplier:</span> <span class="number">10.0</span></span><br><span class="line">    <span class="attr">level.compaction.dynamic.level.bytes:</span> <span class="literal">false</span></span><br><span class="line">    <span class="comment"># 一次性最大打压缩字节</span></span><br><span class="line">    <span class="comment"># Default: target.file.size.base * 25</span></span><br><span class="line">    <span class="attr">max.compaction.bytes:</span> <span class="string">*target_file_size_base</span> <span class="string">*</span> <span class="number">25</span></span><br><span class="line">    <span class="comment"># 软限制：当需要压缩的估计字节数超过这个阈值时，所有的写都会被减速到至少 delayed_write_rate</span></span><br><span class="line">    <span class="comment"># default: 64GB</span></span><br><span class="line">    <span class="attr">soft.pending.compaction.bytes.limit:</span> <span class="number">64</span> <span class="string">*</span> <span class="number">1024</span> <span class="string">*</span> <span class="number">1024</span> <span class="string">*</span> <span class="number">1024</span></span><br><span class="line">    <span class="comment"># 硬限制：当需要压缩的估计字节数超过这个阈值时，所有的写都停止</span></span><br><span class="line">    <span class="comment"># default: 256GB</span></span><br><span class="line">    <span class="attr">hard.pending.compaction.bytes.limit:</span> <span class="number">256</span> <span class="string">*</span> <span class="number">1024</span> <span class="string">*</span> <span class="number">1024</span> <span class="string">*</span> <span class="number">1024</span></span><br><span class="line">    <span class="comment"># 是否使用fsync刷盘</span></span><br><span class="line">    <span class="comment"># default: false</span></span><br><span class="line">    <span class="attr">use.fsync:</span> <span class="literal">false</span></span><br><span class="line">    <span class="comment"># 指定数据库的日志目录的绝对路径，如果为空则和数据放在同一个目录</span></span><br><span class="line">    <span class="comment"># default: empty</span></span><br><span class="line">    <span class="attr">db.log.dir:</span> <span class="string">""</span></span><br><span class="line">    <span class="comment"># 指定数据库的WAL(预写入日志)的目录的绝对路径，如果为空则和数据放在同一个目录</span></span><br><span class="line">    <span class="attr">wal.dir:</span> <span class="string">""</span></span><br><span class="line">    <span class="comment"># 设置过期文件被删除的周期</span></span><br><span class="line">    <span class="comment"># 通过压缩过程超出作用域的文件在每次压缩时仍然会被自动删除，不管这个设置是什么</span></span><br><span class="line">    <span class="comment"># default: 6 hours</span></span><br><span class="line">    <span class="attr">delete.obsolete.files.period.micros:</span> <span class="number">6</span> <span class="string">*</span> <span class="number">60</span> <span class="string">*</span> <span class="number">60</span> <span class="string">*</span> <span class="number">1000</span> <span class="string">*</span> <span class="number">1000</span></span><br><span class="line">    <span class="comment"># 设置后台任务的最大并发数，作用与低优先级线程池</span></span><br><span class="line">    <span class="comment"># default: 1</span></span><br><span class="line">    <span class="attr">max.background.compactions:</span> <span class="number">2</span></span><br><span class="line">    <span class="comment"># 高优先级线程池的后台 memtable 的 flush 任务的最大并发数</span></span><br><span class="line">    <span class="comment"># 默认所有任务都在低优先级池</span></span><br><span class="line">    <span class="comment"># default: 0</span></span><br><span class="line">    <span class="attr">max.background.flushes:</span> <span class="number">0</span></span><br><span class="line">    <span class="comment"># 设置日志文件的最大大小，如果日志文件大于这个值将会被创建一个新的日志文件</span></span><br><span class="line">    <span class="comment"># 如果等于0，则日志只会写入一个日志文件</span></span><br><span class="line">    <span class="comment"># default: 0</span></span><br><span class="line">    <span class="attr">max.log.file.size:</span> <span class="number">0</span></span><br><span class="line">    <span class="comment"># 日志文件滚动的时间(以秒为单位)，日志按一定时间轮转</span></span><br><span class="line">    <span class="comment"># default: 0 (禁用状态)</span></span><br><span class="line">    <span class="attr">log.file.time.to.roll:</span> <span class="number">24</span> <span class="string">*</span> <span class="number">60</span> <span class="string">*</span> <span class="number">60</span></span><br><span class="line">    <span class="comment"># 最多保留的日志文件数</span></span><br><span class="line">    <span class="comment"># default: 1000</span></span><br><span class="line">    <span class="attr">keep.log.file.num:</span> <span class="number">30</span></span><br><span class="line">    <span class="comment"># 软速率限制</span></span><br><span class="line">    <span class="comment"># 当任何level的压缩分数超过soft_rate_limit时，put被延迟0-1毫秒。当 等于 0.0时，此参数将被忽略</span></span><br><span class="line">    <span class="comment"># soft_rate_limit &lt;= hard_rate_limit。如果此约束不存在，RocksDB将设置soft_rate_limit = hard_rate_limit</span></span><br><span class="line">    <span class="comment"># default: 0.0(禁用状态)</span></span><br><span class="line">    <span class="attr">soft.rate.limit:</span> <span class="number">0.0</span></span><br><span class="line">    <span class="comment"># 硬速率限制</span></span><br><span class="line">    <span class="comment"># 当任何level的压缩分数超过hard_rate_limit时，put每次延迟1ms。当 小于等于 1.0 时，此参数被忽略</span></span><br><span class="line">    <span class="comment"># default: 0.0(禁用状态)</span></span><br><span class="line">    <span class="attr">hard.rate.limit:</span> <span class="number">0.0</span></span><br><span class="line">    <span class="comment"># 设置当强制执行hard_rate_limit时，put被停止的最大时间, 0 = 没有限制</span></span><br><span class="line">    <span class="comment"># default: 1000</span></span><br><span class="line">    <span class="attr">rate.limit.delay.max.milliseconds:</span> <span class="number">1000</span></span><br><span class="line">    <span class="comment"># 设置最大清单文件大小，直到滚动为止, 会删除旧的清单文件</span></span><br><span class="line">    <span class="comment"># 默认值:MAX_INT，这样滚动就不会发生</span></span><br><span class="line">    <span class="attr">max.manifest.file.size:</span> <span class="number">1</span><span class="string">&lt;&lt;64</span> <span class="bullet">-</span> <span class="number">1</span></span><br><span class="line">    <span class="comment"># 设置表缓存使用的分片数量</span></span><br><span class="line">    <span class="comment"># default: 4</span></span><br><span class="line">    <span class="attr">table.cache.numshardbits:</span> <span class="number">4</span></span><br><span class="line">    <span class="comment"># 设置扫描过程中的计数限制</span></span><br><span class="line">    <span class="comment"># 在表的LRU缓存数据回收时，严格遵循LRU是低效的，因为这块内存不会真正被释放，除非它的refcount降到零。</span></span><br><span class="line">    <span class="comment"># 相反，进行两次传递:第一次传递将释放refcount = 1的项，如果在扫描该参数指定的元素数量后没有足够的空间释放，将按LRU顺序删除项</span></span><br><span class="line">    <span class="comment"># default: 16</span></span><br><span class="line">    <span class="attr">table.cache.remove.scan.count.limit:</span> <span class="number">16</span></span><br><span class="line">    <span class="comment"># default: 0 (自动计算一个合适的值)</span></span><br><span class="line">    <span class="attr">arena.block.size:</span> <span class="number">0</span></span><br><span class="line">    <span class="comment"># 启用/禁用自动压缩</span></span><br><span class="line">    <span class="comment"># default: false</span></span><br><span class="line">    <span class="attr">disable.auto.compactions:</span> <span class="literal">false</span></span><br><span class="line">    <span class="comment"># 设置 Wal 的恢复模式</span></span><br><span class="line">    <span class="comment"># TolerateCorruptedTailRecordsRecovery = 0 / AbsoluteConsistencyRecovery = 1</span></span><br><span class="line">    <span class="comment"># PointInTimeRecovery = 2 / SkipAnyCorruptedRecordsRecovery = 3</span></span><br><span class="line">    <span class="comment"># default: 0</span></span><br><span class="line">    <span class="attr">w.a.l.recovery.mode:</span> <span class="number">0</span></span><br><span class="line">    <span class="comment"># 设置wal的ttl时间</span></span><br><span class="line">    <span class="comment"># 有2个值影响 归档的 wal 是否会被删除</span></span><br><span class="line">    <span class="comment"># 1。如果两者都设置为0，日志将被尽快删除，并且不会进入存档。</span></span><br><span class="line">    <span class="comment"># 2。如果wal_ttl_seconds为0,wal_size_limit_mb不为0，则每10分钟检查一次WAL文件，如果总大小大于wal_size_limit_mb，则从最早的文件开始删除，直到满足size_limit。所有的空文件将被删除。</span></span><br><span class="line">    <span class="comment"># 3。如果wal_ttl_seconds不为0,wall_size_limit_mb为0，那么每个wal_ttl_seconds / 2都会检查WAL文件，比wal_ttl_seconds老的文件会被删除。</span></span><br><span class="line">    <span class="comment"># 4。如果两个都不是0，则每10分钟检查一次WAL文件，并且两个检查都将在ttl优先的情况下执行。</span></span><br><span class="line">    <span class="comment"># default: 0</span></span><br><span class="line">    <span class="attr">w.a.l.ttl.seconds:</span> <span class="number">0</span></span><br><span class="line">    <span class="comment"># 设置WAL大小限制，单位为MB</span></span><br><span class="line">    <span class="comment"># 如果WAL文件的总大小大于wal_size_limit_mb，则从最早的文件开始删除，直到满足size_limit值为止</span></span><br><span class="line">    <span class="comment"># default: 0</span></span><br><span class="line">    <span class="attr">wal.size.limit.mb:</span> <span class="number">0</span></span><br><span class="line">    <span class="comment"># 允许管道写入</span></span><br><span class="line">    <span class="comment"># default: false</span></span><br><span class="line">    <span class="attr">enable.pipelined.write:</span> <span class="literal">false</span></span><br><span class="line">    <span class="comment"># 设置预分配(通过fallocate) manifest文件的字节数</span></span><br><span class="line">    <span class="comment"># 默认值是4mb，这对于减少随机IO以及防止预分配大量数据的挂载(例如xfs的allocsize选项)过度分配是合理的</span></span><br><span class="line">    <span class="comment"># default: 4mb</span></span><br><span class="line">    <span class="attr">manifest.preallocation.size:</span> <span class="number">1024</span> <span class="string">*</span> <span class="number">1024</span> <span class="string">*</span> <span class="number">4</span></span><br><span class="line">    <span class="comment"># 当memtable被刷新到存储中时[启用|禁用] 清除 [重复\被删除]的 键</span></span><br><span class="line">    <span class="comment"># default: true</span></span><br><span class="line">    <span class="attr">purge.redundant.kvs.while.flush:</span> <span class="literal">true</span></span><br><span class="line">    <span class="comment"># 开启/关闭sst表的mmap读功能</span></span><br><span class="line">    <span class="comment"># default: false</span></span><br><span class="line">    <span class="attr">allow.mmap.reads:</span> <span class="literal">false</span></span><br><span class="line">    <span class="comment"># 开启/关闭sst表的mmap写功能</span></span><br><span class="line">    <span class="comment"># default: false</span></span><br><span class="line">    <span class="attr">allow.mmap.writes:</span> <span class="literal">false</span></span><br><span class="line">    <span class="comment"># 启用/禁用读操作的直接I/O模式(O_DIRECT)</span></span><br><span class="line">    <span class="comment"># default: false</span></span><br><span class="line">    <span class="attr">use.direct.reads:</span> <span class="literal">false</span></span><br><span class="line">    <span class="comment"># 启用/禁用后台flush和compaction的直接I/O模式(O_DIRECT)</span></span><br><span class="line">    <span class="comment"># 当为true时，new_table_reader_for_compaction_inputs被强制为true。</span></span><br><span class="line">    <span class="comment"># default: false</span></span><br><span class="line">    <span class="attr">use.direct.i.o.for.flush.and.compaction:</span> <span class="literal">false</span></span><br><span class="line">    <span class="comment"># [开启|禁用] 子进程继承打开的文件</span></span><br><span class="line">    <span class="comment"># default: true</span></span><br><span class="line">    <span class="attr">is.fd.close.on.exec:</span> <span class="literal">true</span></span><br><span class="line">    <span class="comment"># [启用|禁用]在恢复时跳过日志损坏错误(如果客户端可以丢失最近的更改)</span></span><br><span class="line">    <span class="comment"># default: false</span></span><br><span class="line">    <span class="attr">skip.log.error.on.recovery:</span> <span class="literal">false</span></span><br><span class="line">    <span class="comment"># 设置统计转储周期，以秒为单位</span></span><br><span class="line">    <span class="comment"># default: 3600 (1 hour)</span></span><br><span class="line">    <span class="attr">stats.dump.period.sec:</span> <span class="number">3600</span></span><br><span class="line">    <span class="comment"># 当打开sst文件时，是否会提示底层文件系统文件访问模式是随机的</span></span><br><span class="line">    <span class="comment"># default: true</span></span><br><span class="line">    <span class="attr">advise.random.on.open:</span> <span class="literal">true</span></span><br><span class="line">    <span class="comment"># 设置所有列族写入磁盘之前在memtables中建立的数据量。</span></span><br><span class="line">    <span class="comment"># 这与write_buffer_size不同，后者强制对单个memtable进行限制</span></span><br><span class="line">    <span class="comment"># default: 0(禁用)</span></span><br><span class="line">    <span class="attr">db.write.buffer.size:</span> <span class="number">0</span></span><br><span class="line">    <span class="comment"># 压缩启动后的文件访问模式</span></span><br><span class="line">    <span class="comment"># NoneCompactionAccessPattern = 0, NormalCompactionAccessPattern = 1</span></span><br><span class="line">    <span class="comment"># SequentialCompactionAccessPattern = 2, WillneedCompactionAccessPattern = 3</span></span><br><span class="line">    <span class="comment"># default: NormalCompactionAccessPattern</span></span><br><span class="line">    <span class="attr">access.hint.on.compaction.start:</span> <span class="number">1</span></span><br><span class="line">    <span class="comment"># 启用/禁用自适应互斥锁，它在求助于内核之前在用户空间旋转</span></span><br><span class="line">    <span class="comment"># 当互斥锁不是严重竞争时，可以减少上下文切换。但是，如果互斥对象是热的，最终可能会浪费旋转时间</span></span><br><span class="line">    <span class="comment"># default: false</span></span><br><span class="line">    <span class="attr">use.adaptive.mutex:</span> <span class="literal">false</span></span><br><span class="line">    <span class="comment"># 允许操作系统在后台异步写入文件时增量同步文件到磁盘</span></span><br><span class="line">    <span class="comment"># 对每写一个bytes_per_sync发出一个请求。</span></span><br><span class="line">    <span class="comment"># default: 0(禁用)</span></span><br><span class="line">    <span class="attr">bytes.per.sync:</span> <span class="number">0</span></span><br><span class="line">    <span class="comment"># 设置压缩样式</span></span><br><span class="line">    <span class="comment"># LevelCompactionStyle = 0 / UniversalCompactionStyle = 1 / FIFOCompactionStyle = 2</span></span><br><span class="line">    <span class="comment"># default: LevelCompactionStyle</span></span><br><span class="line">    <span class="attr">compaction.style:</span> <span class="number">0</span></span><br><span class="line">    <span class="comment"># 指定迭代-&gt;Next()是否按顺序跳过具有相同user-key的键</span></span><br><span class="line">    <span class="comment"># 这个数字指定在重寻之前将被连续跳过的键数(与userkey相同)</span></span><br><span class="line">    <span class="comment"># default: 8</span></span><br><span class="line">    <span class="attr">max.sequential.skip.in.iterations:</span> <span class="number">8</span></span><br><span class="line">    <span class="comment">#[启用|禁用]线程安全的就地更新</span></span><br><span class="line">    <span class="comment"># default: false</span></span><br><span class="line">    <span class="attr">inplace.update.support:</span> <span class="literal">false</span></span><br><span class="line">    <span class="comment"># 用于就地更新的锁的数量</span></span><br><span class="line">    <span class="comment"># default: 0, ，如果 inplace_update_support = true ，则为 10000</span></span><br><span class="line">    <span class="attr">inplace.update.num.locks:</span> <span class="number">0</span></span><br><span class="line">    <span class="comment"># 设置memtable使用的arena的大页大小</span></span><br><span class="line">    <span class="comment"># 如果&lt;=0，它不会从大页分配，而是从malloc分配。用户有责任为它预留巨大的页面以供分配。</span></span><br><span class="line">    <span class="comment"># 例如:sysctl -w vm.nr_hugepages=20</span></span><br><span class="line">    <span class="comment"># 参见linux doc Documentation/vm/hugetlbpage.txt</span></span><br><span class="line">    <span class="comment"># 如果没有足够的空闲大页，它会退回到malloc</span></span><br><span class="line">    <span class="comment"># 通过SetOptions() API动态更改</span></span><br><span class="line">    <span class="attr">memtable.huge.page.size:</span> <span class="number">0</span></span><br><span class="line">    <span class="comment"># 设置布隆过滤器的指针位置</span></span><br><span class="line">    <span class="comment"># 控制bloom filter探针的位置，以提高缓存遗漏率。</span></span><br><span class="line">    <span class="comment"># 该选项仅适用于memtable前缀bloom和plain前缀bloom。</span></span><br><span class="line">    <span class="comment"># 它本质上限制了每个bloom filter检查可以触及的缓存线的最大数量。</span></span><br><span class="line">    <span class="comment"># 设置为0时，此优化被关闭。这个数目不应该大于探测的数目。</span></span><br><span class="line">    <span class="comment"># 这个选项可以提高内存工作负载的性能，但应该小心使用，因为它可能会导致更高的误报率。</span></span><br><span class="line">    <span class="comment"># default: 0</span></span><br><span class="line">    <span class="attr">bloom.locality:</span> <span class="number">0</span></span><br><span class="line">    <span class="comment"># 设置memtable中一个键的最大连续合并操作数</span></span><br><span class="line">    <span class="comment"># default: 0 (禁用状态)</span></span><br><span class="line">    <span class="attr">max.successive.merges:</span> <span class="number">0</span></span><br><span class="line">    <span class="comment"># 开启统计</span></span><br><span class="line">    <span class="comment"># default: 无参数，false代表不启动，true则会调用对应的api</span></span><br><span class="line">    <span class="attr">enable.statistics:</span> <span class="literal">false</span></span><br><span class="line">    <span class="comment"># 预加载数据库为了批量加载</span></span><br><span class="line">    <span class="comment"># 所有数据将在0级没有任何自动压缩</span></span><br><span class="line">    <span class="comment"># 建议在从数据库读取数据之前手动调用CompactRange(NULL, NULL)，否则读取速度会非常慢</span></span><br><span class="line">    <span class="comment"># default: 无参数，false代表不启动，true则会调用对应的api</span></span><br><span class="line">    <span class="attr">prepare.for.bulk.load:</span> <span class="literal">false</span></span><br><span class="line">    <span class="comment"># 设置一个MemTableRep，它由一个向量支持</span></span><br><span class="line">    <span class="comment"># 在迭代时，向量被排序，这对于迭代非常少且读操作开始后通常不执行写操作的工作负载非常有用</span></span><br><span class="line">    <span class="comment"># default: 无参数，false代表不启动，true则会调用对应的api</span></span><br><span class="line">    <span class="attr">memtable.vector.rep:</span> <span class="literal">false</span></span><br><span class="line">    <span class="comment"># 如果不存在列族是否自动建立</span></span><br><span class="line">    <span class="comment"># default: true</span></span><br><span class="line">    <span class="attr">create.if.missing.column.families:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure><h2 id="SQL-AST的支持"><a href="#SQL-AST的支持" class="headerlink" title="SQL-AST的支持"></a>SQL-AST的支持</h2><p>我们的db希望能做到与语言无关，不仅仅是我们的目前的golang，就算是php也可以用到本地持久化的方式的话，就需要借助<code>RPC协议</code>或者<code>特定DSL</code>来实现，但是既然是数据库，这里优先选择了以<code>sql语法</code>来管理数据。</p><p>那么我们就需要拿到<code>sql</code>的抽象语法树(<code>sql-ast</code>)，拿到<code>sql-ast</code>之后，我们就可以拿到我们所需要的信息去<code>hit data</code>。</p><p><a href="github.com/pingcap/parser">Pingcap-parser</a></p><p>这里我们用到了pingcap公司的<code>parser</code>库，该库同样是<code>TiDB</code>的sql解析库，借助该库，我们可以很方便的拿到<code>sql-ast</code></p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">"fmt"</span></span><br><span class="line"><span class="string">"github.com/pingcap/parser"</span></span><br><span class="line">_ <span class="string">"github.com/pingcap/parser/test_driver"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> visitor <span class="keyword">struct</span> &#123;</span><br><span class="line">table  <span class="keyword">string</span></span><br><span class="line">fields []<span class="keyword">string</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(v *visitor)</span> <span class="title">Enter</span><span class="params">(in ast.Node)</span> <span class="params">(out ast.Node, skipChildren <span class="keyword">bool</span>)</span></span> &#123;</span><br><span class="line"><span class="comment">//fmt.Printf("Enter: %T\n", in)</span></span><br><span class="line"><span class="keyword">switch</span> n := in.(<span class="keyword">type</span>) &#123;</span><br><span class="line"><span class="keyword">case</span> *ast.SelectStmt:</span><br><span class="line"><span class="keyword">case</span> *ast.FieldList:</span><br><span class="line"><span class="keyword">case</span> *ast.SelectField:</span><br><span class="line"><span class="keyword">case</span> *ast.ColumnNameExpr:</span><br><span class="line"><span class="comment">//fmt.Printf("Enter: %v\n", n.Name)</span></span><br><span class="line"><span class="keyword">case</span> *ast.ColumnName:</span><br><span class="line"><span class="comment">//v.fields = append(v.fields, n.Name.L)</span></span><br><span class="line"><span class="keyword">case</span> *ast.TableName:</span><br><span class="line"><span class="comment">//v.table = n.Name.L</span></span><br><span class="line"><span class="keyword">case</span> *ast.BinaryOperationExpr:</span><br><span class="line"><span class="comment">//fmt.Printf("Enter: %v\n", n.Op)</span></span><br><span class="line"><span class="keyword">case</span> *ast.Join:</span><br><span class="line"><span class="comment">//fmt.Printf("Enter: %v\n", n.Left)</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> in, <span class="literal">false</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(v *visitor)</span> <span class="title">Leave</span><span class="params">(in ast.Node)</span> <span class="params">(out ast.Node, ok <span class="keyword">bool</span>)</span></span> &#123;</span><br><span class="line">fmt.Printf(<span class="string">"Leave: %T\n"</span>, in)</span><br><span class="line"></span><br><span class="line"><span class="keyword">switch</span> n := in.(<span class="keyword">type</span>) &#123;</span><br><span class="line"><span class="keyword">case</span> *ast.SelectStmt:</span><br><span class="line"><span class="keyword">case</span> *ast.FieldList:</span><br><span class="line"><span class="keyword">case</span> *ast.SelectField:</span><br><span class="line"><span class="keyword">case</span> *ast.ColumnNameExpr:</span><br><span class="line"><span class="keyword">case</span> *ast.ColumnName:</span><br><span class="line"><span class="keyword">case</span> *ast.TableName:</span><br><span class="line">v.table = n.Name.L</span><br><span class="line"><span class="keyword">case</span> *ast.BinaryOperationExpr:</span><br><span class="line"><span class="comment">//fmt.Printf("Leave: %v\n", n.L)</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> in, <span class="literal">true</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">p := parser.New()</span><br><span class="line"></span><br><span class="line">sql := <span class="string">"SELECT emp_no, first_name, last_name "</span> +</span><br><span class="line"><span class="string">"FROM employees "</span> +</span><br><span class="line"><span class="string">"where id='Aamodt' and (create_time &gt; 0 or last_name ='caiwenhui')"</span></span><br><span class="line">stmtNodes, _, err := p.Parse(sql, <span class="string">""</span>, <span class="string">""</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">fmt.Printf(<span class="string">"parse error:\n%v\n%s"</span>, err, sql)</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _, stmtNode := <span class="keyword">range</span> stmtNodes &#123;</span><br><span class="line">v := visitor&#123;&#125;</span><br><span class="line">stmtNode.Accept(&amp;v)</span><br><span class="line">fmt.Printf(<span class="string">"%v\n"</span>, v)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里用到了<code>github.com/pingcap/parser/test_driver</code> 的原因是因为该库和tidb的driver存在依赖关系，tidb在设计的时候，并未做到很好的分离，所以当其他项目需要使用该库的时候，需要引入这个驱动。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Visitor visits a Node.</span></span><br><span class="line"><span class="keyword">type</span> Visitor <span class="keyword">interface</span> &#123;</span><br><span class="line"><span class="comment">// Enter is called before children nodes are visited.</span></span><br><span class="line"><span class="comment">// The returned node must be the same type as the input node n.</span></span><br><span class="line"><span class="comment">// skipChildren returns true means children nodes should be skipped,</span></span><br><span class="line"><span class="comment">// this is useful when work is done in Enter and there is no need to visit children.</span></span><br><span class="line">Enter(n Node) (node Node, skipChildren <span class="keyword">bool</span>)</span><br><span class="line"><span class="comment">// Leave is called after children nodes have been visited.</span></span><br><span class="line"><span class="comment">// The returned node's type can be different from the input node if it is a ExprNode,</span></span><br><span class="line"><span class="comment">// Non-expression node must be the same type as the input node n.</span></span><br><span class="line"><span class="comment">// ok returns false to stop visiting.</span></span><br><span class="line">Leave(n Node) (node Node, ok <span class="keyword">bool</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>并且这里，我们看到有一个结构体<code>visitor</code>，该结构体就是用来访问<code>ast</code>用的，因为 <code>tidb</code>的<code>parser库</code> 和<code>阿里巴巴</code> 的 <code>druid sql</code> 类似，都是采用 访问器的方式来遍历 <code>ast</code>的，所以我们只需要定义好我们的访问器，那么就可以访问对应的结构数据。<br>至于访问器的接口如上图，只有2个API，一个是 <code>Enter(n Node) (node Node, skipChildren bool)</code>，另外一个是 <code>Leave(n Node) (node Node, ok bool)</code> 。2个接口返回的第二个参数分别定义为 <code>是否跳过剩下的节点</code>, <code>是否成功退出节点</code>。</p><h3 id="interface在这里的应用"><a href="#interface在这里的应用" class="headerlink" title="interface在这里的应用"></a>interface在这里的应用</h3><p>在parser中，大量运用了interface, 充分的给我们的展示了golang的<code>组合</code>特性。</p><p>例如：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Node is the basic element of the AST.</span></span><br><span class="line"><span class="comment">// Interfaces embed Node should have 'Node' name suffix.</span></span><br><span class="line"><span class="keyword">type</span> Node <span class="keyword">interface</span> &#123;</span><br><span class="line"><span class="comment">// Restore returns the sql text from ast tree</span></span><br><span class="line">Restore(ctx *format.RestoreCtx) error</span><br><span class="line"><span class="comment">// Accept accepts Visitor to visit itself.</span></span><br><span class="line"><span class="comment">// The returned node should replace original node.</span></span><br><span class="line"><span class="comment">// ok returns false to stop visiting.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// Implementation of this method should first call visitor.Enter,</span></span><br><span class="line"><span class="comment">// assign the returned node to its method receiver, if skipChildren returns true,</span></span><br><span class="line"><span class="comment">// children should be skipped. Otherwise, call its children in particular order that</span></span><br><span class="line"><span class="comment">// later elements depends on former elements. Finally, return visitor.Leave.</span></span><br><span class="line">Accept(v Visitor) (node Node, ok <span class="keyword">bool</span>)</span><br><span class="line"><span class="comment">// Text returns the original text of the element.</span></span><br><span class="line">Text() <span class="keyword">string</span></span><br><span class="line"><span class="comment">// SetText sets original text to the Node.</span></span><br><span class="line">SetText(text <span class="keyword">string</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// SelectStmt represents the select query node.</span></span><br><span class="line"><span class="comment">// See https://dev.mysql.com/doc/refman/5.7/en/select.html</span></span><br><span class="line"><span class="keyword">type</span> SelectStmt <span class="keyword">struct</span> &#123;</span><br><span class="line">dmlNode</span><br><span class="line">resultSetNode</span><br><span class="line"></span><br><span class="line"><span class="comment">// SelectStmtOpts wraps around select hints and switches.</span></span><br><span class="line">*SelectStmtOpts</span><br><span class="line"><span class="comment">// Distinct represents whether the select has distinct option.</span></span><br><span class="line">Distinct <span class="keyword">bool</span></span><br><span class="line"><span class="comment">// From is the from clause of the query.</span></span><br><span class="line">From *TableRefsClause</span><br><span class="line"><span class="comment">// Where is the where clause in select statement.</span></span><br><span class="line">Where ExprNode</span><br><span class="line"><span class="comment">// Fields is the select expression list.</span></span><br><span class="line">Fields *FieldList</span><br><span class="line"><span class="comment">// GroupBy is the group by expression list.</span></span><br><span class="line">GroupBy *GroupByClause</span><br><span class="line"><span class="comment">// Having is the having condition.</span></span><br><span class="line">Having *HavingClause</span><br><span class="line"><span class="comment">// WindowSpecs is the window specification list.</span></span><br><span class="line">WindowSpecs []WindowSpec</span><br><span class="line"><span class="comment">// OrderBy is the ordering expression list.</span></span><br><span class="line">OrderBy *OrderByClause</span><br><span class="line"><span class="comment">// Limit is the limit clause.</span></span><br><span class="line">Limit *Limit</span><br><span class="line"><span class="comment">// LockTp is the lock type</span></span><br><span class="line">LockTp SelectLockType</span><br><span class="line"><span class="comment">// TableHints represents the table level Optimizer Hint for join type</span></span><br><span class="line">TableHints []*TableOptimizerHint</span><br><span class="line"><span class="comment">// IsAfterUnionDistinct indicates whether it's a stmt after "union distinct".</span></span><br><span class="line">IsAfterUnionDistinct <span class="keyword">bool</span></span><br><span class="line"><span class="comment">// IsInBraces indicates whether it's a stmt in brace.</span></span><br><span class="line">IsInBraces <span class="keyword">bool</span></span><br><span class="line"><span class="comment">// QueryBlockOffset indicates the order of this SelectStmt if counted from left to right in the sql text.</span></span><br><span class="line">QueryBlockOffset <span class="keyword">int</span></span><br><span class="line"><span class="comment">// SelectIntoOpt is the select-into option.</span></span><br><span class="line">SelectIntoOpt *SelectIntoOption</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">splitWhere</span><span class="params">(where ast.ExprNode)</span> []<span class="title">ast</span>.<span class="title">ExprNode</span></span> &#123;</span><br><span class="line"><span class="keyword">var</span> conditions []ast.ExprNode</span><br><span class="line"><span class="keyword">switch</span> x := where.(<span class="keyword">type</span>) &#123;</span><br><span class="line"><span class="keyword">case</span> <span class="literal">nil</span>:</span><br><span class="line"><span class="keyword">case</span> *ast.BinaryOperationExpr:</span><br><span class="line"><span class="keyword">if</span> x.Op == opcode.LogicAnd &#123;</span><br><span class="line">conditions = <span class="built_in">append</span>(conditions, splitWhere(x.L)...)</span><br><span class="line">conditions = <span class="built_in">append</span>(conditions, splitWhere(x.R)...)</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">conditions = <span class="built_in">append</span>(conditions, x)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">case</span> *ast.ParenthesesExpr:</span><br><span class="line">conditions = <span class="built_in">append</span>(conditions, splitWhere(x.Expr)...)</span><br><span class="line"><span class="keyword">default</span>:</span><br><span class="line">conditions = <span class="built_in">append</span>(conditions, where)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> conditions</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>ast.Node</code> 是ast的基础接口，所有的节点都需要在此之上实现自己的功能。其他接口同理，一环扣一环，设计得十分巧妙。</p><h2 id="KEY-VALUE的编码规则"><a href="#KEY-VALUE的编码规则" class="headerlink" title="KEY-VALUE的编码规则"></a>KEY-VALUE的编码规则</h2><p>DB 对每个表分配一个 TableID，每一个索引都会分配一个 IndexID，每一行分配一个 RowID， 其中 DbId/TableID 在整个集群内唯一，IndexID/RowID 在表内唯一，这些 ID 都是 int64 类型。</p><p>其中细节如下：</p><p>database 编码成 Key-Value pair：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Key: metaPrefix(+)databasePrefix&#123;dbID&#125;</span><br><span class="line">Value: database struct json marshal</span><br></pre></td></tr></table></figure><p>database indexed 编码成 Key-Value pair：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Key: metaPrefix(+)databasePrefix_indexPrefix&#123;database_name&#125;</span><br><span class="line">Value: dbID</span><br></pre></td></tr></table></figure><p>table 编码成 Key-Value pair：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Key: metaPrefix(+)tablePrefix&#123;dbID&#125;_recordPrefixSep&#123;tableID&#125;</span><br><span class="line">Value: table struct json marshal</span><br></pre></td></tr></table></figure><p>table indexed 编码成 Key-Value pair：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Key: metaPrefix(+)tablePrefix&#123;dbID&#125;_indexPrefix&#123;databaseId&#125;&#123;table_name&#125;</span><br><span class="line">Value: tableID</span><br></pre></td></tr></table></figure><p>每行数据按照如下规则进行编码成 Key-Value pair：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Key: databasePrefix&#123;dbID&#125;_tablePrefix&#123;tableID&#125;_recordPrefixSep&#123;rowID&#125;</span><br><span class="line">Value: [col1, col2, col3, col4]</span><br></pre></td></tr></table></figure><p>对于 Unique Index 数据，会按照如下规则编码成 Key-Value pair：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Key: databasePrefix&#123;dbID&#125;_tablePrefix&#123;tableID&#125;_indexPrefixSep&#123;indexID&#125;_indexedColumnsValue</span><br><span class="line">Value: rowID</span><br></pre></td></tr></table></figure><p>Index 数据还需要考虑 Unique Index 和非 Unique Index 两种情况，对于 Unique Index，可以按照上述编码规则。 但是对于非 Unique Index，通过这种编码并不能构造出唯一的 Key，因为同一个<br>Index 的 <code>databasePrefix{dbID}_tablePrefix{tableID}_indexPrefixSep{indexID}</code>都一样，可能有多行数据的 ColumnsValue 是一样的.</p><p>对于 “非” Unique Index 的编码做了一点调整：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Key: databasePrefix&#123;dbID&#125;_tablePrefix&#123;tableID&#125;_indexPrefixSep&#123;indexID&#125;_indexedColumnsValue_&#123;rowID&#125;</span><br><span class="line">Value: null</span><br></pre></td></tr></table></figure><p>对应的标识符如下定义：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> (</span><br><span class="line">databasePrefix  = []<span class="keyword">byte</span>&#123;<span class="string">'d'</span>&#125;</span><br><span class="line">tablePrefix     = []<span class="keyword">byte</span>&#123;<span class="string">'t'</span>&#125;</span><br><span class="line">recordPrefixSep = []<span class="keyword">byte</span>(<span class="string">"_r"</span>)</span><br><span class="line">indexPrefixSep  = []<span class="keyword">byte</span>(<span class="string">"_i"</span>)</span><br><span class="line">metaPrefix      = []<span class="keyword">byte</span>&#123;<span class="string">'m'</span>&#125;</span><br><span class="line">sepPrefix       = []<span class="keyword">byte</span>&#123;<span class="string">'_'</span>&#125;</span><br><span class="line"></span><br><span class="line">mdPrefix  = <span class="built_in">append</span>(metaPrefix, databasePrefix...)</span><br><span class="line">mdiPrefix = <span class="built_in">append</span>(<span class="built_in">append</span>(metaPrefix, databasePrefix...), indexPrefixSep...)</span><br><span class="line">mtPrefix  = <span class="built_in">append</span>(metaPrefix, tablePrefix...)</span><br><span class="line">mtiPrefix = <span class="built_in">append</span>(<span class="built_in">append</span>(metaPrefix, tablePrefix...), indexPrefixSep...)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> (</span><br><span class="line">idLen                = <span class="number">8</span></span><br><span class="line">sepPrefixLen         = <span class="number">1</span></span><br><span class="line">prefixLen            = databasePrefixLength + idLen <span class="comment">/*dbID*/</span> + sepPrefixLen + tablePrefixLength + idLen <span class="comment">/*tableID*/</span> + recordPrefixSepLength</span><br><span class="line">uniqPrefixLen        = databasePrefixLength + idLen <span class="comment">/*dbID*/</span> + sepPrefixLen + tablePrefixLength + idLen <span class="comment">/*tableID*/</span> + indexPrefixSepLength + idLen <span class="comment">/*indexID*/</span> + sepPrefixLen <span class="comment">/* +indexedColumnsValue */</span></span><br><span class="line">indexPrefixLen       = databasePrefixLength + idLen <span class="comment">/*dbID*/</span> + sepPrefixLen + tablePrefixLength + idLen <span class="comment">/*tableID*/</span> + indexPrefixSepLength + idLen <span class="comment">/*indexID*/</span> + sepPrefixLen + sepPrefixLen</span><br><span class="line">indexPrefixLenWithID = databasePrefixLength + idLen <span class="comment">/*dbID*/</span> + sepPrefixLen + tablePrefixLength + idLen <span class="comment">/*tableID*/</span> + indexPrefixSepLength + idLen <span class="comment">/*indexID*/</span> + sepPrefixLen + sepPrefixLen + idLen</span><br><span class="line"><span class="comment">// RecordRowKeyLen is public for calculating avgerage row size.</span></span><br><span class="line">RecordRowKeyLen       = prefixLen + idLen <span class="comment">/*handle*/</span></span><br><span class="line">tablePrefixLength     = <span class="number">1</span></span><br><span class="line">databasePrefixLength  = <span class="number">1</span></span><br><span class="line">recordPrefixSepLength = <span class="number">2</span></span><br><span class="line">indexPrefixSepLength  = <span class="number">2</span></span><br><span class="line">metaPrefixLength      = <span class="number">1</span></span><br><span class="line">mdPrefixLen           = metaPrefixLength + databasePrefixLength</span><br><span class="line">mdiPrefixLen          = mdPrefixLen + indexPrefixSepLength</span><br><span class="line">mtPrefixLen           = metaPrefixLength + tablePrefixLength</span><br><span class="line">mtiPrefixLen          = mtPrefixLen + indexPrefixSepLength</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>我们把rowkey的编码规则来看</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// EncodeRowKey encodes the table id and record handle into a kv.Key</span></span><br><span class="line"><span class="comment">// EncodeRowKey databasePrefix&#123;dbID&#125;_tablePrefix&#123;tableID&#125;_recordPrefixSep&#123;rowID&#125;</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">EncodeRowKey</span><span class="params">(databaseId, tableID, rowId <span class="keyword">int64</span>)</span> <span class="title">kv</span>.<span class="title">Key</span></span> &#123;</span><br><span class="line">buf := <span class="built_in">make</span>([]<span class="keyword">byte</span>, <span class="number">0</span>, prefixLen+idLen <span class="comment">/*rowId*/</span>)</span><br><span class="line">buf = <span class="built_in">append</span>(buf, databasePrefix...)</span><br><span class="line">buf = <span class="built_in">append</span>(buf, EncodeIdBuf(databaseId)...)</span><br><span class="line">buf = <span class="built_in">append</span>(buf, sepPrefix...)</span><br><span class="line">buf = <span class="built_in">append</span>(buf, tablePrefix...)</span><br><span class="line">buf = <span class="built_in">append</span>(buf, EncodeIdBuf(tableID)...)</span><br><span class="line">buf = <span class="built_in">append</span>(buf, recordPrefixSep...)</span><br><span class="line">buf = <span class="built_in">append</span>(buf, EncodeIdBuf(rowId)...)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> buf</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">EncodeIdBuf</span><span class="params">(id <span class="keyword">int64</span>)</span> <span class="title">kv</span>.<span class="title">Key</span></span> &#123;</span><br><span class="line"><span class="keyword">var</span> buf = <span class="built_in">make</span>([]<span class="keyword">byte</span>, <span class="number">8</span>)</span><br><span class="line">binary.BigEndian.PutUint64(buf[:], <span class="keyword">uint64</span>(id))</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> buf</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">DecodeIdBuf</span><span class="params">(b []<span class="keyword">byte</span>)</span> <span class="title">int64</span></span> &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">int64</span>(binary.BigEndian.Uint64(b))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里我们通过<code>EncodeRowKey(databaseId, tableID, rowId int64) kv.Key</code> 来生成数据的<code>row-key</code>，我们利用<code>make([]byte,0, len)</code> 的方式预申请内存的方式，后面再通过append的方式往 <code>slice</code> 中不断追加字节，当遇到<code>int64</code>的数据的时候，我们会调用<code>EncodeIdBuf(id int64) kv.Key</code> 来把int64转换为 <code>大端(网络字节序)</code> 的二进制字节。最后一个row-key就生成了。</p><h3 id="database-编码"><a href="#database-编码" class="headerlink" title="database 编码"></a>database 编码</h3><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> database <span class="keyword">struct</span> &#123;</span><br><span class="line">Id   <span class="keyword">int64</span></span><br><span class="line">Name <span class="keyword">string</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// createDatabaseHandle Create database</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(s *Store)</span> <span class="title">createDatabaseHandle</span><span class="params">(result *Result, stmt *ast.CreateDatabaseStmt)</span></span> &#123;</span><br><span class="line">indexedKey := etccodec.EncodeDatabaseMetaIndexedKey([]<span class="keyword">byte</span>(stmt.Name))</span><br><span class="line">rdb := rocksdb.Load().(*Rocksdb)</span><br><span class="line">slice, err := rdb.Get(rdb.NewDefaultReadOptions(), indexedKey)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">errorlog(UnexpectErrorCategory&#123;&#125;, UnknowRCode)</span><br><span class="line">result.Record(UnknowRCode, <span class="literal">nil</span>)</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> slice.Exists() &#123;</span><br><span class="line"><span class="keyword">if</span> !stmt.IfNotExists &#123;</span><br><span class="line">errorlog(UnexpectErrorCategory&#123;&#125;, DatabaseExistsRCode)</span><br><span class="line">result.Record(DatabaseExistsRCode, <span class="literal">nil</span>)</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">result.Success()</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">dbId, err := getDatabaseId()</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">msg := fmt.Sprintf(<span class="string">"get database id failed, err: %s"</span>, err)</span><br><span class="line">errorlog(UnexpectErrorCategory&#123;&#125;, msg)</span><br><span class="line">result.Record(CreateDatabaseFailedRCode, &amp;msg)</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line">db := &amp;database&#123;</span><br><span class="line">Id:   dbId,</span><br><span class="line">Name: stmt.Name,</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">var</span> buf = etccodec.EncodeIdBuf(dbId)</span><br><span class="line">key := etccodec.EncodeDatabaseMetaKey(dbId)</span><br><span class="line">value, err := json.Marshal(db)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">msg := fmt.Sprintf(<span class="string">"marshal database error, err: %s"</span>, err)</span><br><span class="line">errorlog(UnexpectErrorCategory&#123;&#125;, msg)</span><br><span class="line">result.Record(CreateDatabaseFailedRCode, &amp;msg)</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">err = rdb.Put(key, value)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">msg := fmt.Sprintf(<span class="string">"rocksdb put metadata failed, err: %s"</span>, err)</span><br><span class="line">errorlog(UnexpectErrorCategory&#123;&#125;, msg)</span><br><span class="line">result.Record(CreateDatabaseFailedRCode, &amp;msg)</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line">err = rdb.Put(indexedKey, buf)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">msg := fmt.Sprintf(<span class="string">"rocksdb put indexed failed, err: %s"</span>, err)</span><br><span class="line">errorlog(UnexpectErrorCategory&#123;&#125;, msg)</span><br><span class="line">result.Record(CreateDatabaseFailedRCode, &amp;msg)</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line">debugf(NormalDebugCategory&#123;&#125;, <span class="string">"create database [%s]"</span>, db)</span><br><span class="line">result.Success()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里，我们借助 <code>create database stmt</code> 来的处理方法来看看 <code>db Key-Value pair</code> 的处理逻辑。<br>我们看到这里，我们通过一个<code>stmt.Name</code> 来拿到数据库的名，并且调用<code>etccodec.EncodeDatabaseMetaIndexedKey([]byte(stmt.Name))</code> 方法来创建符合<code>metaPrefix(+)databasePrefix_indexPrefix{database_name}</code> 索引的key，然后判断是否存在所以索引来判断后续的逻辑。<br>我们通过一个 <code>getDatabaseId()</code> 方法来获取一个全局的数据库id，并且初始化<code>type database struct</code>，然后我们调用了 <code>etccodec.EncodeDatabaseMetaKey(dbid)</code> 来对key进行生成，也就是上面所列出来的 <code>metaPrefix(+)databasePrefix{dbID}</code>, 接下来就是<code>value</code>的生成，这里的value比较直接，就是<code>json.marshal</code>来处理后的字节。然后我们把数据<code>put</code> 到<code>rocksdb</code>就结束了，索引数据也是如此，不过索引存储的dbid。</p><blockquote><p>table的编码也类似</p></blockquote><p>如果对其他的<code>stmt</code>，例如<code>insert stmt/delete stmt</code>具体的逻辑感兴趣的话，可以查阅源码，但是类似差不多。</p><p>todo : 画图</p><h2 id="COUNTER计数器-发号器"><a href="#COUNTER计数器-发号器" class="headerlink" title="COUNTER计数器-发号器"></a>COUNTER计数器-发号器</h2><p>这里我们需要讲一下<code>counter</code>，因为我们所有的数据都会有<code>row_id</code>，并且我们在<code>create table</code>的时候也有<code>AUTO_INCREMENT</code>的列，这个时候，我们也是需要一个ID发号器。</p><p>目前常见的发号器实现方案如下：</p><ul><li><ol><li>UUID</li></ol></li><li><ol start="2"><li>snowflake</li></ol></li><li><ol start="3"><li>数据库生成</li></ol></li><li><ol start="4"><li>美团的Leaf（基于数据库）</li></ol></li></ul><h3 id="UUID"><a href="#UUID" class="headerlink" title="UUID"></a>UUID</h3><p>UUID(Universally Unique Identifier)的标准型式包含<code>32个16进制</code>数字，以连字号分为<code>五段</code>，形式为<code>8-4-4-4-12</code>的36个字符，示例：<code>550e8400-e29b-41d4-a716-446655440000</code>，到目前为止业界一共有5种方式生成UUID，详情见IETF发布的UUID规范 <a href="https://www.ietf.org/rfc/rfc4122.txt" target="_blank" rel="noopener">A Universally Unique IDentifier (UUID) URN Namespace</a></p><p><img src="/images/%E6%95%B0%E6%8D%AE%E5%BA%93/uuid.png" alt=" UUID "></p><blockquote><p>由于他的无序性，不符合我们所期待的增长序列，所以抛弃</p></blockquote><h3 id="类snowflake方案"><a href="#类snowflake方案" class="headerlink" title="类snowflake方案"></a>类snowflake方案</h3><p>这种方案大致来说是一种以划分命名空间（UUID也算，由于比较常见，所以单独分析）来生成ID的一种算法，这种方案把64-bit分别划分成多段，分开来标示机器、时间等，比如在snowflake中的64-bit分别表示如下图所示：</p><p><img src="/images/%E6%95%B0%E6%8D%AE%E5%BA%93/snowflake.png" alt=" snowflake "></p><blockquote><p>生成繁琐，由多个指令码组成，并且我们不需要用到分布式，这个还对本地的时间有强依赖，不够简洁</p></blockquote><h3 id="基于数据库的"><a href="#基于数据库的" class="headerlink" title="基于数据库的"></a>基于数据库的</h3><p>基于数据库的其实就是利用自增的自增机制+发号机制的组合，但是由于我们这里不基于数据库，所以给予数据库的基本也不考虑，但是其中的发号机制可以参考。例如：预分配机制。</p><h3 id="自己的发号器"><a href="#自己的发号器" class="headerlink" title="自己的发号器"></a>自己的发号器</h3><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> msource</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">"fmt"</span></span><br><span class="line"><span class="string">"gitlab.mingchao.com/basedev-deps/gorocksdb"</span></span><br><span class="line"><span class="string">"os"</span></span><br><span class="line"><span class="string">"os/signal"</span></span><br><span class="line"><span class="string">"reflect"</span></span><br><span class="line"><span class="string">"strconv"</span></span><br><span class="line"><span class="string">"syscall"</span></span><br><span class="line"><span class="string">"time"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> counter <span class="keyword">struct</span> &#123;</span><br><span class="line">*gorocksdb.ReadOptions</span><br><span class="line"></span><br><span class="line">IdKey []<span class="keyword">byte</span></span><br><span class="line"></span><br><span class="line">GroupId <span class="keyword">string</span></span><br><span class="line"></span><br><span class="line">idChan <span class="keyword">chan</span> <span class="keyword">int64</span></span><br><span class="line"></span><br><span class="line">sig <span class="keyword">chan</span> os.Signal</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *counter)</span> <span class="title">UnmarshalJSON</span><span class="params">(data []<span class="keyword">byte</span>)</span> <span class="params">(err error)</span></span> &#123;</span><br><span class="line">c.IdKey = data[<span class="number">1</span> : <span class="built_in">len</span>(data)<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">turboNew(c)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *counter)</span> <span class="title">MarshalJSON</span><span class="params">()</span> <span class="params">([]<span class="keyword">byte</span>, error)</span></span> &#123;</span><br><span class="line">b := <span class="built_in">make</span>([]<span class="keyword">byte</span>, <span class="number">0</span>, <span class="built_in">len</span>(c.IdKey)+<span class="number">2</span>)</span><br><span class="line">b = <span class="built_in">append</span>(b, <span class="string">'"'</span>)</span><br><span class="line">b = <span class="built_in">append</span>(b, c.IdKey...)</span><br><span class="line">b = <span class="built_in">append</span>(b, <span class="string">'"'</span>)</span><br><span class="line"><span class="keyword">return</span> b, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *counter)</span> <span class="title">String</span><span class="params">()</span> <span class="title">string</span></span> &#123;</span><br><span class="line">t := reflect.TypeOf(c).Elem()</span><br><span class="line">v := reflect.ValueOf(c).Elem()</span><br><span class="line">p := fmt.Sprintf(<span class="string">"%s &#123;"</span>, t.Name())</span><br><span class="line"><span class="keyword">for</span> i := <span class="number">0</span>; i &lt; v.NumField(); i++ &#123;</span><br><span class="line"><span class="keyword">if</span> v.Field(i).CanInterface() &#123;</span><br><span class="line"><span class="keyword">if</span> v.Field(i).Kind() == reflect.Slice &#123;</span><br><span class="line">p += fmt.Sprintf(<span class="string">"\n\t %s(%s): %s"</span>, t.Field(i).Name, t.Field(i).Type, <span class="keyword">string</span>(v.Field(i).Bytes()))</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">p += fmt.Sprintf(<span class="string">"\n\t %s(%s): %v"</span>, t.Field(i).Name, t.Field(i).Type, v.Field(i).Interface())</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">p += <span class="string">"\n&#125;"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> p</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">NewCounter</span><span class="params">(prefix <span class="keyword">string</span>)</span> *<span class="title">counter</span></span> &#123;</span><br><span class="line"><span class="keyword">return</span> newCounter(prefix)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">newCounter</span><span class="params">(prefix <span class="keyword">string</span>, args ...<span class="keyword">interface</span>&#123;&#125;)</span> *<span class="title">counter</span></span> &#123;</span><br><span class="line">c := <span class="built_in">new</span>(counter)</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(args) &gt; <span class="number">0</span> &#123;</span><br><span class="line">c.GroupId = args[<span class="number">0</span>].(<span class="keyword">string</span>)</span><br><span class="line">&#125;</span><br><span class="line">c.IdKey = []<span class="keyword">byte</span>(fmt.Sprintf(<span class="string">"%s:%s"</span>, prefix, c.GroupId))</span><br><span class="line"></span><br><span class="line">turboNew(c)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> c</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">turboNew</span><span class="params">(c *counter)</span></span> &#123;</span><br><span class="line">ct := custom.Load().(*Custom)</span><br><span class="line">c.idChan = <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">int64</span>, ct.IdStep)</span><br><span class="line">c.sig = <span class="built_in">make</span>(<span class="keyword">chan</span> os.Signal, <span class="number">1</span>)</span><br><span class="line">c.ReadOptions = gorocksdb.NewDefaultReadOptions()</span><br><span class="line">signal.Notify(c.sig, syscall.SIGINT, syscall.SIGTERM)</span><br><span class="line"></span><br><span class="line">c.sender()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *counter)</span> <span class="title">sender</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">ct := custom.Load().(*Custom)</span><br><span class="line"><span class="keyword">for</span> &#123;</span><br><span class="line"><span class="keyword">select</span> &#123;</span><br><span class="line"><span class="keyword">case</span> &lt;-c.sig:</span><br><span class="line"><span class="built_in">close</span>(c.idChan)</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line"><span class="keyword">default</span>:</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(c.idChan) &lt; ct.IdStep/<span class="number">10</span> &#123;</span><br><span class="line">rdb := rocksdb.Load().(*Rocksdb)</span><br><span class="line"><span class="keyword">if</span> c.ReadOptions == <span class="literal">nil</span> &#123;</span><br><span class="line">c.ReadOptions = gorocksdb.NewDefaultReadOptions()</span><br><span class="line">&#125;</span><br><span class="line">slice, err := rdb.Get(c.ReadOptions, c.getIdKey())</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">fatal(UnexpectErrorCategory&#123;<span class="string">"counter sender error"</span>&#125;, err)</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">var</span> idStr <span class="keyword">string</span></span><br><span class="line"><span class="keyword">if</span> slice.Exists() &amp;&amp; slice.Size() &gt; <span class="number">0</span> &#123;</span><br><span class="line">idStr = <span class="keyword">string</span>(slice.Data())</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">idStr = <span class="string">"0"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">cid, err := strconv.ParseInt(idStr, <span class="number">10</span>, <span class="number">64</span>)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">fatal(UnexpectErrorCategory&#123;<span class="string">"counter sender error"</span>&#125;, err)</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">ct := custom.Load().(*Custom)</span><br><span class="line"><span class="comment">// id回滚</span></span><br><span class="line"><span class="keyword">if</span> ((<span class="number">1</span>&lt;&lt;<span class="number">63</span><span class="number">-1</span>)/<span class="number">2</span>)-ct.IdStep &lt; ct.IdStep &#123;</span><br><span class="line">cid = <span class="number">0</span></span><br><span class="line">&#125;</span><br><span class="line">nextId := cid + <span class="keyword">int64</span>(ct.IdStep)</span><br><span class="line">err = c.ackId(nextId)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">fatal(UnexpectErrorCategory&#123;<span class="string">"counter sender error"</span>&#125;, err)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span> cid &lt; nextId &#123;</span><br><span class="line">cid++</span><br><span class="line">c.idChan &lt;- cid</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">time.Sleep(<span class="number">50</span> * time.Millisecond)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Rocksdb to get a globally unique self increment ID</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *counter)</span> <span class="title">getId</span><span class="params">()</span> <span class="params">(<span class="keyword">int64</span>, error)</span></span> &#123;</span><br><span class="line">id := &lt;-c.idChan</span><br><span class="line"><span class="keyword">return</span> id, <span class="literal">nil</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//return -1, GetIdError&#123;bytes2string(c.IdKey)&#125;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *counter)</span> <span class="title">GetId</span><span class="params">()</span> <span class="params">(<span class="keyword">int64</span>, error)</span></span> &#123;</span><br><span class="line"><span class="keyword">return</span> c.getId()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *counter)</span> <span class="title">ackId</span><span class="params">(id <span class="keyword">int64</span>)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">rdb := rocksdb.Load().(*Rocksdb)</span><br><span class="line">err := rdb.Put(c.getIdKey(), []<span class="keyword">byte</span>(strconv.FormatInt(id, <span class="number">10</span>)))</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *counter)</span> <span class="title">AckId</span><span class="params">(id <span class="keyword">int64</span>)</span> <span class="title">error</span></span> &#123;</span><br><span class="line"><span class="keyword">return</span> c.ackId(id)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Gets the key of the ID</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *counter)</span> <span class="title">getIdKey</span><span class="params">()</span> []<span class="title">byte</span></span> &#123;</span><br><span class="line"><span class="keyword">return</span> c.IdKey</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里我们优先考虑可以通过内存直接通过<code>++</code>或者<code>+1操作符</code>分配的方式。我们重点看到：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">nextId := cid + <span class="keyword">int64</span>(ct.IdStep)</span><br><span class="line">err = c.ackId(nextId)</span><br><span class="line"><span class="keyword">for</span> cid &lt; nextId &#123;</span><br><span class="line">cid++</span><br><span class="line">c.idChan &lt;- cid</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以再这里看到，我们通过拿到当前cid的数值，通过<code>idStep</code>来增加固定的步长，然后先通过回写nextId的值到rocksdb进行持久化，再通过<code>for</code>循环来对cid进行叠加，每次都推送到<code>有缓冲区</code>的<code>idChan</code>中。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Rocksdb to get a globally unique self increment ID</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *counter)</span> <span class="title">getId</span><span class="params">()</span> <span class="params">(<span class="keyword">int64</span>, error)</span></span> &#123;</span><br><span class="line">id := &lt;-c.idChan</span><br><span class="line"><span class="keyword">return</span> id, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *counter)</span> <span class="title">GetId</span><span class="params">()</span> <span class="params">(<span class="keyword">int64</span>, error)</span></span> &#123;</span><br><span class="line"><span class="keyword">return</span> c.getId()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>通过 <code>func (c *counter) getId() (int64, error)</code> 来消费<code>idChan</code>中的id，达到一个获取id的效果。 </p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// id回滚</span></span><br><span class="line"><span class="keyword">if</span> ((<span class="number">1</span>&lt;&lt;<span class="number">63</span><span class="number">-1</span>)/<span class="number">2</span>)-ct.IdStep &lt; ct.IdStep &#123;</span><br><span class="line">cid = <span class="number">0</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们看到这里有一行代码，当int64的cid已经到达分配的极限了，那么cid就会进行回滚，基本保证了发号的可重复利用性。</p><p>扩展问题：id回溯了，怎么做递增判断？</p><p>这个问题其实有点类似tcp的syn回溯的问题。因为syn一开始是随机生成的，并且这个过程了syn是会不断增加的。当syn到达分配的极限进行了回溯的时候，如何比较大小？</p><p>我们查看到内核的tcp源码，可以看到提供的判断方式十分巧妙，如下：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">* The next routines deal with comparing 32 bit unsigned ints</span></span><br><span class="line"><span class="comment">* and worry about wraparound (automatic with unsigned arithmetic).</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">before</span><span class="params">(__u32 seq1, __u32 seq2)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">return</span> (__s32)(seq1-seq2) &lt; <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> after(seq2, seq1) before(seq1, seq2)</span></span><br></pre></td></tr></table></figure><p>为什么<code>（__s32）(seq1-seq2)&lt;0</code>就可以判断<code>seq1&lt;seq2</code>呢？这里的<code>__s32</code>是有符号整型的意思，而<code>__u32</code>则是无符号整型。<br>为了方便说明，我们以<code>unsigned char</code>和<code>char</code>为例来说明：</p><p>假设seq1=255， seq2=1（发生了回绕）<br>seq1 = 1111 1111 seq2 = 0000 0001<br>我们希望比较结果是 <code>seq1&lt;seq2</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> seq1 - seq2&#x3D;</span><br><span class="line"> 1111 1111</span><br><span class="line">-0000 0001</span><br><span class="line">-----------</span><br><span class="line"> 1111 1110</span><br></pre></td></tr></table></figure><p>由于我们将结果转化成了有符号数，<code>由于最高位是1</code>，因此结果是<code>一个负数</code>，负数的绝对值为<br> 0000 0001 + 1 = 0000 0010 = 2 (补码：取反+1)</p><p>因此 <code>seq1 - seq2 &lt; 0</code></p><p>注意：</p><p>如果seq2=128的话，我们会发现：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> seq1 - seq2&#x3D;</span><br><span class="line"> 1111 1111</span><br><span class="line">-1000 0000</span><br><span class="line">-----------</span><br><span class="line"> 0111 1111</span><br></pre></td></tr></table></figure><p>此时结果尤为正了，判断的结果是<code>seq1&gt;seq2</code>。因此，上述算法正确的前提是，<code>回绕后的增量小于2^(n-1)-1</code>。</p><p>由于tcp序列号用的<code>32位无符号数</code>，因此可以支持的<code>回绕幅度是2^31-1</code>，满足要求了。</p><blockquote><p>但是由于我们这里不需要比较发号的先后次序，只需要保证其唯一性，所以这个回溯的大小比较问题，并不需要过多的关注</p></blockquote><h2 id="行级锁的实现"><a href="#行级锁的实现" class="headerlink" title="行级锁的实现"></a>行级锁的实现</h2><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">var</span> (</span><br><span class="line"><span class="comment">// dbID:tblID</span></span><br><span class="line"><span class="comment">// dbID:tblID:rowID</span></span><br><span class="line">rowLockLock sync.RWMutex</span><br><span class="line">rowLock     rl</span><br><span class="line">ttlTime     = <span class="keyword">int64</span>(<span class="number">30</span> * <span class="number">60</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> (</span><br><span class="line">rl <span class="keyword">map</span>[<span class="keyword">string</span>]*lock</span><br><span class="line"></span><br><span class="line">lock <span class="keyword">struct</span> &#123;</span><br><span class="line">ttl  <span class="keyword">int64</span></span><br><span class="line">lock sync.RWMutex</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">init</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="comment">// clean row lock, release memory</span></span><br><span class="line"><span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">t := time.NewTicker(<span class="number">10</span> * time.Minute)</span><br><span class="line"><span class="keyword">for</span> &#123;</span><br><span class="line"><span class="keyword">select</span> &#123;</span><br><span class="line"><span class="keyword">case</span> &lt;-t.C:</span><br><span class="line">ct := time.Now().Unix()</span><br><span class="line">rowLockLock.Lock()</span><br><span class="line"><span class="keyword">for</span> key, lock := <span class="keyword">range</span> rowLock &#123;</span><br><span class="line"><span class="keyword">if</span> ct &gt; lock.ttl &#123;</span><br><span class="line"><span class="built_in">delete</span>(rowLock, key)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">rowLockLock.Unlock()</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;()</span><br><span class="line"></span><br><span class="line">rowLock = <span class="built_in">make</span>(rl, <span class="number">10</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">rowLockKey</span><span class="params">(dbId, tblId, rowId <span class="keyword">int64</span>)</span> <span class="title">string</span></span> &#123;</span><br><span class="line"><span class="keyword">return</span> fmt.Sprintf(<span class="string">"%d:%d:%d"</span>, dbId, tblId, rowId)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *rl)</span> <span class="title">Lock</span><span class="params">(lockKey <span class="keyword">string</span>)</span></span> &#123;</span><br><span class="line">rowLockLock.Lock()</span><br><span class="line">m, ok := (*r)[lockKey]</span><br><span class="line"><span class="keyword">if</span> !ok &#123;</span><br><span class="line">m = <span class="built_in">new</span>(lock)</span><br><span class="line">(*r)[lockKey] = m</span><br><span class="line">m.ttl = time.Now().Unix() + ttlTime</span><br><span class="line">&#125;</span><br><span class="line">rowLockLock.Unlock()</span><br><span class="line"></span><br><span class="line">m.lock.Lock()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *rl)</span> <span class="title">UnLock</span><span class="params">(lockKey <span class="keyword">string</span>)</span></span> &#123;</span><br><span class="line">rowLockLock.Lock()</span><br><span class="line">m, ok := (*r)[lockKey]</span><br><span class="line"><span class="keyword">if</span> !ok &#123;</span><br><span class="line">m = <span class="built_in">new</span>(lock)</span><br><span class="line">(*r)[lockKey] = m</span><br><span class="line">m.ttl = time.Now().Unix() + ttlTime</span><br><span class="line">&#125;</span><br><span class="line">rowLockLock.Unlock()</span><br><span class="line"></span><br><span class="line">m.lock.Unlock()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *rl)</span> <span class="title">RLock</span><span class="params">(lockKey <span class="keyword">string</span>)</span></span> &#123;</span><br><span class="line">rowLockLock.Lock()</span><br><span class="line">m, ok := (*r)[lockKey]</span><br><span class="line"><span class="keyword">if</span> !ok &#123;</span><br><span class="line">m = <span class="built_in">new</span>(lock)</span><br><span class="line">(*r)[lockKey] = m</span><br><span class="line">m.ttl = time.Now().Unix() + ttlTime</span><br><span class="line">&#125;</span><br><span class="line">rowLockLock.Unlock()</span><br><span class="line"></span><br><span class="line">m.lock.RLock()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *rl)</span> <span class="title">RUnlock</span><span class="params">(lockKey <span class="keyword">string</span>)</span></span> &#123;</span><br><span class="line">rowLockLock.Lock()</span><br><span class="line">m, ok := (*r)[lockKey]</span><br><span class="line"><span class="keyword">if</span> !ok &#123;</span><br><span class="line">m = <span class="built_in">new</span>(lock)</span><br><span class="line">(*r)[lockKey] = m</span><br><span class="line">m.ttl = time.Now().Unix() + ttlTime</span><br><span class="line">&#125;</span><br><span class="line">rowLockLock.Unlock()</span><br><span class="line"></span><br><span class="line">m.lock.RUnlock()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>以上是行级锁的实现方式，主要是利用<code>sync.RWMutex</code>来实现读写锁，并且带有ttl的机制，每次加锁的时候，都会更新ttl的时间。<br>其中在<code>init阶段</code>，我们利用的ticker来实现对锁进行一个类似<code>LRU</code>的机制，对于不活跃的锁对象进行释放，防止在这里造成内存只增不减。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(s *Store)</span> <span class="title">updateHandle</span><span class="params">(result *Result, stmt *ast.UpdateStmt)</span></span> &#123;</span><br><span class="line"><span class="comment">// 通过ast获取old数据</span></span><br><span class="line">....</span><br><span class="line"><span class="comment">// 行级锁锁定 </span></span><br><span class="line">rowID, _ := item[<span class="number">0</span>].(json2.Number).Int64()</span><br><span class="line">rowlockKey := rowLockKey(db.Id, tbl.Id, rowID)</span><br><span class="line">rowLock.Lock(rowlockKey)</span><br><span class="line"><span class="keyword">defer</span> rowLock.UnLock(rowlockKey)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 更新索引数据</span></span><br><span class="line"><span class="keyword">for</span> _, index := <span class="keyword">range</span> indexs &#123;</span><br><span class="line">...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 更新为new数据</span></span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="逆波兰表达式-amp-amp-波兰表达式"><a href="#逆波兰表达式-amp-amp-波兰表达式" class="headerlink" title="逆波兰表达式 &amp;&amp; 波兰表达式"></a>逆波兰表达式 &amp;&amp; 波兰表达式</h2><p>这一块其实暂时还没实现，但是他的原理有必要和大家说一下，我们的db实现，都是基于<code>sql</code> 来实现的，我们知道 <code>sql</code> 中也有表达式计算，并且是有优先级之分的。</p><p><code>前/中/后</code>序遍历，相信大家基本都听说过，但是实际运用中少之又少，这是因为大家可能在实际中没有找到合适的模式和套用这些树的遍历方式。</p><ul><li><p>前序遍历：根结点 —&gt; 左子树 —&gt; 右子树</p></li><li><p>中序遍历：左子树—&gt; 根结点 —&gt; 右子树</p></li><li><p>后序遍历：左子树 —&gt; 右子树 —&gt; 根结点</p></li></ul><p>例如：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> (<span class="keyword">count</span> * price) <span class="keyword">AS</span> <span class="keyword">sum</span> <span class="keyword">FROM</span> orders <span class="keyword">WHERE</span> order_id &lt; <span class="number">100</span></span><br></pre></td></tr></table></figure><p>其中 <code>order_id &lt; 10</code> 就是一个表达式，它有一个列输入参数： <code>order_id</code>，输出：<code>Bool</code></p><h3 id="RPN-表达式-逆波兰表示法"><a href="#RPN-表达式-逆波兰表示法" class="headerlink" title="RPN 表达式(逆波兰表示法)"></a>RPN 表达式(逆波兰表示法)</h3><p>RPN 是树的<code>后序遍历</code>，后序遍历在每个节点知道自己有几个子节点的时候等价于原本的树结构。</p><blockquote><p>所以你波澜是后序遍历：<code>左右中</code></p></blockquote><p>比如说我们有一个数学算式 <code>2 *（3 + 4）+ 5</code>：</p><p><img src="/images/%E6%95%B0%E6%8D%AE%E5%BA%93/RPN.png" alt=" RPN "></p><p>由于数学上习惯写法是<code>中序遍历</code>，我们通常要加上括号消除歧义（比如加减和乘除的顺序）。通过把操作符后移 我们得到 <code>RPN：2 3 4 + * 5 +</code>，这样我们无需括号就能无歧义地遍历这个表达式：</p><p><code>中序表达式</code>转<code>后序表达式</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">原式：a+b*(c+d&#x2F;e)</span><br><span class="line">补全括号：(a+(b*(c+(d&#x2F;e))))</span><br><span class="line">操作符右移：(a(b(c(de)&#x2F;)+)*)+</span><br><span class="line">去掉括号：abcde&#x2F;+*+</span><br></pre></td></tr></table></figure><blockquote><p>所以波兰表达式是中序遍历：<code>左右中</code></p></blockquote><p><img src="/images/%E6%95%B0%E6%8D%AE%E5%BA%93/RPN2.png" alt=" RPN "></p><p>执行 RPN 的过程需要一个<code>栈</code>来缓存中间结果，比如说对于 <code>2 3 4 + * 5 +</code>，我们<code>从左到右</code>遍历表达式，遇到值就压入栈中。直到 <code>+</code> 操作符，栈中已经压入了 <code>2 3 4</code>。</p><p><img src="/images/%E6%95%B0%E6%8D%AE%E5%BA%93/RPN3.png" alt=" RPN "></p><p>因为 <code>+</code> 是二元操作符，需要从栈中弹出两个值 <code>3 4</code>，结果为 <code>7</code>，<code>重新</code>压入栈中：</p><p><img src="/images/%E6%95%B0%E6%8D%AE%E5%BA%93/RPN4.png" alt=" RPN "></p><p>此时栈中的值为 <code>2 7</code>。</p><p><img src="/images/%E6%95%B0%E6%8D%AE%E5%BA%93/RPN5.png" alt=" RPN "></p><p>下一个是 <code>*</code> 运算符，也需要弹出两个值 <code>2 7</code>，结果为 <code>14</code> 压入栈中。</p><p><img src="/images/%E6%95%B0%E6%8D%AE%E5%BA%93/RPN6.png" alt=" RPN "></p><p>接着压入 <code>5</code> 。</p><p><img src="/images/%E6%95%B0%E6%8D%AE%E5%BA%93/RPN7.png" alt=" RPN "></p><p>最后 <code>+</code> 运算符弹出 <code>14 5</code>，结果为 <code>19</code> ，压入<code>栈</code>。</p><p><img src="/images/%E6%95%B0%E6%8D%AE%E5%BA%93/RPN8.png" alt=" RPN "></p><p>最后留在栈里的就是表达式的结果，因此，如果需要计算表达式优先级的话，可以采用RPN的方式来读取tree结构来进行顺序计算。</p><h2 id="单独使用DB例子："><a href="#单独使用DB例子：" class="headerlink" title="单独使用DB例子："></a>单独使用DB例子：</h2><p><img src="/images/%E6%95%B0%E6%8D%AE%E5%BA%93/db-debug.png" alt=" db-debug "></p><p>这里有一个类似于<code>mysql-client</code> 的一个 <code>bin</code> 程序</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">"flag"</span></span><br><span class="line"><span class="string">"github.com/pingcap/parser"</span></span><br><span class="line"><span class="string">"github.com/pingcap/parser/ast"</span></span><br><span class="line"><span class="string">"gitlab.mingchao.com/basedev-deps/logbdev"</span></span><br><span class="line"><span class="string">"gitlab.mingchao.com/basedev-deps/msource/v2"</span></span><br><span class="line"><span class="string">"os"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> sql = flag.String(<span class="string">"sql"</span>, <span class="string">""</span>, <span class="string">"Input Your Sql"</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">init</span><span class="params">()</span></span> &#123;</span><br><span class="line">flag.Parse()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="keyword">if</span> os.Getenv(<span class="string">"DEBUG"</span>) == <span class="string">"true"</span> &#123;</span><br><span class="line">logbdev.SetLevel(logbdev.DebugLevel)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">msource.PreparePhase()</span><br><span class="line">store := msource.NewStore()</span><br><span class="line"></span><br><span class="line">p := parser.New()</span><br><span class="line">stmtNode, err := p.ParseOneStmt(*sql, <span class="string">""</span>, <span class="string">""</span>)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">logbdev.Error(err)</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">_, ok := stmtNode.(*ast.SelectStmt)</span><br><span class="line"><span class="keyword">var</span> r *msource.Result</span><br><span class="line"><span class="keyword">if</span> ok &#123;</span><br><span class="line">r, err = store.Query(*sql)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">logbdev.Error(err)</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">r, err = store.Execute(*sql)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">logbdev.Error(err)</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> r != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">if</span> r.Data != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">switch</span> ar := r.Data.(<span class="keyword">type</span>) &#123;</span><br><span class="line"><span class="keyword">case</span> *msource.InsertResultData:</span><br><span class="line">logbdev.Info(ar.GetSliceInt64())</span><br><span class="line">logbdev.Info(ar.Raw())</span><br><span class="line"><span class="keyword">case</span> *msource.ShowDatabasesResultData:</span><br><span class="line">logbdev.Info(ar.GetSliceString())</span><br><span class="line"><span class="keyword">case</span> *msource.ShowTablesResultData:</span><br><span class="line">logbdev.Info(ar.GetSliceString())</span><br><span class="line"><span class="keyword">case</span> *msource.SelectResultSetData:</span><br><span class="line">logbdev.Info(ar.GetFields())</span><br><span class="line">logbdev.Info(ar.GetValues())</span><br><span class="line">logbdev.Info(ar.Count())</span><br><span class="line"><span class="keyword">case</span> *msource.DeleteResultData:</span><br><span class="line">logbdev.Info(ar.GetAffected())</span><br><span class="line"><span class="keyword">case</span> *msource.UpdateResultData:</span><br><span class="line">logbdev.Info(ar.GetAffected())</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">logbdev.Info(r)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>具体用法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">go run example&#x2F;msource_db&#x2F;customStmt&#x2F;main.go --sql &quot;INSERT INTO users(\&#96;name\&#96;,\&#96;age\&#96;,\&#96;last_login\&#96;) VALUES (\&quot;caiwenhui\&quot;, 18, 1614776101)&quot;</span><br><span class="line">go run example&#x2F;msource_db&#x2F;customStmt&#x2F;main.go --sql &quot;show databases;&quot;</span><br><span class="line">go run example&#x2F;msource_db&#x2F;customStmt&#x2F;main.go --sql &quot;show tables;&quot;</span><br><span class="line">go run example&#x2F;msource_db&#x2F;customStmt&#x2F;main.go --sql &quot;INSERT INTO mingchao.users2(\&#96;name\&#96;,\&#96;age\&#96;) VALUES (\&quot;caiwenhui\&quot;, 18),(\&quot;caiwenhui\&quot;, 19)&quot;</span><br><span class="line">go run example&#x2F;msource_db&#x2F;customStmt&#x2F;main.go --sql &quot;INSERT INTO mingchao.users2 VALUES (1000,\&quot;caiwenhui\&quot;, 18)&quot;</span><br></pre></td></tr></table></figure><p>我们可以用对外暴露一个<code>msource.NewStore()</code>来创建一个存储器对象，然后通过API进行<code>数据库</code>的操作。</p><blockquote><p>NewStore我们用了sync.Pool封装，对象可以做到尽可能的复用。</p></blockquote><p>可以看到如果是<code>SELECT STMT</code>的话，我们调用的是<code>QUERY</code>API，如果是<code>非SELECT STMT</code>的话，我们调用的是<code>EXECUTE</code>API。</p><h2 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h2><p>基于目前尚未实现，所以暂时不再展开讲叙，后续可以升级处理的点为：</p><ul><li>事务处理，例如前面所说的<code>redolog</code>和<code>undolog</code>可实现。</li><li>orderby， 数据排序。</li><li>全双工的通信获取数据，无需一次性读取所有数据。</li><li>Explain执行计划的实现，逻辑根据执行计划走。</li></ul><h1 id="SPOUT"><a href="#SPOUT" class="headerlink" title="SPOUT"></a>SPOUT</h1><p>另外一篇文章中，记录了我们的<code>spout</code>的作用，在这里，再简单说一下，<code>spout</code> 是我们<code>msource</code>组件的核心角色，它是用于把数据推送到上层业务的所使用。上层业务通过<code>spout</code>角色提供的<code>API</code>，可以获取到从数据源拿到的数据。</p><p><code>spout</code> 自身保持了一套 <code>高可靠</code>, <code>高性能</code>, <code>可容错</code> 的数据机制，主要用于区别出<code>ACK</code>, <code>NACK</code>，并且自带有 <code>失败重传</code>, <code>多阶段状态机的checkpoint</code>等机制。</p><h2 id="channel-mode-大体数据流程图"><a href="#channel-mode-大体数据流程图" class="headerlink" title="channel-mode 大体数据流程图"></a>channel-mode 大体数据流程图</h2><p><img src="/images/%E6%95%B0%E6%8D%AE%E5%BA%93/channel-mode.png" alt=" channel-mode "></p><p>之前有一篇文章，专门讲解channel-mode下，是如何工作的，这里不做过多详细的说明。简单复述一下。</p><p>channel模式下，是直接把数据推送到我们的<code>golang</code>的<code>channel</code> 当中，上层业务直接用过channel拿到数据，拿到数据后根据自身业务处理数据来判断可以ack或者nack掉数据，同时保存offset。</p><p>这里的问题就在于：</p><blockquote><p>由于我们是本地存储的offset，因为<code>不信任</code> kafka-client的<code>auto-commit</code>机制，当程序在某个节点crash的时候，这会让我们的程序在下次启动的时候，重复消费到数据或者遗漏数据</p></blockquote><p>缺点：每个partition的offset都需要顺序消费，在上层业务无法并发处理，这极大程度的降低了我们的消费效率</p><p>期望：如果我们提前把offset存储起来了，而不需要<code>ACK</code>之后再存储offset的话，那么我们就可以再上层业务并发的处理消息，而无需关注offset的问题</p><h2 id="db-demo-大体数据流程图"><a href="#db-demo-大体数据流程图" class="headerlink" title="db-demo 大体数据流程图"></a>db-demo 大体数据流程图</h2><p><img src="/images/%E6%95%B0%E6%8D%AE%E5%BA%93/db-mode.png" alt=" db-mode "></p><p>鉴于<code>channel-mode</code>下的缺点，由此诞生了我们的<code>db-mode</code>，其原理是把数据先存储在本地的数据库，也就是我们上面所说的<code>db</code>，所以这里我们可以得出关系<code>spout &lt;- db</code>， <code>db角色</code> 可以被 <code>spout角色</code>所依赖。</p><p>我们创建了4个表来存储不同的数据：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> (</span><br><span class="line">DefaultDatabase = <span class="string">"default"</span></span><br><span class="line">DefaultDatabaseSql = <span class="string">"CREATE DATABASE IF NOT EXISTS `"</span> +</span><br><span class="line">DefaultDatabase +</span><br><span class="line"><span class="string">"`"</span></span><br><span class="line">UseDefaultDatabase = <span class="string">"USE `"</span> +</span><br><span class="line">DefaultDatabase +</span><br><span class="line"><span class="string">"`"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 推送消息的时候使用</span></span><br><span class="line"><span class="comment">// 因为正常消息过来的时候是没有row_id的，所以这个payload_marshal内的row_id没意义</span></span><br><span class="line"><span class="comment">// 该表仅仅遍历数据到Runner表，到了Runner表和Loser表，Row_id才有用</span></span><br><span class="line">SpoutStoreStorageTable         = <span class="string">"storage"</span></span><br><span class="line">SpoutStoreStorageBuildTableSql = <span class="string">"create table if not exists "</span> + SpoutStoreStorageTable +</span><br><span class="line"><span class="string">"("</span> +</span><br><span class="line"><span class="string">"`payload_marshal` varchar(255) not null comment \"序列化后的payload\","</span> +</span><br><span class="line"><span class="string">"`is_multi_phase` int(11) not null default 0 comment \"是否多阶段，0=否, 1=是\","</span> +</span><br><span class="line"><span class="string">"`cur_state` int(11) not null default 0 comment \"当前状态\","</span> +</span><br><span class="line"><span class="string">"`fin_state` int(11) not null default 0 comment \"最终状态\""</span> +</span><br><span class="line"><span class="string">")"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Storage -&gt; Runner 使用</span></span><br><span class="line"><span class="comment">// 刚启动服务的时候Runner-&gt;Storage有用</span></span><br><span class="line"><span class="comment">// Ack的时候有用</span></span><br><span class="line">SpoutStoreRunningTable         = <span class="string">"runner"</span></span><br><span class="line">SpoutStoreRunningBuildTableSql = <span class="string">"create table if not exists "</span> + SpoutStoreRunningTable +</span><br><span class="line"><span class="string">"("</span> +</span><br><span class="line"><span class="string">"`payload_marshal` varchar(255) not null comment \"序列化后的payload\","</span> +</span><br><span class="line"><span class="string">"`is_multi_phase` int(11) not null default 0 comment \"是否多阶段，0=否, 1=是\","</span> +</span><br><span class="line"><span class="string">"`cur_state` int(11) not null default 0 comment \"当前状态\","</span> +</span><br><span class="line"><span class="string">"`fin_state` int(11) not null default 0 comment \"最终状态\""</span> +</span><br><span class="line"><span class="string">")"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Runner -&gt; Loser 使用</span></span><br><span class="line"><span class="comment">// Nack的时候有用</span></span><br><span class="line"><span class="comment">// 失败重传的时候用/刚启动服务的时候Loser-&gt;Storage有用</span></span><br><span class="line">SpoutStoreLoserTable         = <span class="string">"loser"</span></span><br><span class="line">SpoutStoreLoserBuildTableSql = <span class="string">"create table if not exists "</span> + SpoutStoreLoserTable +</span><br><span class="line"><span class="string">"("</span> +</span><br><span class="line"><span class="string">"`payload_marshal` varchar(255) not null comment \"序列化后的payload\","</span> +</span><br><span class="line"><span class="string">"`is_multi_phase` int(11) not null default 0 comment \"是否多阶段，0=否, 1=是\","</span> +</span><br><span class="line"><span class="string">"`cur_state` int(11) not null default 0 comment \"当前状态\","</span> +</span><br><span class="line"><span class="string">"`fin_state` int(11) not null default 0 comment \"最终状态\""</span> +</span><br><span class="line"><span class="string">")"</span></span><br><span class="line"></span><br><span class="line">SpoutStoreDefaultOffsetTable         = <span class="string">"offset"</span></span><br><span class="line">SpoutStoreDefaultOffsetBuildTableSql = <span class="string">"create table if not exists "</span> + SpoutStoreDefaultOffsetTable +</span><br><span class="line"><span class="string">"("</span> +</span><br><span class="line"><span class="string">"`group_id` varchar(255) not null comment \"消费组\","</span> +</span><br><span class="line"><span class="string">"`topic` varchar(255) not null comment \"消费的topic\","</span> +</span><br><span class="line"><span class="string">"`partition` int(11) not null comment \"topic的partition\","</span> +</span><br><span class="line"><span class="string">"`offset` int(11) not null comment \"当前消费的offset\","</span> +</span><br><span class="line"><span class="string">"    UNIQUE KEY `uniq_idx` (`group_id`,`topic`,`partition`)"</span> +</span><br><span class="line"><span class="string">")"</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p><img src="/images/%E6%95%B0%E6%8D%AE%E5%BA%93/phase-deal-with.png" alt=" phase-deal-with "></p><h2 id="DB模式下的用法例子："><a href="#DB模式下的用法例子：" class="headerlink" title="DB模式下的用法例子："></a>DB模式下的用法例子：</h2><blockquote><p>channel-mode下的例子和db模式的差不多，但是更加简单，这里就不列出来了。</p></blockquote><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> common</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">"context"</span></span><br><span class="line"><span class="string">"gitlab.mingchao.com/basedev-deps/logbdev"</span></span><br><span class="line"><span class="string">"gitlab.mingchao.com/basedev-deps/msource/v2"</span></span><br><span class="line"><span class="string">"os"</span></span><br><span class="line"><span class="string">"os/signal"</span></span><br><span class="line"><span class="string">"sync"</span></span><br><span class="line"><span class="string">"syscall"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">PreparePhase</span><span class="params">()</span></span> &#123;</span><br><span class="line">logbdev.SetLevel(logbdev.DebugLevel)</span><br><span class="line">S = msource.PreparePhase()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> S *msource.Spout</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">CoreStart</span><span class="params">(function <span class="keyword">func</span>(payload *msource.Payload)</span>)</span> &#123;</span><br><span class="line">wg := <span class="built_in">new</span>(sync.WaitGroup)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建主协程上下文</span></span><br><span class="line">ctx, cannel := context.WithCancel(context.Background())</span><br><span class="line"></span><br><span class="line"><span class="comment">// 启动msource，并且传递主协程上下文，用于任务间的通信控制</span></span><br><span class="line">S.Start(ctx)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册信号量</span></span><br><span class="line">sig := <span class="built_in">make</span>(<span class="keyword">chan</span> os.Signal, <span class="number">1</span>)</span><br><span class="line">signal.Notify(sig, syscall.SIGINT, syscall.SIGTERM)</span><br><span class="line">wg.Add(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="keyword">defer</span> wg.Done()</span><br><span class="line"><span class="keyword">for</span> &#123;</span><br><span class="line"><span class="keyword">select</span> &#123;</span><br><span class="line"><span class="keyword">case</span> &lt;-sig:</span><br><span class="line">cannel()</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 这里我们可以再创建更多的worker协助我们的消费数据</span></span><br><span class="line"><span class="comment">// 使用方式基本向后兼容</span></span><br><span class="line">innerWorker := <span class="number">3</span></span><br><span class="line"></span><br><span class="line">logbdev.Infof(<span class="string">"total chan: %d\n"</span>, S.ChanSize())</span><br><span class="line"><span class="comment">// 主协程中创建子协程（worker）工作</span></span><br><span class="line"><span class="keyword">for</span> i := <span class="number">0</span>; i &lt; S.ChanSize(); i++ &#123;</span><br><span class="line"><span class="keyword">for</span> ii := <span class="number">0</span>; ii &lt; innerWorker; ii++ &#123;</span><br><span class="line">wg.Add(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">(idx, idx2 <span class="keyword">int</span>)</span></span> &#123;</span><br><span class="line"><span class="keyword">defer</span> wg.Done()</span><br><span class="line">logbdev.Infof(<span class="string">"start chan[%d-%d]\n"</span>, idx, idx2)</span><br><span class="line">payloadCh := S.GetPayloadChanById(idx)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> payload := <span class="keyword">range</span> payloadCh.GetCh() &#123;</span><br><span class="line">function(payload)</span><br><span class="line">&#125;</span><br><span class="line">&#125;(i, ii)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 等待子协程结束</span></span><br><span class="line">wg.Wait()</span><br><span class="line"><span class="comment">// 等待msource退出</span></span><br><span class="line">S.Stop()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="ACK"><a href="#ACK" class="headerlink" title="ACK"></a>ACK</h3><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">"gitlab.mingchao.com/basedev-deps/logbdev"</span></span><br><span class="line"><span class="string">"gitlab.mingchao.com/basedev-deps/msource/v2"</span></span><br><span class="line"><span class="string">"gitlab.mingchao.com/basedev-deps/msource/v2/example/spout/db/common"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">common.CoreStart(<span class="function"><span class="keyword">func</span><span class="params">(payload *msource.Payload)</span></span> &#123;</span><br><span class="line"><span class="keyword">if</span> err := common.S.Ack(payload); err != <span class="literal">nil</span> &#123;</span><br><span class="line">logbdev.Error(err)</span><br><span class="line">&#125;</span><br><span class="line">&#125;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="NACK"><a href="#NACK" class="headerlink" title="NACK"></a>NACK</h3><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">"gitlab.mingchao.com/basedev-deps/logbdev"</span></span><br><span class="line"><span class="string">"gitlab.mingchao.com/basedev-deps/msource/v2"</span></span><br><span class="line"><span class="string">"gitlab.mingchao.com/basedev-deps/msource/v2/example/spout/db/common"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">common.PreparePhase()</span><br><span class="line">common.CoreStart(<span class="function"><span class="keyword">func</span><span class="params">(payload *msource.Payload)</span></span> &#123;</span><br><span class="line"><span class="keyword">if</span> err := common.S.MarkFailure(payload); err != <span class="literal">nil</span> &#123;</span><br><span class="line">logbdev.Error(err)</span><br><span class="line">&#125;</span><br><span class="line">&#125;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="STATE-MACHINE"><a href="#STATE-MACHINE" class="headerlink" title="STATE-MACHINE"></a>STATE-MACHINE</h3><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">"context"</span></span><br><span class="line"><span class="string">"fmt"</span></span><br><span class="line"><span class="string">"gitlab.mingchao.com/basedev-deps/logbdev"</span></span><br><span class="line"><span class="string">"gitlab.mingchao.com/basedev-deps/msource/v2/example/spout/db/common"</span></span><br><span class="line"><span class="string">"os"</span></span><br><span class="line"><span class="string">"os/signal"</span></span><br><span class="line"><span class="string">"sync"</span></span><br><span class="line"><span class="string">"syscall"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> MyStateMachine <span class="keyword">struct</span> &#123;</span><br><span class="line">phases []<span class="keyword">string</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(msm *MyStateMachine)</span> <span class="title">AddPhase</span><span class="params">(name <span class="keyword">string</span>)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">msm.phases = <span class="built_in">append</span>(msm.phases, name)</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// get all phase</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(msm *MyStateMachine)</span> <span class="title">GetPhases</span><span class="params">()</span> []<span class="title">string</span></span> &#123;</span><br><span class="line"><span class="keyword">return</span> msm.phases</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">common.PreparePhase()</span><br><span class="line">wg := <span class="built_in">new</span>(sync.WaitGroup)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建主协程上下文</span></span><br><span class="line">ctx, cannel := context.WithCancel(context.Background())</span><br><span class="line"></span><br><span class="line"><span class="comment">// 启动msource，并且传递主协程上下文，用于任务间的通信控制</span></span><br><span class="line"><span class="comment">// 需要设置为多阶段的话，必须设置状态机，并且在msource服务Start之前设置</span></span><br><span class="line">sms := &amp;MyStateMachine&#123;&#125;</span><br><span class="line">_ = sms.AddPhase(<span class="string">"step1"</span>)</span><br><span class="line">_ = sms.AddPhase(<span class="string">"step2"</span>)</span><br><span class="line">_ = sms.AddPhase(<span class="string">"step3"</span>)</span><br><span class="line">common.S.SetStateMachine(sms)</span><br><span class="line">common.S.Start(ctx)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册信号量</span></span><br><span class="line">sig := <span class="built_in">make</span>(<span class="keyword">chan</span> os.Signal, <span class="number">1</span>)</span><br><span class="line">signal.Notify(sig, syscall.SIGINT, syscall.SIGTERM)</span><br><span class="line">wg.Add(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="keyword">defer</span> wg.Done()</span><br><span class="line"><span class="keyword">for</span> &#123;</span><br><span class="line"><span class="keyword">select</span> &#123;</span><br><span class="line"><span class="keyword">case</span> &lt;-sig:</span><br><span class="line">cannel()</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;()</span><br><span class="line"></span><br><span class="line">logbdev.Infof(<span class="string">"total chan: %d\n"</span>, common.S.ChanSize())</span><br><span class="line"><span class="comment">// 主协程中创建子协程（worker）工作</span></span><br><span class="line"><span class="keyword">for</span> i := <span class="number">0</span>; i &lt; common.S.ChanSize(); i++ &#123;</span><br><span class="line">wg.Add(<span class="number">1</span>)</span><br><span class="line">chCtx, _ := context.WithCancel(ctx)</span><br><span class="line"><span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">(idx <span class="keyword">int</span>, childCtx context.Context)</span></span> &#123;</span><br><span class="line"><span class="keyword">defer</span> wg.Done()</span><br><span class="line">logbdev.Infof(<span class="string">"start chan[%d]\n"</span>, idx)</span><br><span class="line">payloadCh := common.S.GetPayloadChanById(idx)</span><br><span class="line"><span class="keyword">for</span> &#123;</span><br><span class="line"><span class="keyword">select</span> &#123;</span><br><span class="line"><span class="keyword">case</span> payload := &lt;-payloadCh.GetCh():</span><br><span class="line"><span class="comment">// 不同阶段之间如果相互无依赖的话，则可以并发处理</span></span><br><span class="line"><span class="comment">// 否则请使用同步的方式</span></span><br><span class="line"><span class="keyword">var</span> wg sync.WaitGroup</span><br><span class="line">wg.Add(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="keyword">defer</span> wg.Done()</span><br><span class="line">phase := <span class="string">"step1"</span></span><br><span class="line"><span class="keyword">if</span> common.S.CanTransition(payload, phase) &#123;</span><br><span class="line">fmt.Println(<span class="string">"Do Step 1 something"</span>)</span><br><span class="line">_ = common.S.Transition(payload, phase)</span><br><span class="line">&#125;</span><br><span class="line">&#125;()</span><br><span class="line"></span><br><span class="line">wg.Add(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="keyword">defer</span> wg.Done()</span><br><span class="line">phase := <span class="string">"step2"</span></span><br><span class="line"><span class="keyword">if</span> common.S.CanTransition(payload, phase) &#123;</span><br><span class="line">fmt.Println(<span class="string">"Do Step 2 something"</span>)</span><br><span class="line">_ = common.S.Transition(payload, phase)</span><br><span class="line">&#125;</span><br><span class="line">&#125;()</span><br><span class="line"></span><br><span class="line">wg.Add(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="keyword">defer</span> wg.Done()</span><br><span class="line">phase := <span class="string">"step3"</span></span><br><span class="line"><span class="keyword">if</span> common.S.CanTransition(payload, phase) &#123;</span><br><span class="line">fmt.Println(<span class="string">"Do Step 3 something"</span>)</span><br><span class="line">_ = common.S.Transition(payload, phase)</span><br><span class="line">&#125;</span><br><span class="line">&#125;()</span><br><span class="line">wg.Wait()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 多阶段的请看下，Ack非必须要，如果手动调用ack的话，那么等于一条条同步删除</span></span><br><span class="line"><span class="comment">// msource 在后台会定期检测runner中的状态机数据</span></span><br><span class="line"><span class="comment">//if err := common.S.Ack(payload); err != nil &#123;</span></span><br><span class="line"><span class="comment">//logbdev.Error(err)</span></span><br><span class="line"><span class="comment">//&#125;</span></span><br><span class="line"><span class="keyword">case</span> &lt;-ctx.Done():</span><br><span class="line">fmt.Println(<span class="string">"Done"</span>)</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;(i, chCtx)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 等待子协程结束</span></span><br><span class="line">wg.Wait()</span><br><span class="line"><span class="comment">// 等待msource退出</span></span><br><span class="line">common.S.Stop()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="小知识总结"><a href="#小知识总结" class="headerlink" title="小知识总结"></a>小知识总结</h1><h2 id="time组件"><a href="#time组件" class="headerlink" title="time组件"></a>time组件</h2><p>在开发的过程中，<code>time组件</code>用得还是比较多的，因为有各种异步任务在后台运行，常规的用法就不记录讲述了，这里说一下一些注意的点。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; After waits for the duration to elapse and then sends the current time</span><br><span class="line">&#x2F;&#x2F; on the returned channel.</span><br><span class="line">&#x2F;&#x2F; It is equivalent to NewTimer(d).C.</span><br><span class="line">&#x2F;&#x2F; The underlying Timer is not recovered by the garbage collector</span><br><span class="line">&#x2F;&#x2F; until the timer fires. If efficiency is a concern, use NewTimer</span><br><span class="line">&#x2F;&#x2F; instead and call Timer.Stop if the timer is no longer needed.</span><br><span class="line">func After(d Duration) &lt;-chan Time &#123;</span><br><span class="line">return NewTimer(d).C</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们看到这个<code>API</code>，如果想要用</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> &#123;</span><br><span class="line"><span class="keyword">select</span> &#123;</span><br><span class="line"><span class="keyword">case</span> &lt;-time.After(<span class="number">1</span>*time.Second)):</span><br><span class="line">fmt.Println(<span class="string">"时间到了"</span>)</span><br><span class="line"><span class="keyword">default</span>:</span><br><span class="line">fmt.Println(<span class="string">"go on"</span>)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>看到这个例子，如果我们这么用的话，每1秒都会重新创建一个Timer对象，不断在堆空间申请内存，然后gc-worker再大量回收没有再使用的对象内存。这就导致cpu做了额外的一些无效工作。</p><p>所以这种用法我是不推荐的。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(t *Timer)</span> <span class="title">Reset</span><span class="params">(d Duration)</span> <span class="title">bool</span></span> &#123;</span><br><span class="line"><span class="keyword">if</span> t.r.f == <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="built_in">panic</span>(<span class="string">"time: Reset called on uninitialized Timer"</span>)</span><br><span class="line">&#125;</span><br><span class="line">w := when(d)</span><br><span class="line"><span class="keyword">return</span> resetTimer(&amp;t.r, w)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们看到其实<code>Timer</code> 其实有一个<code>Reset</code>的API，我们可以对同一个timer进行<code>Reset</code>的操作，不断是重置时间即可。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">d := <span class="number">1</span>*time.Second</span><br><span class="line">t:= NewTimer(d)</span><br><span class="line"><span class="keyword">for</span> &#123;</span><br><span class="line"><span class="keyword">select</span> &#123;</span><br><span class="line"><span class="keyword">case</span> &lt;-t.C:</span><br><span class="line">t.Reset(d)</span><br><span class="line"><span class="keyword">default</span>:</span><br><span class="line">fmt.Println(<span class="string">"go on"</span>)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="make函数"><a href="#make函数" class="headerlink" title="make函数"></a>make函数</h2><p>make函数是一个很强大的函数，我们会经常使用到，但是有一些细节，需要大家知道的。</p><p><code>make([]byte,0,10)</code> 与 <code>make([]byte,10)</code> 这是2中不同的切片，对于可能刚学习golang的小伙伴来说，会有点疑惑，但是这是需要了解的，如果是三个参数的时候，一个是<code>cap</code>,一个是<code>len</code>，他们是有区别的。如果是三个参数的话，那代表当前大小<code>cap</code> = <code>len</code></p><p>我们经常会用三个参数来进行预分配空间，第二个参数默认都是填写0来进行优化，特别是我们在写<code>DB</code>的时候，用到了大量<code>[]byte</code>类型，在组装编码字节的时候，我们就需要使用这种方式来处理，否则。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a := <span class="built_in">make</span>([]<span class="keyword">byte</span>, <span class="number">0</span>, <span class="number">5</span>)</span><br><span class="line">a = <span class="built_in">append</span>(a, []<span class="keyword">byte</span>&#123;<span class="string">'a'</span>&#125;) <span class="comment">// a = [97]</span></span><br><span class="line"></span><br><span class="line">a := <span class="built_in">make</span>([]<span class="keyword">byte</span>, <span class="number">5</span>)</span><br><span class="line">a = <span class="built_in">append</span>(a, []<span class="keyword">byte</span>&#123;<span class="string">'a'</span>&#125;) <span class="comment">// a = [0,0,0,0,0,97]</span></span><br></pre></td></tr></table></figure><p>看到这里，大家就会明白区别。</p><h2 id="once函数"><a href="#once函数" class="headerlink" title="once函数"></a>once函数</h2><p>有些时候，我们想要保证只运行一次，这里，我们就需要借助 <code>sync.Once</code>，需要注意的是 一个<code>sync.Once</code>只能与一个函数绑定！</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">once := <span class="built_in">new</span>(sync.Once)</span><br><span class="line">callback:= <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123; fmt.Println(<span class="string">"我只想运行一次"</span>)&#125;</span><br><span class="line">once.Do(callback) <span class="comment">// 会 输出</span></span><br><span class="line"></span><br><span class="line">once.Do(callback) <span class="comment">// 无 输出</span></span><br><span class="line">once.Do(callback) <span class="comment">// 无 输出</span></span><br><span class="line">once.Do(callback) <span class="comment">// 无 输出</span></span><br></pre></td></tr></table></figure><h2 id="自定义marshal和unmarshal"><a href="#自定义marshal和unmarshal" class="headerlink" title="自定义marshal和unmarshal"></a>自定义marshal和unmarshal</h2><p>有时候，我们想要自己定义json的<code>marshal</code> 和 <code>unmarshal</code>，这里我们的<code>发号计数器</code> 就用到了，用它的原因其实是因为，我们的发号计数器在发号的过程中，其实是后台跑着一个异步任务在发号，所以在被反编码的时候，我们需要启动这个异步任务。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *counter)</span> <span class="title">UnmarshalJSON</span><span class="params">(data []<span class="keyword">byte</span>)</span> <span class="params">(err error)</span></span> &#123;</span><br><span class="line">c.IdKey = data[<span class="number">1</span> : <span class="built_in">len</span>(data)<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">// 重点关注这里</span></span><br><span class="line">turboNew(c)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *counter)</span> <span class="title">MarshalJSON</span><span class="params">()</span> <span class="params">([]<span class="keyword">byte</span>, error)</span></span> &#123;</span><br><span class="line">b := <span class="built_in">make</span>([]<span class="keyword">byte</span>, <span class="number">0</span>, <span class="built_in">len</span>(c.IdKey)+<span class="number">2</span>)</span><br><span class="line">b = <span class="built_in">append</span>(b, <span class="string">'"'</span>)</span><br><span class="line">b = <span class="built_in">append</span>(b, c.IdKey...)</span><br><span class="line">b = <span class="built_in">append</span>(b, <span class="string">'"'</span>)</span><br><span class="line"><span class="keyword">return</span> b, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">turboNew</span><span class="params">(c *counter)</span></span> &#123;</span><br><span class="line">ct := custom.Load().(*Custom)</span><br><span class="line">c.idChan = <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">int64</span>, ct.IdStep)</span><br><span class="line">c.sig = <span class="built_in">make</span>(<span class="keyword">chan</span> os.Signal, <span class="number">1</span>)</span><br><span class="line">c.ReadOptions = gorocksdb.NewDefaultReadOptions()</span><br><span class="line">signal.Notify(c.sig, syscall.SIGINT, syscall.SIGTERM)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 这里会启动一个异步任务</span></span><br><span class="line">c.sender()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *counter)</span> <span class="title">sender</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="comment">// 异步任务</span></span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="lockfree-queue和lockfree-stack"><a href="#lockfree-queue和lockfree-stack" class="headerlink" title="lockfree-queue和lockfree-stack"></a>lockfree-queue和lockfree-stack</h2><p>我们知道如果想要做到并发安全的话，普遍做法就是2种</p><ul><li>无锁化结构的设计（需要针对特定的业务常用，并且不允许乱用）</li><li>有锁结构</li></ul><p>无锁化(<code>lock-free</code>)的实现方式有很多种，在开发的过程中，我也有想过利用<code>lock-free-stack</code>以及<code>lock-free-queue</code>，分别想要运用在<code>RPN</code>的实现以及<code>发号器</code>当中，虽然后来发现用不到，但是可以拿到这里和大家分享一下。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// inrInt64 Increase</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">inrInt64</span><span class="params">(i *<span class="keyword">int64</span>)</span></span> &#123;</span><br><span class="line">t := <span class="keyword">int64</span>(+<span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> &#123;</span><br><span class="line">value := atomic.LoadInt64(i)</span><br><span class="line"><span class="keyword">if</span> atomic.CompareAndSwapInt64(i, value, value+t) &#123;</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line">time.Sleep(time.Nanosecond)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// dcrInt64 Decrease</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">dcrInt64</span><span class="params">(i *<span class="keyword">int64</span>)</span></span> &#123;</span><br><span class="line">t := <span class="keyword">int64</span>(<span class="number">-1</span>)</span><br><span class="line"><span class="keyword">for</span> &#123;</span><br><span class="line">value := atomic.LoadInt64(i)</span><br><span class="line"><span class="keyword">if</span> atomic.CompareAndSwapInt64(i, value, value+t) &#123;</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line">time.Sleep(time.Nanosecond)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// LKStack returns an empty queue.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">NewLKStack</span><span class="params">()</span> *<span class="title">LKStack</span></span> &#123;</span><br><span class="line">n := unsafe.Pointer(&amp;node&#123;&#125;)</span><br><span class="line"><span class="keyword">return</span> &amp;LKStack&#123;head: n&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// LKStack is a lock-free unbounded stack.</span></span><br><span class="line"><span class="keyword">type</span> LKStack <span class="keyword">struct</span> &#123;</span><br><span class="line"><span class="built_in">len</span>  <span class="keyword">int64</span></span><br><span class="line">head unsafe.Pointer</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(q *LKStack)</span> <span class="title">IsEmpty</span><span class="params">()</span> <span class="title">bool</span></span> &#123;</span><br><span class="line"><span class="keyword">return</span> q.Len() == <span class="number">0</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(q *LKStack)</span> <span class="title">Len</span><span class="params">()</span> <span class="title">int64</span></span> &#123;</span><br><span class="line"><span class="keyword">return</span> q.<span class="built_in">len</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// LKStack puts the given value v at the tail of the stack.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(q *LKStack)</span> <span class="title">Push</span><span class="params">(v <span class="keyword">interface</span>&#123;&#125;)</span></span> &#123;</span><br><span class="line">n := &amp;node&#123;value: v&#125;</span><br><span class="line"><span class="keyword">for</span> &#123;</span><br><span class="line">head := load(&amp;q.head)</span><br><span class="line">next := load(&amp;n.next)</span><br><span class="line">cas(&amp;n.next, next, head)</span><br><span class="line"><span class="keyword">if</span> cas(&amp;q.head, head, n) &#123;</span><br><span class="line">inrInt64(&amp;q.<span class="built_in">len</span>)</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">time.Sleep(time.Nanosecond)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Pop removes and returns the value at the head of the stack.</span></span><br><span class="line"><span class="comment">// It returns nil if the stack is empty.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(q *LKStack)</span> <span class="title">Pop</span><span class="params">()</span> <span class="title">interface</span></span>&#123;&#125; &#123;</span><br><span class="line"><span class="keyword">for</span> &#123;</span><br><span class="line">head := load(&amp;q.head)</span><br><span class="line">next := load(&amp;head.next)</span><br><span class="line"><span class="keyword">if</span> next == <span class="literal">nil</span> &#123; <span class="comment">// is stack empty?</span></span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="comment">// read value before CAS otherwise another pop might free the next node</span></span><br><span class="line">v := head.value</span><br><span class="line"><span class="keyword">if</span> cas(&amp;q.head, head, next) &#123;</span><br><span class="line">dcrInt64(&amp;q.<span class="built_in">len</span>)</span><br><span class="line"><span class="keyword">return</span> v <span class="comment">// Pop is done.  return</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">time.Sleep(time.Nanosecond)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// load from atomic load pointer node</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">load</span><span class="params">(p *unsafe.Pointer)</span> <span class="params">(n *node)</span></span> &#123;</span><br><span class="line"><span class="keyword">return</span> (*node)(atomic.LoadPointer(p))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// cas swap set</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">cas</span><span class="params">(p *unsafe.Pointer, old, <span class="built_in">new</span> *node)</span> <span class="params">(ok <span class="keyword">bool</span>)</span></span> &#123;</span><br><span class="line"><span class="keyword">return</span> atomic.CompareAndSwapPointer(</span><br><span class="line">p, unsafe.Pointer(old), unsafe.Pointer(<span class="built_in">new</span>))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里也不过多在这里描述了，我们大家查看源码吧，主要就是利用了<code>atomic</code>包中的<code>原子性</code>操作<code>CompareAndSwapXxx</code>, 因为这是一个原子性的指令，合理的运用即可做到<code>无锁化并发安全</code>的结构。</p><p><code>atomic</code>包的<code>CompareAndSwapXxx</code>其实就是一个<code>CAS</code>的理念，用<code>乐观锁(逻辑锁)</code>来做数据处理。</p><h2 id="unsafa包中的指针的作用：零拷贝string和byte的转换"><a href="#unsafa包中的指针的作用：零拷贝string和byte的转换" class="headerlink" title="unsafa包中的指针的作用：零拷贝string和byte的转换"></a>unsafa包中的指针的作用：零拷贝string和byte的转换</h2><p><code>零拷贝(zero-copy)</code>，传统较多的说法就是无需经过用户态到内核态到数据copy，即可做到想做的事情。 通俗一点就是不经过copy就能转换数据。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> StringHeader <span class="keyword">struct</span> &#123;</span><br><span class="line">    Data <span class="keyword">uintptr</span></span><br><span class="line">    Len  <span class="keyword">int</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> SliceHeader <span class="keyword">struct</span> &#123;</span><br><span class="line">    Data <span class="keyword">uintptr</span></span><br><span class="line">    Len  <span class="keyword">int</span></span><br><span class="line">    Cap  <span class="keyword">int</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这是String和slice的底层数据结构，他们基本是一致的，区别其实就是在于一个有Cap，一个是固定的Len。</p><p>只需要共享底层 Data 和 Len 就可以实现 zero-copy。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">string2bytes</span><span class="params">(s <span class="keyword">string</span>)</span> []<span class="title">byte</span></span> &#123;</span><br><span class="line"><span class="keyword">return</span> *(*[]<span class="keyword">byte</span>)(unsafe.Pointer(&amp;s))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">bytes2string</span><span class="params">(b []<span class="keyword">byte</span>)</span> <span class="title">string</span></span> &#123;</span><br><span class="line"><span class="keyword">return</span> *(*<span class="keyword">string</span>)(unsafe.Pointer(&amp;b))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="context控制上下文也讲解一下"><a href="#context控制上下文也讲解一下" class="headerlink" title="context控制上下文也讲解一下"></a>context控制上下文也讲解一下</h2><p>我们这里用到了大量协程，他们之间有一些或许是有上下文关系的，因此，我们这里就需要用到<code>context</code>来对协程进行一个上下文的管理，做到协助的作用。</p><p>特别是我们在退出程序的时候，我们想要某一些异步任务<code>优雅</code>,<code>可靠</code>,<code>安全</code>的退出程序，那么我们就需要用到context来控制每个后台运行的程序。</p><p>我们这里用到比较多的其实就是 <code>context.WithCancel(ctx)</code>， 我们需要管理每个协程的退出需要做的事情，例如：我需要msource在退出的时候，保存一下当前在内存中最新的数据到rocksdb中，那么这个时候context的作用就十分有效了。</p><h2 id="pprof的查看"><a href="#pprof的查看" class="headerlink" title="pprof的查看"></a>pprof的查看</h2><p>要利用pprof粗略查看性能，及时它不能准确的反馈出所有的问题，起码它能帮助我们在前面的大问题上更容易发现问题。</p><h2 id="sync-Pool如何做到优化"><a href="#sync-Pool如何做到优化" class="headerlink" title="sync.Pool如何做到优化"></a>sync.Pool如何做到优化</h2><ul><li>对STW暂停时间做了优化, 避免大的sync.Pool严重影响STW时间</li><li>第二个优化是GC时入对<code>sync.Pool</code>进行回收，不会一次将池化对象全部回收，这就避免了<code>sync.Pool</code>释放对象和重建对象导致的性能尖刺，造福于<code>sync.Pool</code>重度用户。</li><li>第三个就是对性能的优化。</li><li>对以上的改进主要是两次提交：<br><a href="https://github.com/golang/go/commit/d5fd2dd6a17a816b7dfd99d4df70a85f1bf0de31#diff-491b0013c82345bf6cfa937bd78b690d" target="_blank" rel="noopener">sync: use lock-free structure for Pool stealing</a><br><a href="https://github.com/golang/go/commit/2dcbf8b3691e72d1b04e9376488cef3b6f93b286#diff-491b0013c82345bf6cfa937bd78b690d" target="_blank" rel="noopener">sync: smooth out Pool behavior over GC with a victim cache</a></li></ul><p>分别是用到了<code>无锁化结构</code> 以及程序GC的行为的优化</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;&lt;code&gt;CAP&lt;/code&gt;原则又称CAP定理，指的是在一个分布式系统中，&lt;code&gt;一致性（Consistency）&lt;/code&gt;、&lt;code&gt;可用性（Availability）&lt;/code&gt;、&lt;code&gt;分区容错性（Partition tolerance）&lt;/code&gt;。CAP 原则指的是，这三个要素最多只能同时实现两点，不可能三者兼顾。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;msource&lt;/code&gt; 是我们的一个 &lt;code&gt;数据源组件&lt;/code&gt;，我们所有的大数据ETL服务都构建在此之上，所以我们msource可以说是所有业务系统的核心。他维护着一个稳定，可靠，高性能的数据传输机制。让我们 &lt;code&gt;业务层&lt;/code&gt; 中可以做各种操作，同步，异步等等。&lt;/p&gt;
&lt;p&gt;msource 的角色我大体分为了2种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;spout （数据推送组件)&lt;/li&gt;
&lt;li&gt;db （数据存储组件）&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="数据结构" scheme="http://blog.crazylaw.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
      <category term="数据库" scheme="http://blog.crazylaw.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="Kafka" scheme="http://blog.crazylaw.cn/tags/Kafka/"/>
    
      <category term="TiDB" scheme="http://blog.crazylaw.cn/tags/TiDB/"/>
    
      <category term="RocketDB" scheme="http://blog.crazylaw.cn/tags/RocketDB/"/>
    
      <category term="SQL" scheme="http://blog.crazylaw.cn/tags/SQL/"/>
    
  </entry>
  
  <entry>
    <title>【Golang】- unsafe Pointer</title>
    <link href="http://blog.crazylaw.cn/2021/01/14/Golang/unsafe-Pointer/"/>
    <id>http://blog.crazylaw.cn/2021/01/14/Golang/unsafe-Pointer/</id>
    <published>2021-01-14T03:37:12.000Z</published>
    <updated>2021-03-20T16:25:01.799Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>由于目前在使用使用一些go写的odbc库，里面涉及到一些cgo的内容，那就避不开内存和指针等问题，在这边文章中记录一下<code>unsafe Pointer</code> 和 <code>uintptr</code> 的相关内容。</p><p>Go语言在设计的时候，为了编写方便、效率高以及降低复杂度，被设计成为一门强类型的静态语言。强类型意味着一旦定义了，它的类型就不能改变了；静态意味着类型检查在运行前就做了。</p><p>同时为了安全的考虑，Go语言是不允许两个指针类型进行转换的。</p><a id="more"></a><h2 id="指针类型转换"><a href="#指针类型转换" class="headerlink" title="指针类型转换"></a>指针类型转换</h2><p>我们一般使用<code>*T</code>作为一个指针类型，表示一个指向类型<code>T变量</code>的<code>指针</code>。为了安全的考虑，两个不同的指针类型不能相互转换，比如<code>*int</code>不能转为<code>*float64</code>。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">i:= <span class="number">10</span></span><br><span class="line">ip:=&amp;i</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> fp *<span class="keyword">float64</span> = (*<span class="keyword">float64</span>)(ip)</span><br><span class="line"></span><br><span class="line">fmt.Println(fp)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>以上代码我们在编译的时候，会提示 <code>cannot convert ip (type *int) to type *float64</code>，也就是不能进行强制转型。那如果我们还是需要进行转换怎么做呢？这就需要我们使用 <code>unsafe包</code> 里的 <code>Pointer</code> 了，下面我们先看看 <code>unsafe.Pointer</code> 是什么，然后再介绍如何转换。</p><h2 id="unsafe-Pointer"><a href="#unsafe-Pointer" class="headerlink" title="unsafe.Pointer"></a>unsafe.Pointer</h2><p><code>unsafe.Pointer</code> 是一种特殊意义的指针，它可以包含任意类型的地址，有点类似于 <code>C语言</code> 里的 <code>void*</code> 指针，全能型的。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">i:= <span class="number">10</span></span><br><span class="line">ip:=&amp;i</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> fp *<span class="keyword">float64</span> = (*<span class="keyword">float64</span>)(unsafe.Pointer(ip))</span><br><span class="line"></span><br><span class="line">*fp = *fp * <span class="number">3</span></span><br><span class="line"></span><br><span class="line">fmt.Println(i)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>以上示例，我们可以把 <code>*int</code> 转为 <code>*float64</code> ,并且我们尝试了对新的 <code>*float64</code> 进行操作，打印输出i，就会发现i的址同样被改变。</p><p>以上这个例子没有任何实际的意义，但是我们说明了，通过 <code>unsafe.Pointer</code> 这个万能的指针，我们可以在 <code>*T</code> 之间做任何转换。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> ArbitraryType <span class="keyword">int</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> Pointer *ArbitraryType</span><br></pre></td></tr></table></figure><p>可以看到 <code>unsafe.Pointer</code> 其实就是一个 <code>*int</code>,一个通用型的指针。</p><p>我们看下关于 <code>unsafe.Pointer</code> 的4个规则。</p><ul><li><code>任何指针</code> 都可以转换为 <code>unsafe.Pointer</code></li><li><code>unsafe.Pointer</code> 可以转换为 <code>任何指针</code></li><li><code>uintptr</code> 可以转换为 <code>unsafe.Pointer</code></li><li><code>unsafe.Pointer</code> 可以转换为 <code>uintptr</code></li></ul><p>前面两个规则我们刚刚已经演示了，主要用于 <code>*T1</code> 和 <code>*T2</code> 之间的转换，那么最后两个规则是做什么的呢？我们都知道 <code>*T</code> 是 <code>不能计算偏移量</code> 的，也不能进行计算，<code>但是uintptr可以</code>，所以我们可以把<code>指针</code>转为<code>uintptr</code>再<code>进行偏移计算</code>，这样我们就可以<code>访问特定的内存</code>了，达到对不同的<code>内存读写</code>的目的。</p><p>下面我们以通过<code>指针偏移</code> 修改Struct结构体内的字段为例，来演示 <code>uintptr</code> 的用法。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">u:=<span class="built_in">new</span>(user)</span><br><span class="line">fmt.Println(*u)</span><br><span class="line"></span><br><span class="line">pName:=(*<span class="keyword">string</span>)(unsafe.Pointer(u))</span><br><span class="line">*pName=<span class="string">"张三"</span></span><br><span class="line"></span><br><span class="line">pAge:=(*<span class="keyword">int</span>)(unsafe.Pointer(<span class="keyword">uintptr</span>(unsafe.Pointer(u))+unsafe.Offsetof(u.age)))</span><br><span class="line">*pAge = <span class="number">20</span></span><br><span class="line"></span><br><span class="line">fmt.Println(*u)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> user <span class="keyword">struct</span> &#123;</span><br><span class="line">name <span class="keyword">string</span></span><br><span class="line">age <span class="keyword">int</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>以上我们通过内存偏移的方式，定位到我们需要操作的字段，然后改变他们的值。</p><p>第一个修改user的name值的时候，<code>因为name是第一个字段，所以不用偏移</code>，我们获取user的指针，然后通过 <code>unsafe.Pointer</code> 转为<code>*string</code> 进行赋值操作<code>即可</code>。</p><p>第二个修改user的age值的时候，<code>因为age不是第一个字段，所以我们需要内存偏移</code>，内存偏移牵涉到的计算只能通过<code>uintptr</code>，所我们要先把<code>user的指针地址</code>转为<code>uintptr</code>，然后我们再通过<code>unsafe.Offsetof(u.age)</code>获取需要偏移的值，进行<code>地址运算(+)偏移</code>即可。</p><p>现在偏移后，地址已经是user的age字段了，如果要给它赋值，我们需要把<code>uintptr</code>转为<code>*int</code>才可以。所以我们通过把<code>uintptr</code>转为<code>unsafe.Pointer</code>,再转为<code>*int</code>就可以操作了。</p><p>这里我们可以看到，我们第二个偏移的表达式非常长，但是也千万不要把他们分段，不能像下面这样。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">temp:=<span class="keyword">uintptr</span>(unsafe.Pointer(u))+unsafe.Offsetof(u.age)</span><br><span class="line">pAge:=(*<span class="keyword">int</span>)(unsafe.Pointer(temp))</span><br><span class="line">*pAge = <span class="number">20</span></span><br></pre></td></tr></table></figure><p>栈内指针在栈扩容的时候，有了新的地址，因此<code>uintptr</code>只能作为指针计算的中间态，<code>不允许</code>使用变量<code>保存uintptr的值</code>。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p><code>unsafe是不安全的，所以我们应该尽可能少的使用它</code>，比如内存的操纵，这是绕过Go本身设计的安全机制的，不当的操作，可能会破坏一块内存，而且这种问题非常不好定位。</p><p>当然必须的时候我们可以使用它，比如底层类型相同的数组之间的转换；比如使用sync/atomic包中的一些函数时；还有访问Struct的私有字段时，该用还是要用，不过一定要慎之又慎。</p><p><code>整个unsafe包都是用于Go编译器的，不用运行时，在我们编译的时候，Go编译器已经把他们都处理了</code>。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;由于目前在使用使用一些go写的odbc库，里面涉及到一些cgo的内容，那就避不开内存和指针等问题，在这边文章中记录一下&lt;code&gt;unsafe Pointer&lt;/code&gt; 和 &lt;code&gt;uintptr&lt;/code&gt; 的相关内容。&lt;/p&gt;
&lt;p&gt;Go语言在设计的时候，为了编写方便、效率高以及降低复杂度，被设计成为一门强类型的静态语言。强类型意味着一旦定义了，它的类型就不能改变了；静态意味着类型检查在运行前就做了。&lt;/p&gt;
&lt;p&gt;同时为了安全的考虑，Go语言是不允许两个指针类型进行转换的。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Golang" scheme="http://blog.crazylaw.cn/categories/Golang/"/>
    
    
      <category term="Golang" scheme="http://blog.crazylaw.cn/tags/Golang/"/>
    
  </entry>
  
  <entry>
    <title>【大数据】- flume + kudu 操作指南</title>
    <link href="http://blog.crazylaw.cn/2021/01/05/%E5%A4%A7%E6%95%B0%E6%8D%AE/flume+kudu/"/>
    <id>http://blog.crazylaw.cn/2021/01/05/%E5%A4%A7%E6%95%B0%E6%8D%AE/flume+kudu/</id>
    <published>2021-01-05T01:56:40.000Z</published>
    <updated>2021-03-20T16:25:01.815Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>由于我们要尝试使用kudu，架构是由flume-&gt;kudu</p><a id="more"></a><p>[kudu-flume-sink(v1.9)] (<a href="https://github.com/apache/kudu/blob/1.9.0/java/kudu-flume-sink/src/main/java/org/apache/kudu/flume/sink/KuduSink.java" target="_blank" rel="noopener">https://github.com/apache/kudu/blob/1.9.0/java/kudu-flume-sink/src/main/java/org/apache/kudu/flume/sink/KuduSink.java</a>)</p><h2 id="1-9-Configuration-of-KuDu-Sink"><a href="#1-9-Configuration-of-KuDu-Sink" class="headerlink" title="1.9 Configuration of KuDu Sink:  "></a>1.9 Configuration of KuDu Sink:  </h2><table><thead><tr><th>Property Name</th><th align="center">Default</th><th align="center">Required</th><th align="left">Description</th></tr></thead><tbody><tr><td>channel</td><td align="center">-</td><td align="center">Yes</td><td align="left">要绑定读取的channel</td></tr><tr><td>type</td><td align="center">-</td><td align="center">Yes</td><td align="left">组件名，必须填写<code>org.apache.kudu.flume.sink.KuduSink</code></td></tr><tr><td>masterAddresses</td><td align="center">-</td><td align="center">Yes</td><td align="left">逗号分隔kudu master地址，例子: <code>host1:port1,host2:port2</code>,其中端口是选填</td></tr><tr><td>tableName</td><td align="center">-</td><td align="center">Yes</td><td align="left">要写入的kudu表名</td></tr><tr><td>batchSize</td><td align="center">1000</td><td align="center">No</td><td align="left">sink每批次处理最大数</td></tr><tr><td>ignoreDuplicateRows</td><td align="center">true</td><td align="center">No</td><td align="left">是否忽略插入导致的重复主键错误。</td></tr><tr><td>timeoutMillis</td><td align="center">10000</td><td align="center">No</td><td align="left">Kudu写操作的超时时间，单位为毫秒</td></tr><tr><td>producer</td><td align="center">SimpleKuduOperationsProducer</td><td align="center">No</td><td align="left">接收器应该使用实现了的<code>KuduOperationsProducer</code> 接口的的完全限定类名。</td></tr><tr><td>producer.*</td><td align="center">-</td><td align="center">(Varies by operations producer)</td><td align="left">要传递给操作生产者实现的配置属性。</td></tr></tbody></table><blockquote><p>由于这种方式必须一个sink对应一个table，不符合我们的使用场景，我们该用了其他方案。文章待定。</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;由于我们要尝试使用kudu，架构是由flume-&amp;gt;kudu&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://blog.crazylaw.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="大数据" scheme="http://blog.crazylaw.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="flume" scheme="http://blog.crazylaw.cn/tags/flume/"/>
    
      <category term="kudu" scheme="http://blog.crazylaw.cn/tags/kudu/"/>
    
  </entry>
  
  <entry>
    <title>【大数据】- 记一次华为云大数据服务对接问题记录</title>
    <link href="http://blog.crazylaw.cn/2021/01/05/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E8%AE%B0%E4%B8%80%E6%AC%A1%E5%8D%8E%E4%B8%BA%E4%BA%91%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%9C%8D%E5%8A%A1%E5%AF%B9%E6%8E%A5%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/"/>
    <id>http://blog.crazylaw.cn/2021/01/05/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E8%AE%B0%E4%B8%80%E6%AC%A1%E5%8D%8E%E4%B8%BA%E4%BA%91%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%9C%8D%E5%8A%A1%E5%AF%B9%E6%8E%A5%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/</id>
    <published>2021-01-05T01:56:40.000Z</published>
    <updated>2021-03-20T16:25:01.816Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>由于我们在调研是否让自建IDC大数据机房上云服务，所以华为云进行一下测试。</p><a id="more"></a><h2 id="问题一：impala的连接问题"><a href="#问题一：impala的连接问题" class="headerlink" title="问题一：impala的连接问题"></a>问题一：impala的连接问题</h2><p>由于我们开启了<code>Kerberos</code>，所以我们在终端执行<code>impala-shell</code>的时候，默认情况下是连不上的。需要通过<code>Kerberos</code>进行<code>身份校验</code>和<code>授权</code>才可以访问。</p><p>我们创建了一个名字叫<code>hiveuser</code>的账号，目前所有的组件服务都通过该账号进行访问。</p><p>所以我们需要初始化和续期凭据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@node-str-coreoVpr ~]# impala-shell</span><br><span class="line">Starting Impala Shell without Kerberos authentication</span><br><span class="line">Error connecting: TTransportException, Could not connect to node-str-coreoVpr:21000</span><br><span class="line">Kerberos ticket found in the credentials cache, retrying the connection with a secure transport.</span><br><span class="line">Error connecting: TTransportException, Could not connect to node-str-coreoVpr:21000</span><br><span class="line">***********************************************************************************</span><br><span class="line">Welcome to the Impala shell.</span><br><span class="line">(Impala Shell v3.2.0 (f63543a) built on Wed Nov  6 11:46:33 CST 2019)</span><br><span class="line"></span><br><span class="line">Want to know what version of Impala you&#39;re connected to? Run the VERSION command to</span><br><span class="line">find out!</span><br><span class="line">***********************************************************************************</span><br><span class="line">[Not connected] &gt; quit;</span><br><span class="line">Connection lost, reconnecting...</span><br><span class="line">Error connecting: TTransportException, Could not connect to node-str-coreoVpr:21000</span><br><span class="line">Goodbye root</span><br></pre></td></tr></table></figure><p>可以看到，我们是连不上的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@node-str-coreoVpr ~]# kinit hiveuser@122451B2_394D_494B_9FCA_B045F596D6D4.COM</span><br><span class="line">Password for hiveuser@122451B2_394D_494B_9FCA_B045F596D6D4.COM:</span><br><span class="line"></span><br><span class="line">[root@node-str-coreoVpr ~]# klist</span><br><span class="line">Ticket cache: FILE:&#x2F;tmp&#x2F;krb5cc_0</span><br><span class="line">Default principal: hiveuser@122451B2_394D_494B_9FCA_B045F596D6D4.COM</span><br><span class="line"></span><br><span class="line">Valid starting       Expires              Service principal</span><br><span class="line">01&#x2F;05&#x2F;2021 17:05:25  01&#x2F;06&#x2F;2021 17:05:21  krbtgt&#x2F;122451B2_394D_494B_9FCA_B045F596D6D4.COM@122451B2_394D_494B_9FCA_B045F596D6D4.COM</span><br></pre></td></tr></table></figure><p>再次尝试连接</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@node-str-coreoVpr ~]# impala-shell -i node-ana-coretXnL</span><br><span class="line">Starting Impala Shell without Kerberos authentication</span><br><span class="line">Opened TCP connection to node-ana-coretXnL:21000</span><br><span class="line">Error connecting: TTransportException, TSocket read 0 bytes</span><br><span class="line">Kerberos ticket found in the credentials cache, retrying the connection with a secure transport.</span><br><span class="line">Opened TCP connection to node-ana-coretXnL:21000</span><br><span class="line">Connected to node-ana-coretXnL:21000</span><br><span class="line">Server version: impalad version 3.2.0 RELEASE (build 83150778f5d85f48878f611da47face9328e9e6a)</span><br><span class="line">***********************************************************************************</span><br><span class="line">Welcome to the Impala shell.</span><br><span class="line">(Impala Shell v3.2.0 (f63543a) built on Wed Nov  6 11:46:33 CST 2019)</span><br><span class="line"></span><br><span class="line">Press TAB twice to see a list of available commands.</span><br><span class="line">***********************************************************************************</span><br><span class="line">[node-ana-coretXnL:21000] default&gt;</span><br></pre></td></tr></table></figure><p>可以看到，已经可以连接上了。</p><h2 id="问题一：flume-amp-amp-kudu"><a href="#问题一：flume-amp-amp-kudu" class="headerlink" title="问题一：flume &amp;&amp; kudu"></a>问题一：flume &amp;&amp; kudu</h2><p>目前，我们对基本配置如下：</p><table><thead><tr><th>服务</th><th align="center">版本</th></tr></thead><tbody><tr><td>flume</td><td align="center">1.6</td></tr><tr><td>kudu</td><td align="center">1.9</td></tr></tbody></table><blockquote><p>Kerberos</p></blockquote><p>由于flume官方没有提供对应的sink。但是kudu有提供，所以我去找了kudu的sink的jar库下来。而版本对应嘛，一开始我们使用和kudu一样的1.9版本，发现api对应不上。查阅了资料之后，选择了1.4版本。</p><p><a href="https://mvnrepository.com/artifact/org.apache.kudu/kudu-flume-sink/1.4.0?__cf_chl_captcha_tk__=7487a6e6d3af56e81833e042ecdd828beb02882a-1609912123-0-AUmmEDCceLCSAVoPRrj8gzU7DJiq0s6FDvbgtDuPNfnkQTpt6VhrByHJNPAW6OqwOUwy0Q-n6GMXMkdwYsWU3MaV8wittp6D5FDijZeJqG8TNgPpuqc3aP1HKMEpyWvRWHUt3zuktylr8P2yT1PNxi1072tE1YU0DpCJAyDsd9hXVKFgATWgSqysO-VYDm7734rOkyA_hIGFIlm_M9SmJoeKnXkVtVuYEVfvioLp_jJ-fYtJ7JXb2Flv-84qw5_CACpgSG6MQclMt-KjamTut5gPyUm_I0RhwSyMzsC07oSFq47KV0ckhsXoghJud1QsxqlSu2VNpR8IjA23bp1qIPLGmxxrQ0GEcZuCjBMUtLSkPHGzTYgjXu6Os5LMP8deUirp5OLmJ-9Jma8FSRvW5tIkQEpuHw9cUvhAqQSmONyFDTUal_War9HvzntivDc4bhsmuDc7t2spTT8_3Y6mwWUuQD8nE4qxlainXMtXIoeK_FD5O8qNFLKAdzL2cudLjlATsSjSSNMXRzlgGf73c_yieLyjHA4vgvxswnmfRgEh-tnfLVW8_U58OkahQxNs4kWVVjgPAr4lARgFeRV1FxQKYMvbrwHcnxL8RZyvMYIAPmsKF9ClFK-UyuxFKpT3ezseQWzEeTmliATFwT_ope0" target="_blank" rel="noopener">kudu-flume-sink-1.4.jar</a></p><p>需要把jar包放在 java运行程序的 <code>classpath目录下</code>，也就是 <code>-cp 参数的目录下</code>。由于不想通过修改<code>GC_OPTS</code>来指定目录，所以我选择了放在 <code>/opt/Bigdata/MRS_2.1.0/install/FusionInsight-Flume-1.6.0/flume/lib/*</code> 目录下。</p><blockquote><p>可是谁曾想最后还要是动到 <code>GC_OPTS</code>到参数配置</p></blockquote><p>还是类似的问题，目前遇到的问题几乎都是 <code>Kerberos</code> 引起的。因为以往我们并没有使用 <code>Kerberos</code> 作为身份认证。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"># &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line"># &#x3D;                  定义信息                       &#x3D;</span><br><span class="line"># &#x3D; 华为云Agent必填:                                &#x3D;</span><br><span class="line"># &#x3D;  - server                                       &#x3D;</span><br><span class="line"># &#x3D; Agent中存在的sources:                           &#x3D;</span><br><span class="line"># &#x3D;  - src_http_41600                               &#x3D;</span><br><span class="line"># &#x3D; Agent中存在的channels:                          &#x3D;</span><br><span class="line"># &#x3D;  - ch_kudu_table                                &#x3D;</span><br><span class="line"># &#x3D; Agent中存在的sinks:                             &#x3D;</span><br><span class="line"># &#x3D;  - sink_kudu_table                              &#x3D;</span><br><span class="line"># &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">server.sources  &#x3D; src_http_41600</span><br><span class="line">server.channels &#x3D; ch_kudu_table</span><br><span class="line">server.sinks    &#x3D; sink_kudu_table</span><br><span class="line"></span><br><span class="line"># &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line"># &#x3D;               Http Source                       &#x3D;</span><br><span class="line"># &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">server.sources.src_http_41600.type &#x3D; http</span><br><span class="line">server.sources.src_http_41600.port &#x3D; 41600</span><br><span class="line">server.sources.src_http_41600.channels &#x3D; ch_kudu_table</span><br><span class="line"></span><br><span class="line"># &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line"># &#x3D;               Http-Kudu&#39;s Channel               &#x3D;</span><br><span class="line"># &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">server.channels.ch_kudu_table.type &#x3D; memory</span><br><span class="line">server.channels.ch_kudu_table.capacity &#x3D; 1000</span><br><span class="line">server.channels.ch_kudu_table.transactionCapacity &#x3D; 100</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># server.sinks.sink_kudu_table.type &#x3D; logger</span><br><span class="line"># server.sinks.sink_kudu_table.channel &#x3D; ch_kudu_table</span><br><span class="line"></span><br><span class="line"># &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line"># &#x3D;                    Kudu Sink                    &#x3D;</span><br><span class="line"># &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line"># 组件名，必须填写&#96;org.apache.kudu.flume.sink.KuduSink&#96;</span><br><span class="line">server.sinks.sink_kudu_table.type &#x3D; org.apache.kudu.flume.sink.KuduSink</span><br><span class="line"># 要绑定读取的channel</span><br><span class="line">server.sinks.sink_kudu_table.channel &#x3D; ch_kudu_table</span><br><span class="line">server.sinks.sink_kudu_table.masterAddresses &#x3D; node-master1rfFB,node-ana-corezDdi,node-master2RSDt</span><br><span class="line">server.sinks.sink_kudu_table.tableName &#x3D; impala::kudu_test.my_first_table</span><br></pre></td></tr></table></figure><p>测试的时候所采用的配置如上</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">omm      17051     1  0 11:31 ?        00:00:09 &#x2F;opt&#x2F;Bigdata&#x2F;jdk1.8.0_212&#x2F;&#x2F;bin&#x2F;java -XX:OnOutOfMemoryError&#x3D;bash &#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;install&#x2F;FusionInsight-Flume-1.6.0&#x2F;flume&#x2F;bin&#x2F;out_memory_error.sh &#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;1_7_Flume&#x2F;etc %p -Xms2G -Xmx4G -XX:CMSFullGCsBeforeCompaction&#x3D;1 -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:+UseCMSCompactAtFullCollection -Djava.security.krb5.conf&#x3D;&#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;1_7_Flume&#x2F;etc&#x2F;krb5.conf -Djava.security.auth.login.config&#x3D;&#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;1_7_Flume&#x2F;etc&#x2F;jaas.conf -Dzookeeper.request.timeout&#x3D;120000 -Djavax.security.auth.useSubjectCredsOnly&#x3D;false -verbose:gc -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles&#x3D;10 -XX:GCLogFileSize&#x3D;1M -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:&#x2F;var&#x2F;log&#x2F;Bigdata&#x2F;flume&#x2F;&#x2F;flume&#x2F;flume-omm-20210106113125-%p-gc.log -Djava.security.krb5.conf&#x3D;&#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;1_5_KerberosClient&#x2F;etc&#x2F;kdc.conf -Djava.security.auth.login.config&#x3D;&#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;1_7_Flume&#x2F;etc&#x2F;jaas.conf -Dzookeeper.server.principal&#x3D;zookeeper&#x2F;hadoop.122451b2_394d_494b_9fca_b045f596d6d4.com -Dzookeeper.request.timeout&#x3D;120000 -Dsolrclient.token.enabled&#x3D;false -Dcom.amazonaws.sdk.disableCertChecking&#x3D;true -Dnet.sf.ehcache.skipUpdateCheck&#x3D;true -Dflume.instance.id&#x3D;1000 -Dflume.role&#x3D;server -Dlog4j.configuration.watch&#x3D;true -Dlog4j.configuration&#x3D;log4j.properties -Dflume_log_dir&#x3D;&#x2F;var&#x2F;log&#x2F;Bigdata&#x2F;flume&#x2F;&#x2F;flume&#x2F; -Dflume.monitoring.type&#x3D;http -Dflume.monitoring.port&#x3D;21150 -Dbeetle.application.home.path&#x3D;&#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;install&#x2F;FusionInsight-Flume-1.6.0&#x2F;flume&#x2F;conf&#x2F;service -Dflume.called.from.service -Dflume.conf.dir&#x3D;&#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;1_7_Flume&#x2F;etc -Dflume.metric.conf.dir&#x3D;&#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;install&#x2F;FusionInsight-Flume-1.6.0&#x2F;flume&#x2F;conf -Dflume.script.home&#x3D;&#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;install&#x2F;FusionInsight-Flume-1.6.0&#x2F;flume&#x2F;bin -cp &#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;1_7_Flume&#x2F;etc:&#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;install&#x2F;FusionInsight-Flume-1.6.0&#x2F;flume&#x2F;lib&#x2F;*:&#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;install&#x2F;FusionInsight-Flume-1.6.0&#x2F;flume&#x2F;conf&#x2F;service&#x2F; -Djava.library.path&#x3D;&#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;install&#x2F;FusionInsight-Flume-1.6.0&#x2F;flume&#x2F;plugins.d&#x2F;native&#x2F;native org.apache.flume.node.Application --conf-file &#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;1_7_Flume&#x2F;etc&#x2F;properties.properties --name server</span><br></pre></td></tr></table></figure><p>启动的参数如上，其中有几个参数是我后面加的：</p><ul><li>-Djava.security.krb5.conf=/opt/Bigdata/MRS_2.1.0/1_7_Flume/etc/krb5.conf   （指定krb5的配置文件路径）</li><li>-Djava.security.auth.login.config=/opt/Bigdata/MRS_2.1.0/1_7_Flume/etc/jaas.conf (获取jass的认证配置文件路径)</li><li>-Dzookeeper.request.timeout=120000</li><li>-Djavax.security.auth.useSubjectCredsOnly=false (通过底层获取凭据)</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">[root@node-str-coreoVpr ~]# cat &#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;1_7_Flume&#x2F;etc&#x2F;jaas.conf</span><br><span class="line">KafkaClient &#123;</span><br><span class="line">com.sun.security.auth.module.Krb5LoginModule required</span><br><span class="line">useKeyTab&#x3D;true</span><br><span class="line">keyTab&#x3D;&quot;&#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;1_7_Flume&#x2F;etc&#x2F;user.keytab&quot;</span><br><span class="line">principal&#x3D;&quot;hiveuser@122451B2_394D_494B_9FCA_B045F596D6D4.COM&quot;</span><br><span class="line">storeKey&#x3D;true</span><br><span class="line">useTicketCache&#x3D;false;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">Client &#123;</span><br><span class="line">com.sun.security.auth.module.Krb5LoginModule required</span><br><span class="line">storeKey&#x3D;true</span><br><span class="line">principal&#x3D;&quot;hiveuser@122451B2_394D_494B_9FCA_B045F596D6D4.COM&quot;</span><br><span class="line">useTicketCache&#x3D;false</span><br><span class="line">keyTab&#x3D;&quot;&#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;1_7_Flume&#x2F;etc&#x2F;user.keytab&quot;</span><br><span class="line">debug&#x3D;true</span><br><span class="line">useKeyTab&#x3D;true;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">com.sun.security.jgss.initiate &#123;</span><br><span class="line">com.sun.security.auth.module.Krb5LoginModule required</span><br><span class="line">useKeyTab&#x3D;true</span><br><span class="line">useTicketCache&#x3D;false</span><br><span class="line">doNotPrompt&#x3D;true</span><br><span class="line">storeKey&#x3D;true</span><br><span class="line">principal&#x3D;&quot;hiveuser@122451B2_394D_494B_9FCA_B045F596D6D4.COM&quot;</span><br><span class="line">keyTab&#x3D;&quot;&#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;1_7_Flume&#x2F;etc&#x2F;user.keytab&quot;</span><br><span class="line">debug&#x3D;true;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>由于华为云自己修改过部分组件，所以不太确定为什么凭据一直失败，所以我加了 <code>-Djavax.security.auth.useSubjectCredsOnly=false</code> 参数，这是让我们从底层去拿凭据，而默认拿的<code>section</code>部分就是 <code>com.sun.security.jgss.initiate</code><a href="https://github.com/frohoff/jdk8u-dev-jdk/blob/master/src/share/classes/sun/security/jgss/LoginConfigImpl.java#L92" target="_blank" rel="noopener">详见源码</a>,所以加上了 <code>com.sun.security.jgss.initiate</code> 之后，flume就可以连接上了kudu.</p><p>以下是我调试的时候遇到的问题。</p><ul><li><p>GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos Ticket) Cause: This may occur if no valid Kerberos credentials are obtained. In particular, this occurs if you want the underlying mechanism to obtain credentials but you forgot to indicate this by setting the javax.security.auth.useSubjectCredsOnly system property value to false (for example via -Djavax.security.auth.useSubjectCredsOnly=false in your execution command).</p></li><li><p>GSSException: No valid credentials provided (Mechanism level: Attempt to obtain new INITIATE credentials failed! (null)) . . . Caused by: javax.security.auth.login.LoginException: Clock skew too great Cause: Kerberos requires the time on the KDC and on the client to be loosely synchronized. (The default is within 5 minutes.) If that’s not the case, you will get this error.</p></li></ul><p>附上 <a href="https://docs.oracle.com/javase/7/docs/technotes/guides/security/jgss/tutorials/Troubleshooting.html" target="_blank" rel="noopener">jgss 的异常尝试解决方案官方文档</a>, PS：反正我根据文档没解决，不知道是不是华运自己修改的部分有什么潜规则。所以我才采用默认的jass配置。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">2021-01-06 11:31:26,235 | INFO  | [lifecycleSupervisor-1-0] |  Configuration provider starting  | org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start(PollingPropertiesFileConfigurationProvider.java:61)</span><br><span class="line">2021-01-06 11:31:26,239 | INFO  | [conf-file-poller-0] |  Reloading configuration file:&#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;1_7_Flume&#x2F;etc&#x2F;properties.properties  | org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherR</span><br><span class="line">unnable.run(PollingPropertiesFileConfigurationProvider.java:133)</span><br><span class="line">2021-01-06 11:31:26,236 | INFO  | [main] |  starting taskCounter  | org.apache.flume.tools.FlumeMetricsMgr.start(FlumeMetricsMgr.java:230)</span><br><span class="line">2021-01-06 11:31:26,250 | INFO  | [conf-file-poller-0] |  Processing:sink_kudu_table  | org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty(FlumeConfiguration.java:1017)</span><br><span class="line">2021-01-06 11:31:26,250 | INFO  | [conf-file-poller-0] |  Processing:sink_kudu_table  | org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty(FlumeConfiguration.java:1017)</span><br><span class="line">2021-01-06 11:31:26,250 | INFO  | [conf-file-poller-0] |  Processing:sink_kudu_table  | org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty(FlumeConfiguration.java:1017)</span><br><span class="line">2021-01-06 11:31:26,251 | INFO  | [conf-file-poller-0] |  Processing:sink_kudu_table  | org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty(FlumeConfiguration.java:1017)</span><br><span class="line">2021-01-06 11:31:26,251 | INFO  | [conf-file-poller-0] |  Added sinks: sink_kudu_table Agent: server  | org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty(FlumeConfiguration.java:931)</span><br><span class="line">2021-01-06 11:31:26,262 | INFO  | [conf-file-poller-0] |  Post-validation flume configuration contains configuration for agents: [server]  | org.apache.flume.conf.FlumeConfiguration.validateConfiguration(FlumeConfiguration.java:141)</span><br><span class="line">2021-01-06 11:31:26,263 | INFO  | [conf-file-poller-0] |  Creating channels  | org.apache.flume.node.AbstractConfigurationProvider.loadChannels(AbstractConfigurationProvider.java:155)</span><br><span class="line">2021-01-06 11:31:26,270 | INFO  | [conf-file-poller-0] |  Creating instance of channel ch_kudu_table type memory  | org.apache.flume.channel.DefaultChannelFactory.create(DefaultChannelFactory.java:42)</span><br><span class="line">2021-01-06 11:31:26,274 | INFO  | [conf-file-poller-0] |  Created channel ch_kudu_table  | org.apache.flume.node.AbstractConfigurationProvider.loadChannels(AbstractConfigurationProvider.java:210)</span><br><span class="line">2021-01-06 11:31:26,274 | INFO  | [conf-file-poller-0] |  Creating instance of source src_http_41600, type http  | org.apache.flume.source.DefaultSourceFactory.create(DefaultSourceFactory.java:41)</span><br><span class="line">2021-01-06 11:31:26,289 | INFO  | [main] |  Monitored counter group for type: OTHER, name: taskcount: Successfully registered new MBean.  | org.apache.flume.instrumentation.MonitoredCounterGroup.register(MonitoredCounterGroup.java:132)</span><br><span class="line">2021-01-06 11:31:26,290 | INFO  | [main] |  Component type: OTHER, name: taskcount started  | org.apache.flume.instrumentation.MonitoredCounterGroup.start(MonitoredCounterGroup.java:105)</span><br><span class="line">2021-01-06 11:31:26,323 | INFO  | [conf-file-poller-0] |  Creating instance of sink: sink_kudu_table, type: org.apache.kudu.flume.sink.KuduSink  | org.apache.flume.sink.DefaultSinkFactory.create(DefaultSinkFactory.java:42)</span><br><span class="line">2021-01-06 11:31:26,328 | WARN  | [conf-file-poller-0] |  No Kudu operations producer provided, using default  | org.apache.kudu.flume.sink.KuduSink.configure(KuduSink.java:202)</span><br><span class="line">2021-01-06 11:31:26,330 | INFO  | [conf-file-poller-0] |  Channel ch_kudu_table connected to [src_http_41600, sink_kudu_table]  | org.apache.flume.node.AbstractConfigurationProvider.getConfiguration(AbstractConfigurationProvider.java:124)</span><br><span class="line">2021-01-06 11:31:26,413 | INFO  | [main] |  ServiceServer started (at port[21151])  | org.wcc.framework.business.service.server.ServiceServer.start(ServiceServer.java:260)</span><br><span class="line">2021-01-06 11:31:26,413 | INFO  | [main] |  flume meric server startred ip:192.168.0.222,port:21151.  | org.apache.flume.tools.FlumeMetricsMgr.initMetricsRpcServer(FlumeMetricsMgr.java:84)</span><br><span class="line">2021-01-06 11:31:26,413 | INFO  | [main] |  current role is server  | org.apache.flume.tools.FlumeMetricsMgr.start(FlumeMetricsMgr.java:237)</span><br><span class="line">2021-01-06 11:31:26,429 | INFO  | [main] |  Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog  | org.mortbay.log.Slf4jLog.info(Slf4jLog.java:67)</span><br><span class="line">2021-01-06 11:31:26,430 | INFO  | [main] |  jetty-6.1.26  | org.mortbay.log.Slf4jLog.info(Slf4jLog.java:67)</span><br><span class="line">2021-01-06 11:31:26,441 | INFO  | [main] |  Started SelectChannelConnector@localhost:21150  | org.mortbay.log.Slf4jLog.info(Slf4jLog.java:67)</span><br><span class="line">2021-01-06 11:31:26,442 | INFO  | [main] |  starting compment mon  | org.apache.flume.node.Application.startCompMon(Application.java:543)</span><br><span class="line">2021-01-06 11:31:26,443 | INFO  | [main] |  started compment mon success  | org.apache.flume.node.Application.startCompMon(Application.java:582)</span><br><span class="line">2021-01-06 11:31:26,443 | INFO  | [main] |  log4j dynamic load is start.  | org.apache.flume.tools.LogDynamicLoad.start(LogDynamicLoad.java:59)</span><br><span class="line">2021-01-06 11:31:26,444 | INFO  | [conf-file-poller-0] |  stopping compment mon  | org.apache.flume.node.Application.stopCompMon(Application.java:587)</span><br><span class="line">2021-01-06 11:31:26,444 | INFO  | [conf-file-poller-0] |  stopped compment mon success  | org.apache.flume.node.Application.stopCompMon(Application.java:608)</span><br><span class="line">2021-01-06 11:31:26,444 | INFO  | [conf-file-poller-0] |  Starting new configuration:&#123; sourceRunners:&#123;src_http_41600&#x3D;EventDrivenSourceRunner: &#123; source:org.apache.flume.source.http.HTTPSource&#123;name:src_http_41600,state:IDLE&#125; &#125;&#125; sinkRunners:&#123;sink_kudu_table&#x3D;SinkRunner: &#123; policy:org.apache.flume.sink.DefaultSinkProcessor@1e1f84ee counterGroup:&#123; name:null counters:&#123;&#125; &#125; &#125;&#125; channels:&#123;ch_kudu_table&#x3D;org.apache.flume.channel.MemoryChannel&#123;name: ch_kudu_table&#125;&#125; &#125;  | org.apache.flume.node.Application.startAllComponents(Application.java:206)</span><br><span class="line">2021-01-06 11:31:26,498 | INFO  | [conf-file-poller-0] |  current role is server  | org.apache.flume.tools.FlumeSendAlarmMgr.start(FlumeSendAlarmMgr.java:173)</span><br><span class="line">2021-01-06 11:31:26,505 | INFO  | [conf-file-poller-0] |  Starting Channel ch_kudu_table  | org.apache.flume.node.Application.startAllComponents(Application.java:217)</span><br><span class="line">2021-01-06 11:31:26,508 | INFO  | [lifecycleSupervisor-1-0] |  Monitored counter group for type: CHANNEL, name: ch_kudu_table: Successfully registered new MBean.  | org.apache.flume.instrumentation.MonitoredCounterGroup.register(MonitoredCounterGroup.java:132)</span><br><span class="line">2021-01-06 11:31:26,508 | INFO  | [lifecycleSupervisor-1-0] |  Component type: CHANNEL, name: ch_kudu_table started  | org.apache.flume.instrumentation.MonitoredCounterGroup.start(MonitoredCounterGroup.java:105)</span><br><span class="line">2021-01-06 11:31:26,508 | INFO  | [conf-file-poller-0] |  Starting Sink sink_kudu_table  | org.apache.flume.node.Application.startAllComponents(Application.java:245)</span><br><span class="line">2021-01-06 11:31:26,508 | INFO  | [conf-file-poller-0] |  Starting Source src_http_41600  | org.apache.flume.node.Application.startAllComponents(Application.java:256)</span><br><span class="line">2021-01-06 11:31:26,510 | INFO  | [conf-file-poller-0] |  Begin start init plugins.  | com.huawei.flume.PluginManager.PluginManager.&lt;init&gt;(PluginManager.java:39)</span><br><span class="line">2021-01-06 11:31:26,510 | INFO  | [conf-file-poller-0] |  Set plugins configuration file dir successful.  | com.huawei.flume.PluginManager.PluginManager.&lt;init&gt;(PluginManager.java:46)</span><br><span class="line">2021-01-06 11:31:26,511 | INFO  | [conf-file-poller-0] |  Reading monitor server configuration from: &#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;1_7_Flume&#x2F;etc&#x2F;flume-check.properties  | com.huawei.flume.configuration.AbstractPluginsConfiguration.loadConfig(AbstractPluginsConfiguration.java:75)</span><br><span class="line">2021-01-06 11:31:26,517 | WARN  | [conf-file-poller-0] |  Needn&#39;t to create PluginsManager, plugins is empty.  | com.huawei.flume.PluginManager.PluginManager.&lt;init&gt;(PluginManager.java:55)</span><br><span class="line">2021-01-06 11:31:26,517 | WARN  | [conf-file-poller-0] |  Have not set any plugins  | com.huawei.flume.PluginManager.PluginManager.start(PluginManager.java:98)</span><br><span class="line">2021-01-06 11:31:26,517 | INFO  | [conf-file-poller-0] |  starting compment mon  | org.apache.flume.node.Application.startCompMon(Application.java:543)</span><br><span class="line">2021-01-06 11:31:26,517 | INFO  | [conf-file-poller-0] |  started compment mon success  | org.apache.flume.node.Application.startCompMon(Application.java:582)</span><br><span class="line">2021-01-06 11:31:26,542 | INFO  | [lifecycleSupervisor-1-0] |  jetty-6.1.26  | org.mortbay.log.Slf4jLog.info(Slf4jLog.java:67)</span><br><span class="line">2021-01-06 11:31:26,577 | INFO  | [lifecycleSupervisor-1-0] |  Started SelectChannelConnector@0.0.0.0:41600  | org.mortbay.log.Slf4jLog.info(Slf4jLog.java:67)</span><br><span class="line">2021-01-06 11:31:26,578 | INFO  | [lifecycleSupervisor-1-0] |  Monitored counter group for type: SOURCE, name: src_http_41600: Successfully registered new MBean.  | org.apache.flume.instrumentation.MonitoredCounterGroup.register(MonitoredCounterGroup.java:132)</span><br><span class="line">2021-01-06 11:31:26,578 | INFO  | [lifecycleSupervisor-1-0] |  Component type: SOURCE, name: src_http_41600 started  | org.apache.flume.instrumentation.MonitoredCounterGroup.start(MonitoredCounterGroup.java:105)</span><br><span class="line">2021-01-06 11:31:27,187 | INFO  | [lifecycleSupervisor-1-3] |  Monitored counter group for type: SINK, name: sink_kudu_table: Successfully registered new MBean.  | org.apache.flume.instrumentation.MonitoredCounterGroup.register(MonitoredCounterGroup.java:132)</span><br></pre></td></tr></table></figure><p>可以看到，sink启动成功。</p><p>记录几个需要用到的命令：</p><h3 id="同步kudu-flume-sink到各flume节点"><a href="#同步kudu-flume-sink到各flume节点" class="headerlink" title="同步kudu-flume-sink到各flume节点"></a>同步kudu-flume-sink到各flume节点</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">str_nodes&#x3D;&quot;node-str-corelgoh node-str-corenydJ node-str-coreoVpr&quot;;for ip in $(echo $&#123;str_nodes&#125;);do scp &#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;install&#x2F;FusionInsight-Flume-1.6.0&#x2F;flume&#x2F;lib&#x2F;kudu-flume-sink-1.4.0.jar $&#123;ip&#125;:&#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;install&#x2F;FusionInsight-Flume-1.6.0&#x2F;flume&#x2F;lib&#x2F;;ssh $&#123;ip&#125; &#39;chmod 751 &#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;install&#x2F;FusionInsight-Flume-1.6.0&#x2F;flume&#x2F;lib&#x2F;kudu-flume-sink-1.4.0.jar &amp;&amp; chown omm:ficommon &#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;install&#x2F;FusionInsight-Flume-1.6.0&#x2F;flume&#x2F;lib&#x2F;kudu-flume-sink-1.4.0.jar&#39;;done</span><br></pre></td></tr></table></figure><h3 id="修改对应的jass配置-和-凭据信息同步"><a href="#修改对应的jass配置-和-凭据信息同步" class="headerlink" title="修改对应的jass配置 和 凭据信息同步"></a>修改对应的jass配置 和 凭据信息同步</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">str_nodes&#x3D;&quot;node-str-corelgoh node-str-corenydJ node-str-coreoVpr&quot;;for ip in $(echo $&#123;str_nodes&#125;);do scp &#x2F;tmp&#x2F;krb5.conf $&#123;ip&#125;:&#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;1_7_Flume&#x2F;etc&#x2F;; scp &#x2F;tmp&#x2F;user.keytab $&#123;ip&#125;:&#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;1_7_Flume&#x2F;etc&#x2F;;ssh $&#123;ip&#125; &quot;sed -i -e &#39;&#x2F;principal&#x2F;s&#x2F;flume&#x2F;hiveuser&#x2F;g&#39; -e &#39;&#x2F;keyTab&#x2F;s&#x2F;\&#x2F;opt\&#x2F;Bigdata\&#x2F;MRS_2.1.0\&#x2F;install\&#x2F;FusionInsight-Flume-1.6.0\&#x2F;flume\&#x2F;conf\&#x2F;flume.keytab&#x2F;\&#x2F;opt\&#x2F;Bigdata\&#x2F;MRS_2.1.0\&#x2F;1_7_Flume\&#x2F;etc\&#x2F;user.keytab&#x2F;g&#39;  -e &#39;&#x2F;^Client&#x2F;s&#x2F;Client&#x2F;com.sun.security.jgss.initiate&#x2F;g&#39; &#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;1_7_Flume&#x2F;etc&#x2F;jaas.conf;chown omm:wheel &#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;1_7_Flume&#x2F;etc&#x2F;jaas.conf;chown -R omm:wheel &#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;1_7_Flume&#x2F;etc&#x2F;&quot;;done</span><br></pre></td></tr></table></figure><h2 id="这个不是命令，但是是记得最新的GC-OPTS参数"><a href="#这个不是命令，但是是记得最新的GC-OPTS参数" class="headerlink" title="这个不是命令，但是是记得最新的GC_OPTS参数"></a>这个不是命令，但是是记得最新的GC_OPTS参数</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-Xms2G -Xmx4G -XX:CMSFullGCsBeforeCompaction&#x3D;1 -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:+UseCMSCompactAtFullCollection -Djava.security.krb5.conf&#x3D;&#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;1_7_Flume&#x2F;etc&#x2F;krb5.conf -Djava.security.auth.login.config&#x3D;&#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;1_7_Flume&#x2F;etc&#x2F;jaas.conf -Dzookeeper.request.timeout&#x3D;120000</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;由于我们在调研是否让自建IDC大数据机房上云服务，所以华为云进行一下测试。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://blog.crazylaw.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="大数据" scheme="http://blog.crazylaw.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="flume" scheme="http://blog.crazylaw.cn/tags/flume/"/>
    
      <category term="kudu" scheme="http://blog.crazylaw.cn/tags/kudu/"/>
    
  </entry>
  
  <entry>
    <title>【数据库开发知识】Rocksdb</title>
    <link href="http://blog.crazylaw.cn/2020/12/29/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%BC%80%E5%8F%91%E7%9F%A5%E8%AF%86/Rocksdb/"/>
    <id>http://blog.crazylaw.cn/2020/12/29/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%BC%80%E5%8F%91%E7%9F%A5%E8%AF%86/Rocksdb/</id>
    <published>2020-12-29T03:16:43.000Z</published>
    <updated>2021-03-20T16:25:01.817Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前沿"><a href="#前沿" class="headerlink" title="前沿"></a>前沿</h1><p>近日工作中使用了 RocksDB。RocksDB 的优点此处无需多说，它的一个 feature 是其有很多优化选项用于对 RocksDB 进行调优。欲熟悉这些参数，必须对其背后的原理有所了解，本文主要整理一些 RocksDB 的 wiki 文档，以备自己参考之用。</p><a id="more"></a><h3 id="1-Basic-Operations"><a href="#1-Basic-Operations" class="headerlink" title="1 Basic Operations"></a>1 <a href="https://github.com/facebook/rocksdb/wiki/Basic-Operations" target="_blank" rel="noopener">Basic Operations</a></h3><hr><p>先介绍一些 RocksDB 的基本操作和基本架构。</p><h4 id="1-1-LSM-与-WriteBatch"><a href="#1-1-LSM-与-WriteBatch" class="headerlink" title="1.1 LSM 与 WriteBatch"></a>1.1 LSM 与 WriteBatch</h4><hr><p><a href="https://github.com/facebook/rocksdb/wiki/RocksDB-Basics" target="_blank" rel="noopener">参考文档5</a>提到RocksDB 是一个快速存储系统，它会充分挖掘 Flash or RAM 硬件的读写特性，支持单个 KV 的读写以及批量读写。RocksDB 自身采用的一些数据结构如 LSM/SKIPLIST 等结构使得其有读放大、写放大和空间使用放大的问题。</p><p><img src="/images/%E6%95%B0%E6%8D%AE%E5%BA%93/log_structured_merge_tree.png" alt=""></p><p>LSM 大致结构如上图所示。LSM 树而且通过批量存储技术规避磁盘随机写入问题。 LSM 树的设计思想非常朴素, 它的原理是把一颗大树拆分成N棵小树， 它首先写入到内存中（内存没有寻道速度的问题，随机写的性能得到大幅提升），在内存中构建一颗有序小树，随着小树越来越大，内存的小树会flush到磁盘上。磁盘中的树定期可以做 merge 操作，合并成一棵大树，以优化读性能【读数据的过程可能需要从内存 memtable 到磁盘 sstfile 读取多次，称之为读放大】。RocksDB 的 LSM 体现在多 level 文件格式上，最热最新的数据尽在 L0 层，数据在内存中，最冷最老的数据尽在 LN 层，数据在磁盘或者固态盘上。RocksDB 还有一种日志文件叫做 manifest，用于记录对 sstfile 的更改，可以认为是 RocksDB 的 GIF，后面将会详述。</p><p>LSM-Tree(Log-Structured-Merge-Tree)<br>LSM从命名上看，容易望文生义成一个具体的数据结构，一个tree。但LSM并不是一个具体的数据结构，也不是一个tree。LSM是一个数据结构的概念，是一个数据结构的设计思想。实际上，要是给LSM的命名断句，Log和Structured这两个词是合并在一起的，LSM-Tree应该断句成Log-Structured、Merge、Tree三个词汇，这三个词汇分别对应以下三点LSM的关键性质：</p><ul><li>将数据形成Log-Structured：在将数据写入LSM内存结构之前，先记录log。这样LSM就可以将有易失性的内存看做永久性存储器。并且信任内存上的数据，等到内存容量达到threshold再集体写入磁盘。将数据形成Log-Structured，也是将整体存储结构转换成了“内存(in-memory)”存储结构。</li><li>将所有磁盘上数据不组织成一个整体索引结构，而组织成有序的文件集：因为磁盘随机读写比顺序读写慢3个数量级，LSM尽量将磁盘读写转换成顺序读写。将磁盘上的数据组织成B树这样的一个整体索引结构，虽然查找很高效，但是面对随机读写，由于大量寻道导致其性能不佳。而LSM用了一种很有趣的方法，将所有数据不组织成一个整体索引结构，而组织成有序的文件集。每次LSM面对磁盘写，将数据写入一个或几个新生成的文件，顺序写入且不能修改其他文件，这样就将随机读写转换成了顺序读写。LSM将一次性集体写入的文件作为一个level，磁盘上划分多level，level与level之间互相隔离。这就形成了，以写入数据时间线形成的逻辑上、而非物理上的层级结构，这也就是为什么LSM被命名为”tree“，但不是“tree”。</li><li>将数据按key排序，在合并不同file、level上的数据时类似merge-join：如果一直保持生成新的文件，不仅写入会造成冗余空间，而且也会大量降低读的性能。所以要高效的、周期性合并不同file、level。而如果数据是乱序的，根本做不到高效合并。所以LSM要将数据按key排序，在合并不同file、level上的数据时类似 merge-join。</li></ul><p>很明显，LSM牺牲了一部分读的性能和增加了合并的开销，换取了高效的写性能。那LSM为什么要这么做？实际上，这就关系到对于磁盘写已经没有什么优化手段了，而对于磁盘读，不论硬件还是软件上都有优化的空间。通过多种优化后，读性能虽然仍是下降，但可以控制在可接受范围内。实际上，用于磁盘上的数据结构不同于用于内存上的数据结构，用于内存上的数据结构性能的瓶颈就在搜索复杂度，而用于磁盘上的数据结构性能的瓶颈在磁盘IO，甚至是磁盘IO的模式。</p><p>以上三段摘抄自<a href="https://www.tuicool.com/articles/7ju2UfI" target="_blank" rel="noopener">参考文档20</a>。个人以为，除了将随机写合并之后转化为顺写之外，LSM 的另外一个关键特性就在于其是一种自带数据 Garbage Collect 的有序数据集合，对外只提供了 Add/Get 接口，其内部的 Compaction 就是其 GC 的关键，通过 Compaction 实现了对数据的删除、附带了 TTL 的过期数据地淘汰、同一个 Key 的多个版本 Value 地合并。RocksDB 基于 LSM 对外提供了 Add/Delete/Get 三个接口，用户则基于 RocksDB 提供的 transaction 还可以实现 Update 语义。</p><p><img src="/images/%E6%95%B0%E6%8D%AE%E5%BA%93/rocksdb_arch.png" alt=""></p><p>RocksDB的三种基本文件格式是 memtable/sstfile/logfile，memtable 是一种内存文件数据系统，新写数据会被写进 memtable，部分请求内容会被写进 logfile。logfile 是一种有利于顺序写的文件系统。memtable 的内存空间被填满之后，会有一部分老数据被转移到 sstfile 里面，这些数据对应的 logfile 里的 log 就会被安全删除。sstfile 中的内容是有序的。</p><p>上图所示，所有 Column Family 共享一个 WAL 文件，但是每个 Column Family 有自己单独的 memtable &amp; ssttable(sstfile)，即 log 共享而数据分离。</p><p>一个进程对一个 DB 同时只能创建一个 rocksdb::DB 对象，所有线程共享之。这个对象内部有锁机制保证访问安全，多个线程同时进行 Get/Put/Fetch Iteration 都没有问题，但是如果直接 Iteration 或者 WriteBatch 则需要额外的锁同步机制保护 Iterator 或者 WriteBatch 对象。</p><p>基于 RocksDB 设计存储系统，要考虑到应用场景进行各种 tradeoff 设置相关参数。譬如，如果 RocksDB 进行 compaction 比较频繁，虽然有利于空间和读，但是会造成读放大；compaction 过低则会造成读放大和空间放大；增大每个 level 的 comparession 难度可以减小空间放大，但是会增加 cpu 负担，是运算时间增加换取使用空间减小；增大 SSTfile 的 data block size，则是增大内存使用量来加快读取数据的速度，减小读放大。</p><p>单独的 Get/Put/Delete 是原子操作，要么成功要么失败，不存在中间状态。</p><p>如果需要进行批量的 Get/Put/Delete 操作且需要操作保持原子属性，则可以使用 WriteBatch。</p><p>WriteBatch 还有一个好处是保持加快吞吐率。</p><h4 id="1-2-同步写-与-异步写"><a href="#1-2-同步写-与-异步写" class="headerlink" title="1.2 同步写 与 异步写"></a>1.2 同步写 与 异步写</h4><hr><p>默认情况下，RocksDB 的写是异步的：仅仅把数据写进了操作系统的缓存区就返回了，而这些数据被写进磁盘是一个异步的过程。如果为了数据安全，可以用如下代码把写过程改为同步写：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rocksdb::WriteOptions write_options;   </span><br><span class="line">write_options.sync = <span class="literal">true</span>;   </span><br><span class="line">db-&gt;Put(write_options, …);</span><br></pre></td></tr></table></figure><p>这个选项会启用 Posix 系统的 <code>fsync(...) or fdatasync(...) or msync(..., MS_SYNC)</code> 等函数。</p><p><font color=red> 异步写的吞吐率是同步写的一千多倍。异步写的缺点是机器或者操作系统崩溃时可能丢掉最近一批写请求发出的由操作系统缓存的数据，但是 RocksDB 自身崩溃并不会导致数据丢失。而机器或者操作系统崩溃的概率比较低，所以大部分情况下可以认为异步写是安全的。</font></p><p>RocksDB 由于有 WAL 机制保证，所以即使崩溃，其重启后会进行写重放保证数据一致性。如果不在乎数据安全性，可以把 <code>write_option.disableWAL</code> 设置为 true，加快写吞吐率。</p><p>RocksDB 调用 Posix API <code>fdatasync()</code> 对数据进行异步写。如果想用 <code>fsync()</code> 进行同步写，可以设置 <code>Options::use_fsync</code> 为 true。</p><h4 id="1-3-Snapshots"><a href="#1-3-Snapshots" class="headerlink" title="1.3 Snapshots"></a>1.3 Snapshots</h4><hr><p>RocksDB 能够保存某个版本的所有数据（可称之为一个 Snapshot）以方便读取操作，创建并读取 Snapshot 方法如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">rocksdb::ReadOptions options;   </span><br><span class="line">options.snapshot = db-&gt;GetSnapshot();   </span><br><span class="line">… apply some updates to db ….  </span><br><span class="line">rocksdb::Iterator* iter = db-&gt;NewIterator(options);   </span><br><span class="line">… <span class="built_in">read</span> <span class="keyword">using</span> iter to view the state when the snapshot was created ….  </span><br><span class="line"><span class="keyword">delete</span> iter;   </span><br><span class="line">db-&gt;ReleaseSnapshot(options.snapshot);</span><br></pre></td></tr></table></figure><p>如果 ReadOptions::snapshot 为 null，则读取的 snapshot 为 RocksDB 当前版本数据的 snapshot。</p><h4 id="1-4-Slice-amp-PinnableSlice"><a href="#1-4-Slice-amp-PinnableSlice" class="headerlink" title="1.4 Slice &amp; PinnableSlice"></a>1.4 Slice &amp; PinnableSlice</h4><hr><p>不管是 <code>it-&gt;key()</code> 还是 <code>it-&gt;value()</code>，其值类型都是 <code>rocksdb::Slice</code>。 Slice 自身由一个长度字段[ size_t size_ ]以及一个指向外部一个内存区域的指针[ const char* data_ ]构成，返回 Slice 比返回一个 string 廉价，并不存在内存拷贝的问题。RocksDB 自身会给 key 和 value 添加一个 C-style 的 ‘\0’，所以 slice 的指针指向的内存区域自身作为字符串输出没有问题。</p><p>Slice 与 string 之间的转换代码如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">rocksdb::Slice s1 = “hello”;</span><br><span class="line"><span class="function"><span class="built_in">std</span>::<span class="built_in">string</span> <span class="title">str</span><span class="params">(“world”)</span></span>;</span><br><span class="line">rocksdb::Slice s2 = str;</span><br><span class="line"></span><br><span class="line">OR:   </span><br><span class="line"></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">string</span> str = s1.ToString();   </span><br><span class="line">assert(str == <span class="built_in">std</span>::<span class="built_in">string</span>(“hello”));</span><br></pre></td></tr></table></figure><p>但是请注意 Slice 的安全性，有代码如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">rocksdb::Slice slice;   </span><br><span class="line"><span class="keyword">if</span> (…) &#123;  </span><br><span class="line"> <span class="built_in">std</span>::<span class="built_in">string</span> str = …;   </span><br><span class="line"> slice = str;   </span><br><span class="line">&#125;  </span><br><span class="line">Use(slice);</span><br></pre></td></tr></table></figure><p>当退出 if 语句块后，slice 内部指针指向的内存区域已经不存在，此时再使用导致程序出问题。</p><p>Slice 自身虽然能够减少内存拷贝，但是在离开相应的 scope 之后，其值就会被释放，rocksdb v5.4.5 版本引入一个 PinnableSlice，其继承自 Slice，可替换之前 Get 接口的出参：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Status <span class="title">Get</span><span class="params">(<span class="keyword">const</span> ReadOptions&amp; options,</span></span></span><br><span class="line"><span class="function"><span class="params">                     ColumnFamilyHandle* column_family, <span class="keyword">const</span> Slice&amp; key,</span></span></span><br><span class="line"><span class="function"><span class="params">                     <span class="built_in">std</span>::<span class="built_in">string</span>* value)</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"><span class="keyword">virtual</span> Status <span class="title">Get</span><span class="params">(<span class="keyword">const</span> ReadOptions&amp; options,</span></span></span><br><span class="line"><span class="function"><span class="params">                     ColumnFamilyHandle* column_family, <span class="keyword">const</span> Slice&amp; key,</span></span></span><br><span class="line"><span class="function"><span class="params">                     PinnableSlice* value)</span></span></span><br></pre></td></tr></table></figure><p>这里的 PinnableSlice 如同 Slice 一样可以减少内存拷贝，提高读性能，但是 PinnableSlice 内部有一个引用计数功能，可以实现数据内存的延迟释放，延长相关数据的生命周期，相关详细分析详见 <a href="https://zhuanlan.zhihu.com/p/30807728" target="_blank" rel="noopener">参考文档15</a>。</p><p><a href="https://rocksdb.org/blog/2017/08/24/pinnableslice.html" target="_blank" rel="noopener">参考文档16</a> 提到 <code>PinnableSlice, as its name suggests, has the data pinned in memory. The pinned data are released when PinnableSlice object is destructed or when ::Reset is invoked explicitly on it.</code>。所谓的 <strong>pinned in memory</strong> 即为引用计数之意，文中提到内存数据释放是在 PinnableSlice 析构或者调用 ::Reset 之后。</p><p>用户也可以把一个 std::string 对象作为 PinnableSlice 构造函数的参数， 把这个 std::string 指定为 PinnableSlice 的初始内部 buffer [ rocksdb/slice.h:PinnableSlice::buf_ ]，使用方法可以参考 <a href="https://github.com/facebook/rocksdb/blob/9e583711144f580390ce21a49a8ceacca338fcd5/include/rocksdb/db.h#L314" target="_blank" rel="noopener">用新 Get 实现的旧版本的 Get</a>：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">inline</span> Status <span class="title">Get</span><span class="params">(<span class="keyword">const</span> ReadOptions&amp; options,</span></span></span><br><span class="line"><span class="function"><span class="params">                           ColumnFamilyHandle* column_family, <span class="keyword">const</span> Slice&amp; key,</span></span></span><br><span class="line"><span class="function"><span class="params">                           <span class="built_in">std</span>::<span class="built_in">string</span>* value)</span> </span>&#123;</span><br><span class="line">   assert(value != <span class="literal">nullptr</span>);</span><br><span class="line">   <span class="function">PinnableSlice <span class="title">pinnable_val</span><span class="params">(value)</span></span>;</span><br><span class="line">   assert(!pinnable_val.IsPinned());</span><br><span class="line">   <span class="keyword">auto</span> s = Get(options, column_family, key, &amp;pinnable_val);</span><br><span class="line">   <span class="keyword">if</span> (s.ok() &amp;&amp; pinnable_val.IsPinned()) &#123;</span><br><span class="line">     value-&gt;assign(pinnable_val.data(), pinnable_val.<span class="built_in">size</span>());</span><br><span class="line">   &#125;  <span class="comment">// else value is already assigned</span></span><br><span class="line">   <span class="keyword">return</span> s;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>通过 rocksdb/slice.h:PinnableSlice::PinSlice 实现代码可以看出，只有在这个函数里 PinnableSlice::pinned_ 被赋值为 true， 同时内存区域存放在 PinnableSlice::data_ 指向的内存区域，故而 PinnableSlice::IsPinned 为 true，则 内部 buffer [ rocksdb/slice.h:PinnableSlice::buf_ ] 必定为空。</p><p>具体的编程用例可参考 <a href="https://github.com/alexstocks/c-practice/blob/master/db/rocksdb/src/pinnalble_slice.cc" target="_blank" rel="noopener">pinnalble_slice.cc</a>。</p><h4 id="1-5-Transactions"><a href="#1-5-Transactions" class="headerlink" title="1.5 Transactions"></a>1.5 Transactions</h4><hr><p>当使用 TransactionDB 或者 OptimisticTransactionDB 的时候，可以使用 RocksDB 的 BEGIN/COMMIT/ROLLBACK 等事务 API。RocksDB 支持活锁或者死等两种事务。</p><p>WriteBatch 默认使用了事务，确保批量写成功。</p><p>当打开一个 TransactionDB 的时候，如果 RocksDB 检测到某个 key 已经被别的事务锁住，则 RocksDB 会返回一个 error。如果打开成功，则所有相关 key 都会被 lock 住，直到事务结束。TransactionDB 的并发特性表现要比 OptimisticTransactionDB 好，但是 TransactionDB 的一个小问题就是不管写发生在事务里或者事务外，他都会进行写冲突检测。TransactionDB 使用示例代码如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">TransactionDB* txn_db;</span><br><span class="line">Status s = TransactionDB::Open(options, path, &amp;txn_db);</span><br><span class="line">Transaction* txn = txn_db-&gt;BeginTransaction(write_options, txn_options);</span><br><span class="line">s = txn-&gt;Put(“key”, “value”);</span><br><span class="line">s = txn-&gt;Delete(“key2”);</span><br><span class="line">s = txn-&gt;Merge(“key3”, “value”);</span><br><span class="line">s = txn-&gt;Commit();</span><br><span class="line"><span class="keyword">delete</span> txn;</span><br></pre></td></tr></table></figure><p>OptimisticTransactionDB 提供了一个更轻量的事务实现，它在进行写之前不会进行写冲突检测，当对写操作进行 commit 的时候如果发生了 lock 冲突导致写操作失败，则 RocksDB 会返回一个 error。这种事务使用了活锁策略，适用于读多写少这种写冲突概率比较低的场景下，使用示例代码如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">DB* db;</span><br><span class="line">OptimisticTransactionDB* txn_db;</span><br><span class="line">Status s = OptimisticTransactionDB::Open(options, path, &amp;txn_db);</span><br><span class="line">db = txn_db-&gt;GetBaseDB();</span><br><span class="line"></span><br><span class="line">OptimisticTransaction* txn = txn_db-&gt;BeginTransaction(write_options, txn_options);</span><br><span class="line">txn-&gt;Put(“key”, “value”);</span><br><span class="line">txn-&gt;Delete(“key2”);</span><br><span class="line">txn-&gt;Merge(“key3”, “value”);</span><br><span class="line">s = txn-&gt;Commit();</span><br><span class="line"><span class="keyword">delete</span> txn;</span><br></pre></td></tr></table></figure><p>参考文档 6 详细描述了 <code>RocksDB</code> 的 Transactions。</p><h4 id="1-6-Column-Family"><a href="#1-6-Column-Family" class="headerlink" title="1.6 Column Family"></a>1.6 Column Family</h4><hr><p>CF 提供了对 DB 进行逻辑划分开来的方法，用户可以通过 CF 同时对多个 CF 的 KV 进行并行读写的方法，提高了并行度。</p><h4 id="1-7-MemTable-and-Table-factories"><a href="#1-7-MemTable-and-Table-factories" class="headerlink" title="1.7 MemTable and Table factories"></a>1.7 MemTable and Table factories</h4><hr><p>RocksDB 内存中的数据格式是 skiplist，磁盘则是以 table 形式存储的 SST 文件格式。</p><p>table 格式有两种：继承自 leveldb 的文件格式【详见参考文档2】和 PlainTable 格式【详见参考文档3】。PlainTable 格式是针对 低查询延迟 或者低延迟存储媒介如 SSD 特别别优化的一种文件格式。</p><h4 id="1-8-Block-size"><a href="#1-8-Block-size" class="headerlink" title="1.8 Block size"></a>1.8 Block size</h4><hr><p>RocksDB 把相邻的 key 放到同一个 block 中，block 是数据存储和传递的基本单元。默认 Block 的大小是 4096B，数据未经压缩。</p><p>经常进行 bulk scan 操作的用户可能希望增大 block size，而经常进行单 key 读写的用户则可能希望减小其值，官方建议这个值减小不要低于 1KB 的下限，变大也不要超过 <code>a few megabytes</code>。启用压缩也可以起到增大 block size 的好处。</p><p>修改 Block size 的方法是修改 <code>Options::block_size</code>。</p><h4 id="1-9-Writer-Buffer"><a href="#1-9-Writer-Buffer" class="headerlink" title="1.9 Writer Buffer"></a>1.9 Writer Buffer</h4><hr><p><code>Options::write_buffer_size</code> 指定了一个写内存 buffer 的大小，当这个 buffer 写满之后数据会被固化到磁盘上。这个值越大批量写入的性能越好。</p><p>RocksDB 控制写内存 buffer 数目的参数是 <code>Options::max_write_buffer_number</code>。这个值默认是 2，当一个 buffer 的数据被 flush 到磁盘上的时候，RocksDB 就用另一个 buffer 作为数据读写缓冲区。</p><p>‘Options::min_write_buffer_number_to_merge’ 设定了把写 buffer 的数据固化到磁盘上时对多少个 buffer 的数据进行合并然后再固化到磁盘上。这个值如果为 1，则 L0 层文件只有一个，这会导致读放大，这个值太小会导致数据固化到磁盘上之前数据去重效果太差劲。</p><p>这两个值并不是越大越好，太大会延迟一个 DB 被重新打开时的数据加载时间。</p><h4 id="1-10-Key-Layout"><a href="#1-10-Key-Layout" class="headerlink" title="1.10 Key Layout"></a>1.10 Key Layout</h4><hr><p>在 <strong>1.8</strong> 章节里提到 “block 是数据存储和传递的基本单元”，RocksDB 的数据是一个 range 的 key-value 构成一个 Region，根据局部性原理每次访问一个 Region 的 key 的时候，有很多概率会访问其相邻的 key，每个 Region 的 keys 放在一个 block 里，多个 Region 的 keys 放在多个 block 里。</p><p>下面以文件系统作为类比，详细解释下 RocksDB 的文件系统：<br>​<br>​    filename -&gt; permission-bits, length, list of file_block_ids<br>​    file_block_id -&gt; data</p><p>以多个维度组织 key 的时候，我们可能希望 filename 的前缀都是 ‘/‘， 而 file_block_id 的前缀都是 ‘0’，这样可以把他们分别放在不同的 block 里，以方便快速查询。</p><h4 id="1-11-Checksums"><a href="#1-11-Checksums" class="headerlink" title="1.11 Checksums"></a>1.11 Checksums</h4><hr><p>Rocksdb 对每个 kv 以及整体数据文件都分别计算了 checksum，以进行数据正确性校验。下面有两个选项对 checksum 的行为进行控制。</p><ul><li><code>ReadOptions::verify_checksums</code> 强制对每次从磁盘读取的数据进行校验，这个选项默认为 true。</li><li><code>Options::paranoid_checks</code> 这个选项为 true 的时候，如果 RocksDB 打开一个数据检测到内部数据部分错乱，马上抛出一个错误。这个选择默认为 false。</li></ul><p>如果 RocksDB 的数据错乱，RocksDB 会尽量把它隔离出来，保证大部分数据的可用性和正确性。</p><h4 id="1-12-Approximate-Sizes"><a href="#1-12-Approximate-Sizes" class="headerlink" title="1.12 Approximate Sizes"></a>1.12 Approximate Sizes</h4><hr><p><code>GetApproximateSizes</code> 方法可以返回一个 key range 的磁盘占用空间大致使用量，示例代码如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">rocksdb::Range ranges[<span class="number">2</span>];  </span><br><span class="line">ranges[<span class="number">0</span>] = rocksdb::Range(“a”, “c”);  </span><br><span class="line">ranges[<span class="number">1</span>] = rocksdb::Range(“x”, “z”);  </span><br><span class="line"><span class="keyword">uint64_t</span> sizes[<span class="number">2</span>];  </span><br><span class="line">rocksdb::Status s = db-&gt;GetApproximateSizes(ranges, <span class="number">2</span>, sizes);</span><br></pre></td></tr></table></figure><p>上面的 <code>sizes[0]</code> 返回 <code>[a..c)</code> key range 的磁盘使用量，而 <code>sizes[1]</code> 返回 <code>[x..z)</code>  key range 的磁盘使用量。</p><h4 id="1-13-Purging-WAL-files"><a href="#1-13-Purging-WAL-files" class="headerlink" title="1.13 Purging WAL files"></a>1.13 Purging WAL files</h4><hr><p>一般情况下，RocksDB 会删除一些过时的 WAL 文件，所谓过时就是 WAL 文件里面对应的一些 key 的数据已经被固化到磁盘了。但是 RocksDB 提供了两个选项以实让用户控制 WAL 何时删除：<code>Options::WAL_ttl_seconds</code> 和 <code>Options::WAL_size_limit_MB</code>，这两个参数分别控制 WAL 文件的超时时间 和 最大文件 size。</p><p>如果这两个值都被设置为 0，则 log 不会被固化到文件系统上。</p><p>如果 <code>Options::WAL_ttl_seconds</code> 为 0 而 <code>Options::WAL_size_limit_MB</code> 不为 0， RocksDB 会每 10 分钟检测所有的 WAL 文件，如果其总体 size 超过 <code>Options::WAL_size_limit_MB</code>，则 RocksDB 会删除最早的日志直到满足这个值位置。一切空文件都会被删除。</p><p>如果 <code>Options::WAL_ttl_seconds</code> 不为 0 而 <code>Options::WAL_size_limit_MB</code> 为 0，RocksDB 会每 <code>Options::WAL_ttl_seconds</code> / 2 检测一次 WAL 文件， 所有 TTL 超过 <code>Options::WAL_ttl_seconds</code> 的 WAL 文件都会被删除。</p><p>如果两个值都不为 0，RocksDB 会每 10 分钟检测所有的 WAL 文件，所有不满足条件的 WAL 文件都会被删除，其中 ttl 参数优先。</p><h4 id="1-14-Prefix-Iterators"><a href="#1-14-Prefix-Iterators" class="headerlink" title="1.14 Prefix Iterators"></a>1.14 Prefix Iterators</h4><hr><p>许多 LSM 引擎不支持高效的 RangeScan 操作，因为 Range 操作需要扫描所有的数据文件。一般情况下常规的技术手段是给 key 建立索引，只用遍历 key 就可以了。应用可以通过确认 <code>prefix_extractor</code> 指定一个可以的前缀，RocksDB 可以为这些 key prefix 建立 Bloom 索引，以加快查询速度。</p><h4 id="1-15-Multi-Threaded-Compactions"><a href="#1-15-Multi-Threaded-Compactions" class="headerlink" title="1.15 Multi-Threaded Compactions"></a>1.15 Multi-Threaded Compactions</h4><hr><p>参考文档 5 的 <code>Compaction Styles</code> 一节提到，如果启用 <code>Level Style Compaction</code>, L0 存储着 RocksDB 最新的数据，Lmax 存储着比较老的数据，<font color=blue><strong>L0 里可能存着重复 keys，但是其他层文件则不可能存在重复 key</strong></font>。每个 compaction 任务都会选择 Ln 层的一个文件以及与其相邻的 Ln+1 层的多个文件进行合并，删除过期 或者 标记为删除 或者 重复 的 key，然后把合并后的文件放入 Ln+1 层。Compaction 过程会导致写放大【如写qps是10MB/s，但是实际磁盘io是50MB/s】效应，但是可以节省空间并减少读放大。</p><p>如果启用 <code>Universal Style Compaction</code>，则只压缩 L0 的所有文件，合并后再放入 L0 层里。</p><p>RocksDB 的 compaction 任务线程不宜过多，过多容易导致写请求被 hang 住。</p><h4 id="1-16-Incremental-Backups-and-Replication"><a href="#1-16-Incremental-Backups-and-Replication" class="headerlink" title="1.16 Incremental Backups and Replication"></a>1.16 Incremental Backups and Replication</h4><hr><p>RocksDB 的 API <code>GetUpdatesSince</code> 可以让调用者从 transaction log 获知最近被更新的 key（原文意为用 tail 方式读取 transaction log），通过这个 API 可以进行数据的增量备份。</p><p>RocksDB 在进行数据备份时候，可以调用 API <code>DisableFileDeletions</code> 停止删除文件操作，调用 API <code>GetLiveFiles/GetSortedWalFiles</code> 以检索活跃文件列表，然后进行数据备份。备份工作完成以后在调用 API <code>EnableFileDeletions</code> 让 RocksDB 再启动过期文件淘汰工作。</p><h4 id="1-17-Thread-Pool"><a href="#1-17-Thread-Pool" class="headerlink" title="1.17 Thread Pool"></a>1.17 Thread Pool</h4><hr><p>RocksDB 会创建一个 thread pool 与 Env 对象进行关联，线程池中线程的数目可以通过 <code>Env::SetBackgroundThreads()</code> 设定。通过这个线程池可以执行 compaction 与 memtable flush 任务。</p><p>当 memtable flush 和 compaction 两个任务同时执行的时候，会导致写请求被 hang 住。RocksDB 建议创建两个线程池，分别指定  HIGH 和 LOW 两个优先级。默认情况下 HIGH 线程池执行 memtable flush 任务，LOW 线程池执行 compaction 任务。</p><p>相关代码示例如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> “rocksdb/env.h”</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> “rocksdb/db.h”</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">auto</span> env = rocksdb::Env::Default();</span><br><span class="line">env-&gt;SetBackgroundThreads(<span class="number">2</span>, rocksdb::Env::<span class="literal">LOW</span>);</span><br><span class="line">env-&gt;SetBackgroundThreads(<span class="number">1</span>, rocksdb::Env::<span class="literal">HIGH</span>);</span><br><span class="line">rocksdb::DB* db;</span><br><span class="line">rocksdb::Options options;</span><br><span class="line">options.env = env;</span><br><span class="line">options.max_background_compactions = <span class="number">2</span>;</span><br><span class="line">options.max_background_flushes = <span class="number">1</span>;</span><br><span class="line">rocksdb::Status status = rocksdb::DB::Open(options, “/tmp/testdb”, &amp;db);</span><br><span class="line">assert(status.ok());</span><br></pre></td></tr></table></figure><p>还有其他一些参数，可详细阅读参考文档4。<br>​</p><h4 id="1-18-Bloom-Filter"><a href="#1-18-Bloom-Filter" class="headerlink" title="1.18 Bloom Filter"></a>1.18 Bloom Filter</h4><hr><p>RocksDB 的每个 SST 文件都包含一个 Bloom filter。Bloom Filter 只对特定的一组 keys 有效，所以只有新的 SST 文件创建的时候才会生成这个 filter。当两个 SST 文件合并的时候，会生成新的 filter 数据。</p><p>当 SST 文件加载进内存的时候，filter 也会被加载进内存，当关闭 SST 文件的时候，filter 也会被关闭。如果想让 filter 常驻内存，可以用如下代码设置：    </p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">BlockBasedTableOptions::cache_index_and_filter_blocks=<span class="literal">true</span></span><br></pre></td></tr></table></figure><p>一般情况下不要修改 filter 相关参数。如果需要修改，相关设置上面已经说过，此处不再多谈，详细内容见参考文档 7。</p><h4 id="1-19-Time-to-Live"><a href="#1-19-Time-to-Live" class="headerlink" title="1.19 Time to Live"></a>1.19 Time to Live</h4><hr><p>RocksDB 在进行 compact 的时候，会删除被标记为删除的数据，会删除重复 key 的老版本的数据，也会删除过期的数据。数据过期时间由 API <code>DBWithTTL::Open(const Options&amp; options, const std::string&amp; name, StackableDB** dbptr, int32_t ttl = 0, bool read_only = false)</code> 的 ttl 参数设定。</p><p>TTL 的使用有以下注意事项：</p><ul><li>1 TTL 其值单位是秒，如果 TTL 值为 0 或者负数，则 TTL 值为 <code>infinity</code>，即永不过期；</li><li>2 每个 kv 被插入数据文件中的时候会带上创建时的机器 <code>(int32_t)Timestamp</code> 时间值；</li><li>3 compaction 时如果 kv 满足条件 <code>Timestamp+ttl&lt;time_now</code>，则会被淘汰掉；</li><li>4 Get/Iterator 的时候可能返回过期的 kv（compact 任务还未执行）；</li><li>5 不同的 <code>DBWithTTL::Open</code> 可能会带上不同的 TTL 值，此时 kv 以最大的 TTL 值为准；</li><li>6 如果 <code>DBWithTTL::Open</code> 的参数 <code>read_only</code> 为 true，则不会触发 compact 任务，不会有过期数据被删除。</li></ul><h3 id="2-RocksDB-Memory"><a href="#2-RocksDB-Memory" class="headerlink" title="2 RocksDB Memory"></a>2 <a href="https://github.com/facebook/rocksdb/wiki/Memory-usage-in-RocksDB" target="_blank" rel="noopener">RocksDB Memory</a></h3><hr><p>RocksDB的内存大致有如下四个区：</p><ul><li>Block Cache</li><li>Indexes and bloom filters</li><li>Memtables</li><li>Blocked pinned by iterators</li></ul><h4 id="2-1-Block-Cache"><a href="#2-1-Block-Cache" class="headerlink" title="2.1 Block Cache"></a>2.1 Block Cache</h4><hr><p>第三节详述了 Block Cache，这里只给出总结性描述：它存储一些读缓存数据，它的下一层是操作系统的 Page Cache。</p><h4 id="2-2-Indexes-and-bloom-filters"><a href="#2-2-Indexes-and-bloom-filters" class="headerlink" title="2.2 Indexes and bloom filters"></a>2.2 Indexes and bloom filters</h4><hr><p>Index 由 key、offset 和 size 三部分构成，当 Block Cache 增大 Block Size 时，block 个数必会减小，index 个数也会随之降低，如果减小 key size，index 占用内存空间的量也会随之降低。</p><p>filter是 bloom filter 的实现，如果假阳率是 1%，每个key占用 10 bits，则总占用空间就是 <code>num_of_keys * 10 bits</code>，如果缩小 bloom 占用的空间，可以设置 <code>options.optimize_filters_for_hits = true</code>，则最后一个 level 的 filter 会被关闭，bloom 占用率只会用到原来的 10% 。</p><p>结合 block cache 所述，index &amp; filter 有如下优化选项：</p><ul><li><code>cache_index_and_filter_blocks</code> 这个 option 如果为 true，则 index &amp; filter 会被存入 block cache，而 block cache 中的内容会随着 page cache 被交换到磁盘上，这就会大大降低 RocksDB的性能，把这个 option 设为 true 的同时也把 <code>pin_l0_filter_and_index_blocks_in_cache</code> 设为 true，以减小对性能的影响。</li></ul><p>如果 <code>cache_index_and_filter_blocks</code> 被设置为 false （其值默认就是 false），index/filter 个数就会受 <code>max_open_files</code> 影响，官方建议把这个选项设置为 -1，以方便 RocksDB 加载所有的 index 和 filter 文件，最大化程序性能。</p><p>可以通过如下代码获取 index &amp; filter 内存量大小：<br>​</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">std</span>::<span class="built_in">string</span> out;</span><br><span class="line">db-&gt;GetProperty(“rocksdb.estimate-table-readers-mem”, &amp;out);</span><br></pre></td></tr></table></figure><h4 id="2-3-Indexes-and-bloom-filters"><a href="#2-3-Indexes-and-bloom-filters" class="headerlink" title="2.3 Indexes and bloom filters"></a>2.3 Indexes and bloom filters</h4><hr><p>block cache、index &amp; filter 都是读 buffer，而 memtable 则是写 buffer，所有 kv 首先都会被写进 memtable，其 size 是 <code>write_buffer_size</code>。 memtable 占用的空间越大，则写放大效应越小，因为数据在内存被整理好，磁盘上就越少的内容会被 compaction。如果 memtable 磁盘空间增大，则 L1 size 也就随之增大，L1 空间大小受 <code>max_bytes_for_level_base</code> option 控制。</p><p>可以通过如下代码获取 memtable 内存量大小：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">std</span>::<span class="built_in">string</span> out;</span><br><span class="line">db-&gt;GetProperty(“rocksdb.cur-<span class="built_in">size</span>-all-mem-tables”, &amp;out);</span><br></pre></td></tr></table></figure><h4 id="2-4-Blocks-pinned-by-iterators"><a href="#2-4-Blocks-pinned-by-iterators" class="headerlink" title="2.4 Blocks pinned by iterators"></a>2.4 Blocks pinned by iterators</h4><hr><p>这部分内存空间一般占用总量不多，但是如果有 100k 之多的transactions 发生，每个 iterator 与一个 data block 外加一个 L1 的 data block，所以内存使用量大约为 <code>num_iterators * block_size * ((num_levels-1) + num_l0_files)</code>。</p><p>可以通过如下代码获取 Pin Blocks 内存量大小：<br>​</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">table_options.block_cache-&gt;GetPinnedUsage();</span><br></pre></td></tr></table></figure><h4 id="2-5-读流程"><a href="#2-5-读流程" class="headerlink" title="2.5 读流程"></a>2.5 读流程</h4><hr><p>RocksDB 的读流程分为逻辑读(logical read)和物理读(physical read)。逻辑读通常是对 cache【Block Cache &amp; Table Cache】进行读取，物理读就是直接读磁盘。</p><p><img src="/images/%E6%95%B0%E6%8D%AE%E5%BA%93/rocksdb_read_process.png" alt=""></p><p>参考文档 12 详细描述了 LeveDB（RocksDB）的读流程，转述如下：</p><ul><li><p>在MemTable中查找，无法命中转到下一流程；</p></li><li><p>在immutable_memtable中查找，查找不中转到下一流程；</p></li><li><p>在第0层SSTable中查找，无法命中转到下一流程；</p><p>对于L0 的文件，RocksDB 采用遍历的方法查找，所以为了查找效率 RocksDB 会控制 L0 的文件个数。</p></li><li><p>在剩余SSTable中查找。</p><p>对于 L1 层以及 L1 层以上层级的文件，每个 SSTable 没有交叠，可以使用二分查找快速找到 key 所在的 Level 以及 SSTfile。</p></li></ul><p>至于写流程，请参阅 ### 5 Flush &amp; Compaction 章节内容。</p><h4 id="2-6-memory-pool"><a href="#2-6-memory-pool" class="headerlink" title="2.6 memory pool"></a>2.6 memory pool</h4><hr><p>不管 RocksDB 有多少 column family，一个 DB 只有一个 WriteController，一旦 DB 中一个 column family 发生堵塞，那么就会阻塞其他 column family 的写。RocksDB 写入时间长了以后，可能会不定时出现较大的写毛刺，可能有两个地方导致 RocksDB 会出现较大的写延时：获取 mutex 时可能出现几十毫秒延迟 和 将数据写入 memtable 时候可能出现几百毫秒延时。</p><p>获取 mutex 出现的延迟是因为 flush/compact 线程与读写线程竞争导致的，可以通过调整线程数量降低毛刺时间。</p><p>至于写入 memtable 时候出现的写毛刺时间，解决方法一就是使用大的 page cache，禁用系统 swap 以及配置 min_free_kbytes、dirty_ratio、dirty_background_ratio 等参数来调整系统的内存回收策略，更基础的方法是使用内存池。</p><p>采用内存池时，memtable 的内存分配和回收流程图如下：</p><p><img src="/images/%E6%95%B0%E6%8D%AE%E5%BA%93/rocksdb_mempool.png" alt=""></p><p>使用内存池时，RocksDB 的内容分配代码模块如下：</p><p><img src="/images/%E6%95%B0%E6%8D%AE%E5%BA%93/rocksdb_mempool_arena.png" alt=""></p><h3 id="3-Block-Cache"><a href="#3-Block-Cache" class="headerlink" title="3 Block Cache"></a>3 <a href="https://github.com/facebook/rocksdb/wiki/Block-Cache" target="_blank" rel="noopener">Block Cache</a></h3><hr><p>Block Cache 是 RocksDB 的数据的缓存，这个缓存可以在多个 RocksDB 的实例下缓存。一般默认的Block Cache 中存储的值是未压缩的，而用户可以再指定一个 Block Cache，里面的数据可以是压缩的。用户访问数据先访问默认的 Block Cache，待无法获取后再访问用户 Cache，用户 Cache 的数据可以直接存入 page cache 中。</p><p>Cache 有两种：LRUCache 和 BlockCache。Block 分为很多 Shard，以减小竞争，所以 shard 大小均匀一致相等，默认 Cache 最多有 64 个 shards，每个 shard 的 最小 size 为 512k，总大小是 8M，类别是 LRU。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;Cache&gt; cache = NewLRUCache(capacity);  </span><br><span class="line">BlockedBasedTableOptions table_options;  </span><br><span class="line">table_options.block_cache = cache;  </span><br><span class="line">Options options;  </span><br><span class="line">options.table_factory.reset(<span class="keyword">new</span> BlockedBasedTableFactory(table_options));</span><br></pre></td></tr></table></figure><p>这个 Cache 是不压缩数据的，用户可以设置压缩数据 BlockCache，方法如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">table_options.block_cache_compressed = cache;</span><br></pre></td></tr></table></figure><p>如果 Cache 为 nullptr，则RocksDB会创建一个，如果想禁用 Cache，可以设置如下 Option：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">table_options.no_block_cache = <span class="literal">true</span>;</span><br></pre></td></tr></table></figure><p>默认情况下RocksDB用的是 LRUCache，大小是 8MB， 每个 shard 单独维护自己的 LRU list 和独立的 hash table，以及自己的 Mutex。</p><p> RocksDB还提供了一个 ClockCache，每个 shard 有自己的一个 circular list，有一个 clock handle 会轮询这个 circular list，寻找过时的 kv，如果 entry 中的 kv 已经被访问过则可以继续存留，相对于 LRU 好处是无 mutex lock，circular list 本质是 tbb::concurrent_hash_map，从 benchmark 来看，二者命中率相似，但吞吐率 Clock 比 LRU 稍高。</p><p>Block Cache初始化之时相关参数：</p><ul><li>capacity 总的内存使用量</li><li>num_shards_bits 把 key 的前 n bits 作为 shard id，则总 shard 的数目为 2 ^ num_shards_bits；</li><li>strict_capacity_limit 在一些极端情况下 block cache 的总体使用量可能超过 capacity，如在对 block 进行读或者迭代读取的时候可能有插入数据的操作，此时可能因为加锁导致有些数据无法及时淘汰，使得总体capacity超标。如果这个选项设置为 true，则此时插入操作是被允许的，但有可能导致进程 OOM。如果设置为 false，则插入操作会被 refuse，同时读取以及遍历操作有可能失败。这个选项对每个 shard 都有效，这就意味着有的 shard 可能内存已满， 别的 shard 却有很多空闲。</li><li>high_pri_pool_ratio block中为高优先级的 block 保留多少比例的空间，这个选项只有 LRU Cache 有。</li></ul><p>默认情况下 index 和filter block 与 block cache 是独立的，用户不能设定二者的内存空间使用量，但为了控制 RocksDB 的内存空间使用量，可以用如下代码把 index 和 filter 也放在 block cache 中：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">BlockBasedTableOptions table_options;</span><br><span class="line">table_options.cache_index_and_filter_blocks = <span class="literal">true</span>;</span><br></pre></td></tr></table></figure><p>index 与 filter 一般访问频次比 data 高，所以把他们放到一起会导致内存空间与 cpu 资源竞争，进而导致 cache 性能抖动厉害。有如下两个参数需要注意：cache_index_filter_blocks_with_high_priority 和 high_pri_pool_ratio 一样，这个参数只对 LRU Cache 有效，两者须同时生效。这个选项会把 LRU Cache 划分为高 prio 和低 prio 区，data 放在 low 区，index 和 filter 放在 high 区，如果高区占用的内存空间超过了 capacity * high_pri_pool_ratio，则会侵占 low 区的尾部数据空间。</p><ul><li>pin_l0_filter_and_index_blocks_in_cache 把 level0 的 index 以及 filter block 放到 Block Cache 中，因为 l0 访问频次最高，一般内存容量不大，占用不了多大内存空间。</li></ul><p>SimCache 用于评测 Cache 的命中率，它封装了一个真正的 Cache，然后用给定的 capacity 进行 LRU 测算，代码如下:<br>​<br>​```c++<br>​// This cache is the actual cache use by the DB.<br>​std::shared_ptr<Cache> cache = NewLRUCache(capacity);<br>​// This is the simulated cache.<br>​std::shared_ptr<Cache> sim_cache = NewSimCache(cache, sim_capacity, sim_num_shard_bits);<br>​BlockBasedTableOptions table_options;<br>​table_options.block_cache = sim_cache;</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">大概只有容量的 2% 会被用于测算。</span><br><span class="line"></span><br><span class="line">### 4 [Column Families](https:&#x2F;&#x2F;github.com&#x2F;facebook&#x2F;rocksdb&#x2F;wiki&#x2F;Column-Families)</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">RocksDB 3.0 以后添加了一个 Column Family【后面简称 CF】 的feature，每个 kv 存储之时都必须指定其所在的 CF。RocksDB为了兼容以往版本，默认创建一个 “default” 的CF。存储 kv 时如果不指定 CF，RocksDB 会把其存入 “default” CF 中。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#### 4.1 Option</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">RocksDB 的 Option 有 Options, ColumnFamilyOptions, DBOptions 三种。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ColumnFamilyOptions 是 table 级的，而 Options 是 DB 级的，Options 继承自 ColumnFamilyOptions 和 DBOptions，它一般影响只有一个 CF 的 DB，如 “default”。</span><br><span class="line"></span><br><span class="line">每个 CF 都有一个 Handle：ColumnFamilyHandle，在 DB 指针被 delete 前，应该先 delete ColumnFamilyHandle。如果 ColumnFamilyHandle 指向的 CF 被别的使用者通过 DropColumnFamily 删除掉，这个 CF 仍然可以被访问，因为其引用计数不为 0.</span><br><span class="line"></span><br><span class="line">在以 Read&#x2F;Write 方式打开一个 DB 的时候，需要指定一个由所有将要用到的 CF string name 构成的 ColumnFamilyDescriptor array。不管 “default” CF 使用与否，都必须被带上。</span><br><span class="line"></span><br><span class="line">CF 存在的意义是所有 table 共享 WAL，但不共享 memtable 和 table 文件，通过 WAL 保证原子写，通过分离 table 可快读快写快删除。每次 flush 一个 CF 后，都会新建一个 WAL，都这并不意味着旧的 WAL 会被删除，因为别的 CF 数据可能还没有落盘，只有所有的 CF 数据都被 flush 且所有的 WAL 有关的 data 都落盘，相关的 WAL 才会被删除。RocksDB 会定时执行 CF flush 任务，可以通过 &#96;Options::max_total_wal_size&#96; 查看已有多少旧的 CF 文件已经被 flush 了。</span><br><span class="line"></span><br><span class="line">RocksDB 会在磁盘上依据 LSM 算法对多级磁盘文件进行 compaction，这会影响写性能，拖慢程序性能，可以通过 &#96;WriteOptions.low_pri &#x3D; true&#96; 降低 compaction 的优先级。</span><br><span class="line"></span><br><span class="line">#### 4.2 [Set Up Option](https:&#x2F;&#x2F;github.com&#x2F;facebook&#x2F;rocksdb&#x2F;wiki&#x2F;Set-Up-Options)</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">RocksDB 有很多选项以专门的目的进行设置，但是大部分情况下不需要进行特殊的优化。这里只列出一个常用的优化选项。</span><br><span class="line"></span><br><span class="line">* &#96;cf_options.write_buffer_size&#96;</span><br><span class="line"></span><br><span class="line">CF 的 write buffer 的最大 size。最差情况下 RocksDB 使用的内存量会翻倍，所以一般情况下不要轻易修改其值。</span><br><span class="line"></span><br><span class="line">* Set block cache size</span><br><span class="line"></span><br><span class="line">这个值一般设置为 RocksDB 想要使用的内存总量的 1&#x2F;3，其余的留给 OS 的 page cache。</span><br><span class="line"></span><br><span class="line">&#96;&#96;&#96;C++</span><br><span class="line">BlockBasedTableOptions table_options;</span><br><span class="line">… \\ set options in table_options</span><br><span class="line">options.table_factory.reset(new</span><br><span class="line"></span><br><span class="line">std::shared_ptr&lt;Cache&gt; cache &#x3D; NewLRUCache(&lt;your_cache_size&gt;);</span><br><span class="line">table_options.block_cache &#x3D; cache;</span><br><span class="line"></span><br><span class="line">BlockBasedTableFactory(table_options));</span><br></pre></td></tr></table></figure><p>本进程的所有的 DB 所有的 CF 所有的 table_options 都必须使用同一个 cahce 对象，或者让所有的 DB 所有的 CF 使用同一个 table_options。</p><ul><li><code>cf_options.compression, cf_options.bottonmost_compression</code></li></ul><p>选择压缩方法跟你的机器、CPU 能力以及内存磁盘空间大小有关，官方推荐 <code>cf_options.compression</code> 使用 kLZ4Compression，<code>cf_options.bottonmost_compression</code> 使用 kZSTD，选用的时候要确认你的机器有这两个库，这两个选项也可以分别使用 Snappy 和 Zlib。</p><ul><li>Bloom filter</li></ul><p><font color=red><strong>官方真正建议修改的参数只有这个 filter 参数。如果大量使用迭代方法，不要修改这个参数，如果大量调用 Get() 接口，建议修改这个参数。</strong></font>修改方法如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">table_options.filter_policy.reset(NewBloomFilterPolicy(<span class="number">10</span>, <span class="literal">false</span>));</span><br></pre></td></tr></table></figure><p>一个可能的优化设定如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cf_options.level_compaction_dynamic_level_bytes &#x3D; true;</span><br><span class="line">options.max_background_compactions &#x3D; 4;</span><br><span class="line">options.max_background_flushes &#x3D; 2;</span><br><span class="line">options.bytes_per_sync &#x3D; 1048576;</span><br><span class="line">table_options.block_size &#x3D; 16 * 1024;</span><br><span class="line">table_options.cache_index_and_filter_blocks &#x3D; true;</span><br><span class="line">table_options.pin_l0_filter_and_index_blocks_in_cache &#x3D; true;</span><br></pre></td></tr></table></figure><p>上面只是罗列了一些优化选项，这些选项也只能在进程启动的时候设定。更多的选项请详细阅读参考文档1。</p><h4 id="4-3-WriteOption-amp-Persistence"><a href="#4-3-WriteOption-amp-Persistence" class="headerlink" title="4.3 WriteOption &amp; Persistence"></a>4.3 WriteOption &amp; Persistence</h4><hr><p>参考文档 5 的 Persistence 一节提到，RocksDB 每次接收写请求的时候，请求内容会先被写入 WAL transaction log，然后再把数据写入 memfile 里面。</p><p>Put 函数的参数 WriteOptions 里有一个选项可以指明是否需要把写请求的内容写入 WAL log 里面。</p><p>RocksDB 内部有一个 batch-commit 机制，通过一次 commit 批量地在一次 sync 操作里把所有 transactions log 写入磁盘。</p><h3 id="5-Flush-amp-Compaction-amp-Merge"><a href="#5-Flush-amp-Compaction-amp-Merge" class="headerlink" title="5 Flush &amp; Compaction &amp; Merge"></a>5 Flush &amp; Compaction &amp; Merge</h3><hr><p>RocksDB 的内存数据在 memtable 中存着，有 active-memtable 和 immutable-memtable 两种。active-memtable 是当前被写操作使用的 memtable，当 active-memtable 空间写满之后( Options.write_buffer_size 控制其内存空间大小 )这个空间会被标记为 readonly 成为 immutable-memtable。memtable 实质上是一种有序 SkipList，所以写过程其实是写 WAL 日志和数据插入 SkipList 的过程。</p><p><img src="/images/%E6%95%B0%E6%8D%AE%E5%BA%93/rocksdb_write_process.png" alt=""></p><p>RocksDB 的数据删除过程跟写过程相同，只不过 插入的数据是 “key:删除标记”。</p><p>immutable-memtable 被写入 L0 的过程被称为 flush 或者 minor compaction。flush 的触发条件是 immutable memtable数量超过 min_write_buffer_number_to_merge。flush 过程以 column family 为单位，一个 column family 会使用一个或者多个 immutable-memtable，flush 会一次把所有这些文件合并后写入磁盘的 L0 sstfile 中。</p><p>在 compaction 过程中如果某个被标记为删除的 key 在某个 snapshot 中存在，则会被一直保留，直到 snapshot 不存在才会被删除。</p><p>RocksDB 的 compaction 策略分为 <code>Universal Compaction</code> 和 <code>Leveled Compaction</code> 两种。两种策略分别有不同的使用场景，下面分两个章节详述。综述就是  <code>Leveled Compaction</code> 有利于减小空间放大却会增加读放大，<code>Universal Compaction</code> 有利于减少读放大却会增大空间放大。</p><h4 id="5-1-Leveled-Compaction"><a href="#5-1-Leveled-Compaction" class="headerlink" title="5.1 Leveled Compaction"></a>5.1 Leveled Compaction</h4><hr><p>compaction 的触发条件是文件个数和文件大小。L0 的触发条件是 sst 文件个数（level0_file_num_compaction_trigger 控制），触发 compaction score 是 L0 sst 文件个数与 level0_file_num_compaction_trigger 的比值或者所有文件的 size 超过 max_bytes_for_level_base。L1 ~ LN 的触发条件则是 sst 文件的大小。</p><p>如果 <code>level_compaction_dynamic_level_bytes</code> 为 false，L1 ~ LN 每个 level 的最大容量由 <code>max_bytes_for_level_base</code> 和 <code>max_bytes_for_level_multiplier</code> 决定，其 compaction score 就是当前总容量与设定的最大容量之比，如果某 level 满足 compaction 的条件则会被加入 compaction 队列。</p><p>如果 <code>level_compaction_dynamic_level_bytes</code> 为 true，则 <code>Target_Size(Ln-1) = Target_Size(Ln) / max_bytes_for_level_multiplier</code>，此时如果某 level 计算出来的 target 值小于 <code>max_bytes_for_level_base / max_bytes_for_level_multiplier</code>，则 RocksDB 不会再这个 level 存储任何 sst 数据。</p><h5 id="5-1-1-Compaction-Score"><a href="#5-1-1-Compaction-Score" class="headerlink" title="5.1.1 Compaction Score"></a>5.1.1 Compaction Score</h5><hr><p>compact 流程的 Compaction Score，不同 level 的计算方法不一样，下面先列出 L0 的计算方法。其中 num 代表未 compact 文件的数目。</p><table><thead><tr><th align="left">Param</th><th align="left">Value</th><th align="left">Description</th><th align="left">Score</th></tr></thead><tbody><tr><td align="left">level0_file_num_compaction_trigger</td><td align="left">4</td><td align="left">num 为 4 时，达到 compact 条件</td><td align="left">num &lt; 20 时 Score = num / 4</td></tr><tr><td align="left">level0_slowdown_writes_trigger</td><td align="left">20</td><td align="left">num 为 20 时，RocksDB 会减慢写入速度</td><td align="left">20 &lt;= num &amp;&amp; num &lt; 24 时 Score = 10000</td></tr><tr><td align="left">level0_stop_writes_trigger</td><td align="left">24</td><td align="left">num 为 24 时，RocksDB 停止写入文件，尽快对 L0 进行 compact</td><td align="left">24 &lt;= num 时 Score = 1000000</td></tr></tbody></table><p>对于 L1+ 层，score = Level_Bytes / Target_Size。</p><h5 id="5-1-2-Level-Max-Bytes"><a href="#5-1-2-Level-Max-Bytes" class="headerlink" title="5.1.2 Level Max Bytes"></a>5.1.2 Level Max Bytes</h5><hr><p>每个 level 容量总大小的计算前文已经提过，</p><table><thead><tr><th align="left">Param</th><th align="left">Value</th><th align="left">Description</th></tr></thead><tbody><tr><td align="left">max_bytes_for_level_base</td><td align="left">10485760</td><td align="left">L1 总大小</td></tr><tr><td align="left">max_bytes_for_level_multiplier</td><td align="left">10</td><td align="left">最大乘法因子</td></tr><tr><td align="left">max_bytes_for_level_multiplier_addtl[2…6]</td><td align="left">1</td><td align="left">L2 ~ L6 总大小调整参数</td></tr></tbody></table><p>每个 level 的总大小计算公式为 <code>Level_max_bytes[N] = Level_max_bytes[N-1] * max_bytes_for_level_multiplier^(N-1)*max_bytes_for_level_multiplier_additional[N-1]</code>。</p><h5 id="5-1-3-compact-file"><a href="#5-1-3-compact-file" class="headerlink" title="5.1.3 compact file"></a>5.1.3 compact file</h5><hr><p>上面详述了 compact level 的选择，但是每个 level 具体的 compact 文件对象，</p><p>L0 层所有文件会被选做 compact 对象，因为它们有很高的概率所有文件的 key range 发生重叠。</p><p>对于 L1+ 层的文件，先对所有文件的大小进行排序以选出最大文件。</p><p>LevelDB 的文件选取过程如下：</p><p>LN 中每个文件都一个 seek 数值，其默认值非零，每次访问后这个数值减 1，其值越小说明访问越频繁。sst 文件的策略如下：</p><ul><li>1 选择 seek 次数为 0 的文件进行 merge，如果没有 seek 次数为 0 的文件，则从第一个文件开始进行 compact；</li><li>2 一次 compact 后记录本次结束的 key，下次 compact 开始时以这个 key 为起始继续进行 compact。</li></ul><h5 id="5-1-4-compaction"><a href="#5-1-4-compaction" class="headerlink" title="5.1.4 compaction"></a>5.1.4 compaction</h5><hr><p>大致的 compaction 流程大致为：</p><ul><li>1 找到 score 最高的 level；</li><li>2 根据一定策略从 level 中选择一个 sst 文件进行 compact，L0 的各个 sst 文件之间 key range 【minkey， maxkey】 有重叠，所以可能一次选取多个；</li><li>3 获取 sst 文件的 minkey 和 maxkey;</li><li>4 从 level + 1 中选取出于 (minkey, maxkey) 用重叠的 sst 文件，有重叠的文件则把文件与 level 中的文件进行合并（merge - sort）作为目标文件，没有重叠文件则把原始文件作为目标文件；</li><li>5 对目标文件进行压缩后放入 level + 1 中。</li></ul><h5 id="5-1-5-并行-Compact-与-sub-compact"><a href="#5-1-5-并行-Compact-与-sub-compact" class="headerlink" title="5.1.5 并行 Compact 与 sub-compact"></a>5.1.5 并行 Compact 与 sub-compact</h5><hr><p>参数 max_background_compactions 大于 1 时，RocksDB 会进行并行 Compact，但 L0 和 L1 层的 Compaction 任务不能进行并行。</p><p><img src="/images/%E6%95%B0%E6%8D%AE%E5%BA%93/rocksdb_subcompaction.png" alt=""></p><p>一次 compaction 只能 compact 一个或者多个文件，这会约束整体 compaction 速度。用户可以设置 max_subcompactions 参数大于 1，RocksDB 如上图一样尝试把一个文件拆为多个 sub，然后启动多个线程执行 sub-compact。</p><h4 id="5-2-Universal-Compaction"><a href="#5-2-Universal-Compaction" class="headerlink" title="5.2 Universal Compaction"></a>5.2 Universal Compaction</h4><hr><p>Univesal Compaction 主要针对 L0。当 L0 中的文件个数多于 <code>level0_file_num_compaction_trigger</code>，则启动 compact。</p><p>L0 中所有的 sst 文件都可能存在重叠的 key range，假设所有的 sst 文件组成了文件队列 R1,R2,R3,…,Rn，R1 文件的数据是最新的，R2 其次，Rn 则包含了最老的数据，其 compact 流程如下：</p><ul><li>1 如果空间放大超过 <code>max_size_amplification_percent</code>，则对所有的 sst 进行 compaction（就是所谓的 full compaction）；</li><li>2 如果前size(R1)小于size(R2)在一定比例，默认1%，则与R1与R2一起进行compaction，如果（R1+R2)*(100+ratio)%100&lt;R3，则将R3也加入到compaction任务中，依次顺序加入sst文件；</li><li>如果第1和第2种情况都没有compaction，则强制选择前N个文件进行合并。</li></ul><p><code>Universal Compaction</code> 主要针对低写放大场景，跟 <code>Leveled Compaction</code> 相比一次合并文件较多但因为一次只处理 L0 所以写放大整体较低，但是空间放大效应比较大。</p><p>RocksDB 还支持一种 FIFO 的 compaction。FIFO 顾名思义就是先进先出，这种模式周期性地删除旧数据。在 FIFO 模式下，所有文件都在 L0，当 sst 文件总大小超过阀值 max_table_files_size，则删除最老的 sst 文件。<a href="https://www.jianshu.com/p/0fdeed70b36a" target="_blank" rel="noopener">参考文档21</a>中提到可以基于 FIFO compaction 机制把 RocksDB 当做一个时序数据库：<code>对于 FIFO 来说，它的策略非常的简单，所有的 SST 都在 Level 0，如果超过了阈值，就从最老的 SST 开始删除，其实可以看到，这套机制非常适合于存储时序数据</code>。</p><p>整个 compaction 是 LSM-tree 数据结构的核心，也是rocksDB的核心，详细内容请阅读 <a href="https://github.com/facebook/rocksdb/wiki/Universal-Compaction" target="_blank" rel="noopener">参考文档8</a> 和 <a href="https://github.com/facebook/rocksdb/wiki/Leveled-Compaction" target="_blank" rel="noopener">参考文档9</a>。</p><h4 id="5-4-Merge"><a href="#5-4-Merge" class="headerlink" title="5.4 Merge"></a>5.4 Merge</h4><hr><p>RocksDB 自身之提供了 Put/Delete/Get 等接口，若需要在现有值上进行修改操作【或者成为增量更新】，可以借助这三个操作进行以下操作实现之：</p><ul><li>调用 Get 接口然后获取其值；</li><li>在内存中修改这个值；</li><li>调用 Put 接口写回 RocksDB。</li></ul><p>如果希望整个过程是原子操作，就需要借助 RocksDB 的 Merge 接口了。<a href="https://www.jianshu.com/p/e13338a3f161" target="_blank" rel="noopener">参考文档14</a> 给出了 RocksDB Merge 接口定义如下：</p><ul><li>1 封装了read - modify - write语义，对外统一提供简单的抽象接口；</li><li>2 减少用户重复触发Get操作引入的性能损耗；</li><li>3 通过决定合并操作的时间和方式，来优化后端性能，并达到并不改变底层更新的语义；</li><li>4 渐进式的更新，来均摊更新带来带来的性能损耗，以得到渐进式的性能提升。</li></ul><p>RocksDB 提供了一个 MergeOperator 作为 Merge 接口，其中一个子类 AssociativeMergeOperator 可在大部分场景下使用，其定义如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// The simpler, associative merge operator.</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AssociativeMergeOperator</span> :</span> <span class="keyword">public</span> MergeOperator &#123;</span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  <span class="keyword">virtual</span> ~AssociativeMergeOperator() &#123;&#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Gives the client a way to express the read -&gt; modify -&gt; write semantics</span></span><br><span class="line">  <span class="comment">// key:           (IN) 操作对象 KV 的 key</span></span><br><span class="line">  <span class="comment">// existing_value:(IN) 操作对象 KV 的 value，如果为 null 则意味着 KV 不存在</span></span><br><span class="line">  <span class="comment">// value:         (IN) 新值，用于替换/更新 @existing_value</span></span><br><span class="line">  <span class="comment">// new_value:    (OUT) 客户端负责把 merge 后的新值填入这个变量</span></span><br><span class="line">  <span class="comment">// logger:        (IN) Client could use this to log errors during merge.</span></span><br><span class="line">  <span class="comment">//</span></span><br><span class="line">  <span class="comment">// Return true on success.</span></span><br><span class="line">  <span class="comment">// All values passed in will be client-specific values. So if this method</span></span><br><span class="line">  <span class="comment">// returns false, it is because client specified bad data or there was</span></span><br><span class="line">  <span class="comment">// internal corruption. The client should assume that this will be treated</span></span><br><span class="line">  <span class="comment">// as an error by the library.</span></span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> <span class="keyword">bool</span> <span class="title">Merge</span><span class="params">(<span class="keyword">const</span> Slice&amp; key,</span></span></span><br><span class="line"><span class="function"><span class="params">                     <span class="keyword">const</span> Slice* existing_value,</span></span></span><br><span class="line"><span class="function"><span class="params">                     <span class="keyword">const</span> Slice&amp; value,</span></span></span><br><span class="line"><span class="function"><span class="params">                     <span class="built_in">std</span>::<span class="built_in">string</span>* new_value,</span></span></span><br><span class="line"><span class="function"><span class="params">                     Logger* logger)</span> <span class="keyword">const</span> </span>= <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">private</span>:</span><br><span class="line">  <span class="comment">// Default implementations of the MergeOperator functions</span></span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> <span class="keyword">bool</span> <span class="title">FullMergeV2</span><span class="params">(<span class="keyword">const</span> MergeOperationInput&amp; merge_in,</span></span></span><br><span class="line"><span class="function"><span class="params">                           MergeOperationOutput* merge_out)</span> <span class="keyword">const</span> <span class="keyword">override</span></span>;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> <span class="keyword">bool</span> <span class="title">PartialMerge</span><span class="params">(<span class="keyword">const</span> Slice&amp; key,</span></span></span><br><span class="line"><span class="function"><span class="params">                            <span class="keyword">const</span> Slice&amp; left_operand,</span></span></span><br><span class="line"><span class="function"><span class="params">                            <span class="keyword">const</span> Slice&amp; right_operand,</span></span></span><br><span class="line"><span class="function"><span class="params">                            <span class="built_in">std</span>::<span class="built_in">string</span>* new_value,</span></span></span><br><span class="line"><span class="function"><span class="params">                            Logger* logger)</span> <span class="keyword">const</span> <span class="keyword">override</span></span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>RocksDB AssociativeMergeOperator 被称为关联性 Merge Operator，<a href="https://www.jianshu.com/p/e13338a3f161" target="_blank" rel="noopener">参考文档14</a>  给出了关联性的定义：</p><ul><li>调用Put接口写入RocksDB的数据的格式和Merge接口是相同的</li><li>用用户自定义的merge操作，可以将多个merge操作数合并成一个</li></ul><p><code>**MergeOperator还可以用于非关联型数据类型的更新。** 例如，在RocksDB中保存json字符串，即Put接口写入data的格式为合法的json字符串。而Merge接口只希望更新json中的某个字段。所以代码可能是这样</code>：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">   <span class="comment">// Put/store the json string into to the database</span></span><br><span class="line">   db_-&gt;Put(put_option_, <span class="string">"json_obj_key"</span>,</span><br><span class="line">            <span class="string">"&#123; employees: [ &#123;first_name: john, last_name: doe&#125;, &#123;first_name: adam, last_name: smith&#125;] &#125;"</span>);</span><br><span class="line">   <span class="comment">// Use a pre-defined "merge operator" to incrementally update the value of the json string</span></span><br><span class="line">   db_-&gt;Merge(merge_option_, <span class="string">"json_obj_key"</span>, <span class="string">"employees[1].first_name = lucy"</span>);</span><br><span class="line">   db_-&gt;Merge(merge_option_, <span class="string">"json_obj_key"</span>, <span class="string">"employees[0].last_name = dow"</span>);</span><br><span class="line">`AssociativeMergeOperator无法处理这种场景，因为它假设Put和Merge的数据格式是关联的。我们需要区分Put和Merge的数据格式，也无法把多个merge操作数合并成一个。这时候就需要Generic MergeOperator。`</span><br><span class="line"></span><br><span class="line">   <span class="comment">// The Merge Operator</span></span><br><span class="line">   <span class="comment">//</span></span><br><span class="line">   <span class="comment">// Essentially, a MergeOperator specifies the SEMANTICS of a merge, which only</span></span><br><span class="line">   <span class="comment">// client knows. It could be numeric addition, list append, string</span></span><br><span class="line">   <span class="comment">// concatenation, edit data structure, ... , anything.</span></span><br><span class="line">   <span class="comment">// The library, on the other hand, is concerned with the exercise of this</span></span><br><span class="line">   <span class="comment">// interface, at the right time (during get, iteration, compaction...)</span></span><br><span class="line">   <span class="class"><span class="keyword">class</span> <span class="title">MergeOperator</span> &#123;</span></span><br><span class="line">    <span class="keyword">public</span>:</span><br><span class="line">     <span class="keyword">virtual</span> ~MergeOperator() &#123;&#125;</span><br><span class="line"></span><br><span class="line">     <span class="comment">// Gives the client a way to express the read -&gt; modify -&gt; write semantics</span></span><br><span class="line">     <span class="comment">// key:         (IN) The key that's associated with this merge operation.</span></span><br><span class="line">     <span class="comment">// existing:    (IN) null indicates that the key does not exist before this op</span></span><br><span class="line">     <span class="comment">// operand_list:(IN) the sequence of merge operations to apply, front() first.</span></span><br><span class="line">     <span class="comment">// new_value:  (OUT) Client is responsible for filling the merge result here</span></span><br><span class="line">     <span class="comment">// logger:      (IN) Client could use this to log errors during merge.</span></span><br><span class="line">     <span class="comment">//</span></span><br><span class="line">     <span class="comment">// Return true on success. Return false failure / error / corruption.</span></span><br><span class="line">     <span class="comment">// 用于对已有的值做Put或Delete操作</span></span><br><span class="line">     <span class="function"><span class="keyword">virtual</span> <span class="keyword">bool</span> <span class="title">FullMerge</span><span class="params">(<span class="keyword">const</span> Slice&amp; key,</span></span></span><br><span class="line"><span class="function"><span class="params">                            <span class="keyword">const</span> Slice* existing_value,</span></span></span><br><span class="line"><span class="function"><span class="params">                            <span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">deque</span>&lt;<span class="built_in">std</span>::<span class="built_in">string</span>&gt;&amp; operand_list,</span></span></span><br><span class="line"><span class="function"><span class="params">                            <span class="built_in">std</span>::<span class="built_in">string</span>* new_value,</span></span></span><br><span class="line"><span class="function"><span class="params">                            Logger* logger)</span> <span class="keyword">const</span> </span>= <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">     <span class="comment">// This function performs merge(left_op, right_op)</span></span><br><span class="line">     <span class="comment">// when both the operands are themselves merge operation types.</span></span><br><span class="line">     <span class="comment">// Save the result in *new_value and return true. If it is impossible</span></span><br><span class="line">     <span class="comment">// or infeasible to combine the two operations, return false instead.</span></span><br><span class="line">     <span class="comment">// 如果连续多次对一个 key 进行操作，则可以可以借助 PartialMerge 将两个操作数合并.</span></span><br><span class="line">     <span class="function"><span class="keyword">virtual</span> <span class="keyword">bool</span> <span class="title">PartialMerge</span><span class="params">(<span class="keyword">const</span> Slice&amp; key,</span></span></span><br><span class="line"><span class="function"><span class="params">                               <span class="keyword">const</span> Slice&amp; left_operand,</span></span></span><br><span class="line"><span class="function"><span class="params">                               <span class="keyword">const</span> Slice&amp; right_operand,</span></span></span><br><span class="line"><span class="function"><span class="params">                               <span class="built_in">std</span>::<span class="built_in">string</span>* new_value,</span></span></span><br><span class="line"><span class="function"><span class="params">                               Logger* logger)</span> <span class="keyword">const</span> </span>= <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">     <span class="comment">// The name of the MergeOperator. Used to check for MergeOperator</span></span><br><span class="line">     <span class="comment">// mismatches (i.e., a DB created with one MergeOperator is</span></span><br><span class="line">     <span class="comment">// accessed using a different MergeOperator)</span></span><br><span class="line">     <span class="function"><span class="keyword">virtual</span> <span class="keyword">const</span> <span class="keyword">char</span>* <span class="title">Name</span><span class="params">()</span> <span class="keyword">const</span> </span>= <span class="number">0</span>;</span><br><span class="line">   &#125;;</span><br></pre></td></tr></table></figure><ul><li><strong>工作原理</strong></li></ul><p>当调用DB::Put()和DB:Merge()接口时, 并不需要立刻计算最后的结果. RocksDB将计算的动作延后触发, 例如在下一次用户调用Get, 或者RocksDB决定做Compaction时. 所以, 当merge的动作真正开始做的时候, 可能积压(stack)了多个操作数需要处理. 这种情况就需要MergeOperator::FullMerge来对existing_value和一个操作数序列进行计算, 得到最终的值.</p><ul><li><strong>PartialMerge 和 Stacking</strong></li></ul><p>有时候, 在调用FullMerge之前, 可以先对某些merge操作数进行合并处理, 而不是将它们保存起来, 这就是PartialMerge的作用: 将两个操作数合并为一个, 减少FullMerge的工作量.<br>当遇到两个merge操作数时, RocksDB总是先会尝试调用用户的PartialMerge方法来做合并, 如果PartialMerge返回false才会保存操作数. 当遇到Put/Delete操作, 就会调用FullMerge将已存在的值和操作数序列传入, 计算出最终的值.</p><ul><li><strong>使用Associative Merge的场景</strong></li></ul><p>merge 操作数的格式和Put相同<br>多个顺序的merge操作数可以合并成一个</p><ul><li><strong>使用Generic Merge的场景</strong></li></ul><p>merge 操作数的格式和Put不同<br>当多个merge操作数可以合并时，PartialMerge()方法返回true</p><p>*!!!: 本节文字摘抄自 <a href="https://www.jianshu.com/p/e13338a3f161" target="_blank" rel="noopener">参考文档14</a>  。</p><h3 id="6-磁盘文件"><a href="#6-磁盘文件" class="headerlink" title="6 磁盘文件"></a>6 磁盘文件</h3><hr><p>参考文档 12 列举了 RocksDB 磁盘上数据文件的种类：</p><pre><code>* db的操作日志* 存储实际数据的 SSTable 文件* DB的元信息 Manifest 文件* 记录当前正在使用的 Manifest 文件，它的内容就是当前的 manifest 文件名* 系统的运行日志，记录系统的运行信息或者错误日志。* 临时数据库文件，repair 时临时生成的。</code></pre><p>manifest 文件记载了所有 SSTable 文件的 key 的范围、level 级别等数据。</p><p><img src="/images/%E6%95%B0%E6%8D%AE%E5%BA%93/leveldb_arch.png" alt=""></p><p>上面是 leveldb 的架构图，可以作为参考，明白各种文件的作用。</p><h4 id="6-1-log-文件"><a href="#6-1-log-文件" class="headerlink" title="6.1 log 文件"></a>6.1 log 文件</h4><hr><p>log 文件就是 WAL。</p><p><img src="/images/%E6%95%B0%E6%8D%AE%E5%BA%93/rocksdb_log_block_record.png" alt=""></p><p>如上图，log 文件的逻辑单位是 Record，物理单位是 block，每个 Record 可以存在于一个 block 中，也可以占用多个 block。Record 的详细结构见上图文字部分，其 type 字段的意义见下图。</p><p><img src="/images/%E6%95%B0%E6%8D%AE%E5%BA%93/rocksdb_log_record.png" alt=""></p><p>从上图可见 Record type的意义：如果某 KV 过长则可以用多 Record 存储。</p><h4 id="6-2-Manifest-文件"><a href="#6-2-Manifest-文件" class="headerlink" title="6.2 Manifest 文件"></a>6.2 Manifest 文件</h4><hr><p>RocksDB 整个 LSM 树的信息需要常驻内存，以让 RocksDB 快速进行 kv 查找或者进行 compaction 任务，RocksDB 会用文件把这些信息固化下来，这个文件就是 Manifest 文件。RocksDB 称 Manifest 文件记录了 DB 状态变化的事务性日志，也就是说它记录了所有改变 DB 状态的操作。主要内容有事务性日志和数据库状态的变化。</p><p>RocksDB 的函数 VersionSet::LogAndApply 是对 Manifest 文件的更新操作，所以可以通过定位这个函数出现的位置来跟踪 Manifest 的记录内容。</p><p>Manifest 文件作为事务性日志文件，只要数据库有变化，Manifest都会记录。其内容 size 超过设定值后会被 VersionSet::WriteSnapShot 重写。</p><p>RocksDB 进程 Crash 后 Reboot 的过程中，会首先读取 Manifest 文件在内存中重建 LSM 树，然后根据 WAL 日志文件恢复 memtable 内容。</p><p><img src="/images/%E6%95%B0%E6%8D%AE%E5%BA%93/leveldb_manifest.jpg" alt=""></p><p>上图是 leveldb 的 Manifest 文件结构，这个 Manifest 文件有以下文件内容：</p><ul><li>coparator名、log编号、前一个log编号、下一个文件编号、上一个序列号，这些都是日志、sstable文件使用到的重要信息，这些字段不一定必然存在；</li><li>其次是compact点，可能有多个，写入格式为{kCompactPointer, level, internal key}</li><li>其后是删除文件，可能有多个，格式为{kDeletedFile, level, file number}。</li><li>最后是新文件，可能有多个，格式为{kNewFile, level, file number, file size, min key, max key}。</li></ul><p>RocksDB MANIFEST文件所保存的数据基本是来自于VersionEdit这个结构，MANIFEST包含了两个文件，一个log文件一个包含最新MANIFEST文件名的文件，Manifest的log文件名是这样 MANIFEST-(seqnumber)，这个seq会一直增长，只有当 超过了指定的大小之后，MANIFEST会刷新一个新的文件，当新的文件刷新到磁盘(并且文件名更新)之后，老的文件会被删除掉，这里可以认为每一次MANIFEST的更新都代表一次snapshot，其结构描述如下：</p><pre><code>MANIFEST = { CURRENT, MANIFEST-&lt;seq-no&gt;* }  CURRENT = File pointer to the latest manifest logMANIFEST-&lt;seq no&gt; = Contains snapshot of RocksDB state and subsequent modifications</code></pre><p>在RocksDB中任意时间存储引擎的状态都会保存为一个Version(也就是SST的集合)，而每次对Version的修改都是一个VersionEdit,而最终这些VersionEdit就是 组成manifest-log文件的内容。</p><p>下面就是MANIFEST的log文件的基本构成:</p><pre><code>version-edit      = Any RocksDB state changeversion           = { version-edit* }manifest-log-file = { version, version-edit* }                  = { version-edit* }</code></pre><p>关于 VersionSet 相关代码分析见<a href="https://yq.aliyun.com/articles/594728?spm=a2c4e.11157919.spm-cont-list.17.302c27aeDR3OyC" target="_blank" rel="noopener">参考文档13</a>。</p><h4 id="6-3-SSTfile"><a href="#6-3-SSTfile" class="headerlink" title="6.3 SSTfile"></a>6.3 SSTfile</h4><hr><p>SSTfile 结构如下：</p><pre><code>&lt;beginning_of_file&gt;[data block 1][data block 2]…[data block N][meta block 1: filter block]                 [meta block 2: stats block]                   [meta block 3: compression dictionary block]  …[meta block K: future extended block][metaindex block][index block][Footer]                              &lt;end_of_file&gt;</code></pre><p>LevelDB 的 SSTfile 结构如下：</p><p><img src="/images/%E6%95%B0%E6%8D%AE%E5%BA%93/leveldb_sst_file.png" alt=""></p><p>见参考文档12，SSTtable 大致分为几个部分：</p><ul><li>数据块 Data Block，直接存储有序键值对，是 SSTfile 的数据实体；</li><li>Meta Block，存储Filter相关信息；</li><li>Meta Index Block，对Meta Block的索引，它只有一条记录，key是meta index的名字（也就是Filter的名字），value为指向meta index的位置；</li><li>Index Block，是对Data Block的索引，对于其中的每个记录，其key &gt;=Data Block最后一条记录的key，同时&lt;其后Data Block的第一条记录的key；value是指向data index的位置信息；</li><li>Footer，指向各个分区的位置和大小。</li></ul><p>block 结构如下图：</p><p><img src="/images/%E6%95%B0%E6%8D%AE%E5%BA%93/leveldb_sst_block.jpg" alt=""></p><p>record 结构如下图：</p><p><img src="/images/%E6%95%B0%E6%8D%AE%E5%BA%93/leveldb_sst_record.jpg" alt=""></p><p>Footer 结构如下图：</p><p><img src="/images/%E6%95%B0%E6%8D%AE%E5%BA%93/leveldb_sst_footer.jpg" alt=""></p><h4 id="6-4-memtable"><a href="#6-4-memtable" class="headerlink" title="6.4 memtable"></a>6.4 memtable</h4><hr><p>memtable 中存储了一些 metadata 和 data，data 在 skiplist 中存储。metadata 数据如下（源自参考文档 12）：</p><ul><li>当前日志句柄；</li><li>版本管理器、当前的版本信息（对应 compaction）和对应的持久化文件标示；</li><li>当前的全部db配置信息比如 comparator 及其对应的 memtable 指针；</li><li>当前的状态信息以决定是否需要持久化 memtable 和合并 sstable；</li><li>sstable 文件集合的信息。</li></ul><h4 id="6-5-VersionSet"><a href="#6-5-VersionSet" class="headerlink" title="6.5 VersionSet"></a>6.5 VersionSet</h4><hr><p>RocksDB 的 Version 表示一个版本的 metadata，其主要内容是 FileMetaData 指针的二维数组，分层记录了所有的SST文件信息。</p><p>FileMetaData 数据结构用来维护一个文件的元信息，包括文件大小，文件编号，最大最小值，引用计数等信息，其中引用计数记录了被不同的Version引用的个数，保证被引用中的文件不会被删除。</p><p>Version中还记录了触发 Compaction 相关的状态信息，这些信息会在读写请求或 Compaction 过程中被更新。在 CompactMemTable 和 BackgroundCompaction 过程中会导致新文件的产生和旧文件的删除，每当这个时候都会有一个新的对应的Version生成，并插入 VersionSet 链表头部，LevelDB 用 VersionEdit 来表示这种相邻 Version 的差值。</p><p><img src="/images/%E6%95%B0%E6%8D%AE%E5%BA%93/leveldb_versionset.png" alt=""></p><p>VersionSet 结构如上图所示，它是一个 Version 构成的双向链表，这些Version按时间顺序先后产生，记录了当时的元信息，链表头指向当前最新的Version，同时维护了每个Version的引用计数，被引用中的Version不会被删除，其对应的SST文件也因此得以保留，通过这种方式，使得LevelDB可以在一个稳定的快照视图上访问文件。</p><p>VersionSet中除了Version的双向链表外还会记录一些如LogNumber，Sequence，下一个SST文件编号的状态信息。</p><h4 id="6-6-MetaData-Restore"><a href="#6-6-MetaData-Restore" class="headerlink" title="6.6 MetaData Restore"></a>6.6 MetaData Restore</h4><hr><p>本节内容节选自参考文档 12。</p><p>为了避免进程崩溃或机器宕机导致的数据丢失，LevelDB 需要将元信息数据持久化到磁盘，承担这个任务的就是 Manifest 文件，每当有新的Version产生都需要更新 Manifest。</p><p><img src="/images/%E6%95%B0%E6%8D%AE%E5%BA%93/leveldb_version_manifest.png" alt=""></p><p>新增数据正好对应于VersionEdit内容，也就是说Manifest文件记录的是一组VersionEdit值，在Manifest中的一次增量内容称作一个Block。</p><p><img src="/images/%E6%95%B0%E6%8D%AE%E5%BA%93/leveldb_manifest_block_item.png" alt=""></p><p>Manifest Block 的详细结构如上图所示。</p><p><img src="/images/%E6%95%B0%E6%8D%AE%E5%BA%93/leveldb_data_restore.png" alt=""></p><p>上图最上面的流程显示了恢复元信息的过程，也就是一次应用 VersionEdit 的过程，这个过程会有大量的临时 Version 产生，但这种方法显然太过于耗费资源，LevelDB 引入 VersionSet::Builder 来避免这种中间变量，方法是先将所有的VersoinEdit内容整理到VersionBuilder中，然后一次应用产生最终的Version，详细流程如上图下边流程所示。</p><p>数据恢复的详细流程如下：</p><ul><li>依次读取Manifest文件中的每一个Block， 将从文件中读出的Record反序列化为VersionEdit；</li><li>将每一个的VersionEdit Apply到VersionSet::Builder中，之后从VersionSet::Builder的信息中生成Version；</li><li>计算compaction_level_、compaction_score_；</li><li>将新生成的Version挂到VersionSet中，并初始化VersionSet的manifest_file_number_， next_file_number_，last_sequence_，log_number_，prev_log_number_ 信息；</li></ul><h4 id="6-7-Snapshot"><a href="#6-7-Snapshot" class="headerlink" title="6.7 Snapshot"></a>6.7 Snapshot</h4><hr><p>RocksDB 每次进行更新操作就会把更新内容写入 Manifest 文件，同时它会更新版本号。</p><p>版本号是一个 8 字节的证书，每个 key 更新时，除了新数据被写入数据文件，同时记录下 RocksDB 的版本号。RocksDB 的 Snapshot 数据仅仅是逻辑数据，并没有对应的真实存在的物理数据，仅仅对应一下当前 RocksDB 的全局版本号而已，只要 Snapshot 存在，每个 key 对应版本号的数据在后面的更新、删除、合并时会一并存在，不会被删除，以保证数据一致性。</p><h5 id="6-7-1-Checkpoints"><a href="#6-7-1-Checkpoints" class="headerlink" title="6.7.1  Checkpoints"></a>6.7.1  <a href="https://github.com/facebook/rocksdb/wiki/Checkpoints" target="_blank" rel="noopener">Checkpoints</a></h5><hr><p>Checkpoints 是 RocksDB 提供的一种 snapshot，独立的存在一个单独的不同于 RocksDB 自身数据目录的目录中，既可以 ReadOnly 模式打开，也可以 Read-Write 模式打开。Checkpoints 被用于全量或者增量 Backup 机制中。</p><p>如果 Checkpoints 目录和 RocksDB 数据目录在同一个文件系统上，则 Checkpoints 目录下的 SST 是一个 hard link【SST 文件是 Read-Only的】，而 manifest 和 CURRENT 两个文件则会被拷贝出来。如果 DB 有多个 Column Family，wal 文件也会被复制，其时间范围足以覆盖 Checkpoints 的起始和结束，以保证数据一致性。</p><p>如果以 Read-Write 模式打开 Checkpoints 文件，则其中过时的 SST 文件会被删除掉。</p><h4 id="6-8-Backup"><a href="#6-8-Backup" class="headerlink" title="6.8 Backup"></a>6.8 Backup</h4><hr><p>RocksDB 提供了 point-of-time 数据备份功能，可以调用 <code>BackupEngine::CreateNewBackup(db, flush_before_backup = false)</code> 接口进行数据备份， 其大致流程如下：</p><ul><li><p>禁止删除文件（sst 文件和 log 文件）；</p></li><li><p>调用 <code>GetLiveFiles()</code> 获取当前的有效文件，如 table files, current, options and manifest file;</p></li><li><p>将 RocksDB 中的所有的 sst/Manifest/配置/CURRENT 等有效文件备份到指定目录；</p><p>  GetLiveFiles() 接口返回的 SST 文件如果已经被备份过，则这个文件不会被重新复制到目标备份目录，但是 <code>BackupEngine</code> 会对这个文件进行 checksum 校验，如果校验失败则会中止备份过程。</p></li><li><p>如果 <code>flush_before_backup</code> 为 false，则<code>BackupEngine</code> 会调用 <code>GetSortedWalFiles()</code> 接口把当前有效的 wal 文件也拷贝到备份目录；</p></li><li><p>重新允许删除文件。</p></li></ul><p>sst 文件只有在 compact 时才会被删除，所以禁止删除就相当于禁止了 compaction。别的 RocksDB 在获取这些备份数据文件后会依据 Manifest 文件重构 LSM 结构的同时，也能恢复出 WAL 文件，进而重构出当时的 memtable 文件。</p><p>在进行 Backup 的过程中，写操作是不会被阻塞的，所以 WAL 文件内容会在 backup 过程中发生改变。RocksDB 的 flush_before_backup 选项用来控制在 backup 时是否也拷贝 WAL，其值为 true 则不拷贝。</p><h5 id="6-8-1-Backup-编程接口"><a href="#6-8-1-Backup-编程接口" class="headerlink" title="6.8.1 Backup 编程接口"></a>6.8.1 Backup 编程接口</h5><hr><p>RocksDB 提供的 Backup 接口使用方法详见 <a href="https://github.com/facebook/rocksdb/wiki/How-to-backup-RocksDB%3F" target="_blank" rel="noopener">参考文档17</a>。include/rocksdb/utilities/backupable_db.h 主要提供了 <code>BackupEngine</code> 和 <code>BackupEngineReadOnly</code>，分别用于备份数据和恢复数据。</p><p><code>BackupEngine</code>备份数据是增量式备份【设置选项 <code>BackupableDBOptions::share_table_files</code> 】，调用 <code>BackupEngine::CreateNewBackup()</code> 接口进行备份后，可以调用接口 <code>BackupEngine::GetBackupInfo()</code>获取备份文件的信息：ID、timestamp、size、file number 和 metadata【用户自定义数据】。</p><p><img src="/images/%E6%95%B0%E6%8D%AE%E5%BA%93/rocksdb_backup_dir_list.png" alt=""></p><p>备份 DB 目录见上图，各个备份文件的 size 是其 private 目录下数据与 shared 目录下数据之和，shared 下面存储的数据是各个备份公共的数据，所以所有备份文件的 size 之和可能大于实际占用的磁盘空间大小。meta 目录下各个文件的格式详见 utilities/backupable/backupable_db.cc，上图中 <code>meta/1</code>内容如下：</p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1536821592   # checksum</span><br><span class="line">1            # backup ID</span><br><span class="line">4            # private file number</span><br><span class="line">private/1/MANIFEST-000008 crc32 272357318</span><br><span class="line">private/1/OPTIONS-000011 crc32 3039312718</span><br><span class="line">private/1/CURRENT crc32 1581506767</span><br><span class="line">private/1/000009.log crc32 3494278128</span><br></pre></td></tr></table></figure><p>Private 目录则包含一些非 SST 文件：options, current, manifest, WALs。如果 <code>Options::share_table_files</code> 为false，则 private 目录会存储 SST 文件。如果 <code>Options::share_table_files</code> 为 true 且 <code>Options::share_files_with_checksum</code> 为 false，shared 目录包含一些 SST 文件，SST 文件命名与原 RocksDB 目录下命名一致，所以在一个备份目录下只能备份一个 RocksDB 实例的数据。</p><p>接口 <code>BackupEngine::VerifyBackups()</code> 用于对备份数据进行校验，但是仅仅根据 meta 目录下各个 ID 文件记录的文件 size 与 相应的 private 目录下的文件的 size 是否相等，并不会进行 checksum 校验， 校验 checksum 需要读取数据文件，比较费时。另外需要注意的是，这个接口相应的 <code>BackupEngine</code> 句柄只能由<code>BackupEngine::CreateNewBackup()</code> 创建，也即只能在进行文件备份且句柄未失效前进行数据校验，因为校验时所依据的数据是在备份过程中产生的。</p><p>接口 <code>BackupEngineReadOnly::RestoreDBFromBackup(backup_id, db_dir, wal_dir,restore_options)</code> 用于备份数据恢复，参数 <code>db_dir</code> 和<code>wal_dir</code>大部分场景下都是同一个目录，但在 <a href="https://www.jianshu.com/p/46bb78bca726?utm_source=oschina-app" target="_blank" rel="noopener">参考文档18</a> 所提供的把 RocksDB 当做纯内存数据库的使用场景下， <code>db_dir</code> 在内存虚拟文件系统上，而 <code>wal_dir</code> 则是一个磁盘文件目录。进行数据恢复时，这个接口还会根据 meta 下相应 ID 记录的 备份数据 checksum 对 private 目录下的数据进行校验，发错错误时返回 <code>Status::Corruption</code> 错误。</p><h5 id="6-8-2-Backup-性能优化"><a href="#6-8-2-Backup-性能优化" class="headerlink" title="6.8.2 Backup 性能优化"></a>6.8.2 Backup 性能优化</h5><hr><p><code>BackupEngine::Open()</code> 启用时需要进行一些初始化工作，所以它会消耗一些时间。例如需要把本地 RocksDB 数据备份到远端的 HDFS 上，这个过程就可能消耗多个 network round-trip，所以在实际使用中不要频繁创建 <code>BackupEngine</code> 对象。</p><p>加快 BackupEngine 对象的方式之一是通过调用 <code>PurgeOldBackups(N)</code>来删除非必要的备份文件，接口 <code>PurgeOldBackups(N)</code> 本身之意就是只保留最近的 N 个备份，多余的会被删除掉。也可以通过调用 <code>DeleteBackup(id)</code> 接口根据备份 ID 删除某个确定的备份。</p><p>初始化 BackupEngine 对象过后，备份的速度就取决于本地与远端的媒介运行速度了。例如，如果本地媒介是 HDD，在其自身饱和运转之后就算是打开再多的线程也无济于事。如果媒介是一个小的 HDFS 集群，其表现也不会很好。如果本地是 SSD 而远端是一个大的 HDFS 集群，则相较于单线程， 16 个备份线程会被备份时间缩短 2/3。</p><h5 id="6-8-3-高级编程接口"><a href="#6-8-3-高级编程接口" class="headerlink" title="6.8.3 高级编程接口"></a>6.8.3 高级编程接口</h5><hr><ul><li><code>BackupEngine::CreateNewBackupWithMetadata()</code> 用于设置 metadata，例如设置你能辨识的备份 ID，metadata 可以通过 <code>BackupEngine::GetBackupInfo()</code> 获取；</li><li><code>rocksdb::LoadLatestOptions()</code> or <code>rocksdb:: LoadOptionsFromFile()</code> RocksDB 现在也能对 RocksDB 的 options 进行备份，可以通过这两个接口获取相应备份的 Options；</li><li><code>BackupableDBOptions::backup_env</code> 用于设置备份目录的 ENV；</li><li><code>BackupableDBOptions::backup_dir</code> 用于设置备份文件的根目录；</li><li><code>BackupableDBOptions::share_table_files</code> 如果这个选项为 true，则 <code>BackupEngine</code> 会进行增量备份，把所有的 SST 文件存储到 “shared/“ 子目录，其危险是 SST 文件名字可能相同【在多个 RocksDB 对象共用同一备份目录的场景下】；</li><li><code>BackupableDBOptions::share_files_with_checksum</code> 在多个 RocksDB 对象共用同一备份目录的场景下，SST 文件名字可能相同，把这个选项设置为 true 可以处理这个冲突，SST 文件会被 <code>BackupEngine</code> 通过 checksum/size/seqnum 三个参数进行校验；</li><li><code>BackupableDBOptions::max_background_operations</code> 这个参数用于设置备份和恢复数据的线程数，在使用分布式文件系统如 HDFS 场景下，这个参数会大大提高备份和恢复的效率；</li><li><code>BackupableDBOptions::info_log</code> 用于设置 LOG 对象，可以在备份和恢复数据时进行日志输出；</li><li><code>BackupableDBOptions::sync</code> 如果设置为 true，<code>BackupEngine</code> 会调用 <code>fsync</code> 系统接口进行文件数据和 metadata 的数据写入，以防备系统重启或者崩溃时的数据不一致现象，大部分情况下如果为追求性能，这个参数可以设置为 false；</li><li><code>BackupableDBOptions::destroy_old_data</code> 如果这个选项为 true，新的 <code>BackupEngine</code> 被创建出来之后备份目录下旧的备份数据会被清空；</li><li><code>BackupEngine::CreateNewBackup(db, flush_before_backup = false)</code> flush_before_backup 被设置为 true 时，<code>BackupEngine</code> 首先 flush memtable，然后再进行数据复制，而 WAL log 文件不会被复制，因为 flush 时候它会被删掉，如果这个为 false 则相应的 WAL 日志文件也会被复制以保证备份数据与当前 RocksDB 状态一致；</li></ul><h3 id="7-FAQ"><a href="#7-FAQ" class="headerlink" title="7 FAQ"></a>7 FAQ</h3><hr><p>官方 wiki 【参考文档 11】提供了一份 FAQ，下面节选一些比较有意义的建议，其他内容请移步官方文档。</p><ul><li>1 如果机器崩溃后重启，则 RocksDB 能够恢复的数据是同步写【WriteOptions.sync=true】调用 <code>DB::SyncWAL()</code> 之前的数据 或者已经被写入 L0 的 memtable 的数据都是安全的；</li><li>2 可以通过 <code>GetIntProperty(cf_handle, “rocksdb.estimate-num-keys&quot;)</code> 获取一个 column family 中大概的 key 的个数；</li><li>3 可以通过 <code>GetAggregatedIntProperty(“rocksdb.estimate-num-keys&quot;, &amp;num_keys)</code> 获取整个 RocksDB 中大概的 key 的总数，之所以只能获取一个大概数值是因为 RocksDB 的磁盘文件有重复 key，而且 compact 的时候会进行 key 的淘汰，所以无法精确获取；</li><li>4 Put()/Write()/Get()/NewIterator() 这几个 API 都是线程安全的；</li><li>5 多个进程可以同时打开同一个 RocksDB 文件，但是其中只能有一个写进程，其他的都只能通过 <code>DB::OpenForReadOnly()</code> 对 RocksDB 进行只读访问；</li><li>6 当进程中还有线程在对 RocksDB 进行 读、写或者手工 compaction 的时候，不能强行关闭它；</li><li>7 RocksDB 本身不建议使用大 key，但是它支持的 key 的最大长度是 8MB，value 的最大长度是 3GB；</li><li>8 RocksDB 最佳实践：一个线程进行写，且以顺序方式写；以 batch 方式把数据写入 RocksDB；使用 vector memtable；确保 <code>options.max_background_flushes</code> 最少为 4；插入数据之前设置关闭自动 compact，把 <code>options.level0_file_num_compaction_trigger/options.level0_slowdown_writes_trigger/options.level0_stop_writes_trigger</code> 三个值放大，数据插入后再启动调用 compact 函数进行 compaction 操作。<br>  如果调用了<code>Options::PrepareForBulkLoad()</code>，后面三个方法会被自动启用；</li><li>9 关闭 RocksDB 对象时，如果是通过 DestroyDB() 去关闭时，这个 RocksDB 还正被另一个进程访问，则会造成不可预料的后果；</li><li>10 可以通过 <code>DBOptions::db_paths/DBOptions::db_log_dir/DBOptions::wal_dir</code> 三个参数分别存储 RocksDB 的数据，这种情况下如果要释放 RocksDB 的数据可以通过 DestroyDB() 这个 API 去执行删除任务；</li><li>11 当 <code>BackupOptions::backup_log_files</code> 或者 <code>flush_before_backup</code> 的值为 true 的时候，如果程序调用 <code>CreateNewBackup()</code> 则 RocksDB 会创建 <code>point-in-time snapshot</code>，RocksDB进行数据备份的时候不会影响正常的读写逻辑；</li><li>12 RocksDB 启动之后不能修改 <code>prefix extractor</code>；</li><li>13 SstFileWriter API 可以用来创建 SST 文件，如果这些 SST 文件被添加到别的 RocksDB 数据库发生 key range 重叠，则会导致数据错乱；</li><li>14 编译 RocksDB 的 gcc 版本 最低是 4.7，推荐 4.8 以上；</li><li>15 单个文件系统如 ext3 或者 xfs 可以使用多个磁盘，然后让 RocksDB 在这个文件系统上运行进而使用多个磁盘；</li><li>16 使用多磁盘时，RAID 的 stripe size 不能小于 64kb，推荐使用1MB；</li><li>17 RocksDB 可以针对每个 SST 文件通过 <code>ColumnFamilyOptions::bottommost_compression</code> 使用不同的压缩的方法；</li><li>18 当多个 Handle 指向同一个 Column Family 时，其中一个线程通过 DropColumnFamily() 删除一个 CF 的时候，其引用计数会减一，直至为 0 时整个 CF 会被删除；</li><li>19 RocksDB 接受一个写请求的时候，可能因为 compact 会导致 RocksDB 多次读取数据文件进行数据合并操作；</li><li>20 RocksDB 不直接支持数据的复制，但是提供了 API GetUpdatesSince() 供用户调用以获取某个时间点以后更新的 kv；</li><li>21 Options 的 block_size 是指 block 的内存空间大小，与数据是否压缩无关；</li><li>22 options.prefix_extractor 一旦启用，就无法继续使用 Prev() 和 SeekToLast() 两个 API，可以把 ReadOptions.total_order_seek 设置为 true，以禁用 <code>prefix iterating</code>；</li><li>23 当 BlockBaseTableOptions::cache_index_and_filter_blocks 的值为 true 时，在进行 Get() 调用的时候相应数据的 bloom filter 和 index block 会被放进 LRU cache 中，如果这个参数为 false 则只有 memtable 的 index 和 bloom filter 会被放进内存中；</li><li>24 当调用 Put() 或者 Write() 时参数 WriteOptions.sync 的值为 true，则本次写以前的所有 WriteOptions.disableWAL 为 false 的写的内容都会被固化到磁盘上；</li><li>25 禁用 WAL 时，DB::Flush() 只对单个 Column Family 的数据固化操作是原子的，对多个 Column Family 的数据操作则不是原子的，官方考虑将来会支持这个 feature；</li><li>26 当使用自定义的 comparators 或者 merge operators 时，ldb 工具就无法读取 sst 文件数据；</li><li>27 RocksDB 执行前台的 Get() 和 Write() 任务遇到错误时，它会返回 rocksdb::IOError 具体值；</li><li>28 RocksDB 执行后台任务遇到错误时 且 options.paranoid_checks 值为 true，则 RocksDB 会自动进入只读模式；</li><li>29 RocksDB 一个线程执行 compact 的时候，这个任务是不可取消的；</li><li>30 RocksDB 一个线程执行 compact 任务的时候，可以在另一个线程调用 CancelAllBackgroundWork(db, true) 以中断正在执行的 compact 任务；</li><li>31 当多个进程打开一个 RocksDB 时，如果指定的 compact 方式不一样，则后面的进程会打开失败；</li><li>32 Column Family 使用场景：(1) 不同的 Column Family 可以使用不同的 setting/comparators/compression types/merge operators/compaction filters；(2) 对数据进行逻辑隔离，方便分别删除；(3) 一个 Column Family 存储 metadata，另一个存储 data；</li><li>33 使用一个 RocksDB 就是使用一个物理存储系统，使用一个 Column Family 则是使用一个逻辑存储系统，二者主要区别体现在 数据备份、原子写以及写性能表现上。DB 是数据备份和复制以及 checkpoint 的基本单位，但是 Column Family 则利用 BatchWrite，因为这个操作是可以跨 Column Family 的，而且多个 Column Family 可以共享同一个 WAL，多个 DB 则无法做到这一点；</li><li>34 RocksDB 不支持多列；</li><li>35 RocksDB 的读并不是无锁的，有如下情况：(1) 访问 sharded block cache (2) 如果 table cache options.max_open_files 不等于 -1 (3) 如果 flush 或者 compaction 刚刚完成，RocksDB 此时会使用全局 mutex lock 以获取 LSM 树的最新 metadata (4) RocksDB 使用的内存分配器如 jemalloc 有时也会加锁，这四种情况总体很少发生，总体可以认为读是无锁的；</li><li>36 如果想高效的对所有数据进行 iterate，则可以创建 snapshot 然后再遍历；</li><li>37 如果一个 key space range (minkey, maxkey) 很大，则使用 Column Family 对其进行 sharding，如果这个 range 不大则不要单独使用一个 Column Family；</li><li>38 RocksDB 没有进行 compaction 的时候，可以通过 <code>rocksdb.estimate-live-data-size</code> 可以估算 RocksDB 使用的磁盘空间；</li><li>39 Snapshot 仅仅存在于逻辑概念上，其对应的实际物理文件可能正被 compaction 线程执行写任务；Checkpoint 则是实际物理文件的一个镜像，或者说是一个硬链接，而出处于同样的 Env 下【都是 RocksDB 数据文件格式】；而 backup 虽然也是物理数据的镜像，但是与原始数据处于不同的 Env 下【如 backup 可能在 HDFS 上】；</li><li>40 推荐使用压缩算法 LZ4，Snappy 次之，压缩之后如果为了更好的压缩效果可以使用 Zlib；</li><li>41 即使没有被标记为删除的 key，也没有数据过期，RocksDB 仍然会执行 compaction，以提高读性能；</li><li>42 RocksDB 的 key 和 value 是存在一起的，遍历一个 key 的时候，RocksDB 已经把其 value 读入 cache 中；</li><li>43 对于一个离线 DB 可以通过 “sst_dump –show_properties –command=none” 命令获取特定 sst 文件的 index &amp; filter 的 size，对于正在运行的 DB 可以通过读取 DB 的属性 “kAggregatedTableProperties” 或者调用 DB::GetPropertiesOfAllTables() 获取 DB 的 index &amp; filter 的 size。</li></ul><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><hr><ul><li>1 <a href="https://github.com/facebook/rocksdb/wiki/RocksDB-Tuning-Guide" target="_blank" rel="noopener">RocksDB Tuning Guide</a></li><li>2 <a href="https://github.com/facebook/rocksdb/wiki/Rocksdb-BlockBasedTable-Format" target="_blank" rel="noopener">Rocksdb BlockBasedTable Format</a></li><li>3 <a href="https://github.com/facebook/rocksdb/wiki/PlainTable-Format" target="_blank" rel="noopener">PlainTable Format</a></li><li>4 <a href="https://github.com/facebook/rocksdb/wiki/Thread-Pool" target="_blank" rel="noopener">Thread Pool</a></li><li>5 <a href="https://github.com/facebook/rocksdb/wiki/RocksDB-Basics" target="_blank" rel="noopener">RocksDB Basics</a></li><li>6 <a href="https://github.com/facebook/rocksdb/wiki/Transactions" target="_blank" rel="noopener">Transactions</a></li><li>7 <a href="https://github.com/facebook/rocksdb/wiki/RocksDB-Bloom-Filter" target="_blank" rel="noopener">Bloom Filter</a></li><li>8 <a href="https://github.com/facebook/rocksdb/wiki/Universal-Compaction" target="_blank" rel="noopener">Universal Compaction</a></li><li>9 <a href="https://github.com/facebook/rocksdb/wiki/Leveled-Compaction" target="_blank" rel="noopener">Leveled Compaction</a></li><li>10 <a href="https://github.com/facebook/rocksdb/wiki/Time-to-Live" target="_blank" rel="noopener">Time to Live</a></li><li>11 <a href="https://github.com/facebook/rocksdb/wiki/RocksDB-FAQ" target="_blank" rel="noopener">RocksDB-FAQ</a></li><li>12 <a href="https://note.youdao.com/share/?id=60b7e3aa14a01c85d05ee8a7e4d16c46&type=note#/" target="_blank" rel="noopener">设计思路和主要知识点</a></li><li>13 <a href="https://yq.aliyun.com/articles/594728?spm=a2c4e.11157919.spm-cont-list.17.302c27aeDR3OyC" target="_blank" rel="noopener">RocksDB · MANIFEST文件介绍</a></li><li>14 <a href="https://www.jianshu.com/p/e13338a3f161" target="_blank" rel="noopener">RocksDB. Merge Operator</a></li><li>15 <a href="https://zhuanlan.zhihu.com/p/30807728" target="_blank" rel="noopener">使用PinnableSlice减少Get时的内存拷贝</a></li><li>16 <a href="https://rocksdb.org/blog/2017/08/24/pinnableslice.html" target="_blank" rel="noopener">PinnableSlice; less memcpy with point lookups</a></li><li>17 <a href="https://github.com/facebook/rocksdb/wiki/How-to-backup-RocksDB%3F" target="_blank" rel="noopener">How to backup RocksDB?</a></li><li>18 <a href="https://www.jianshu.com/p/46bb78bca726?utm_source=oschina-app" target="_blank" rel="noopener">RocksDB系列十三:How to persist in memory RocksDB database?</a></li><li>19 <a href="https://github.com/facebook/rocksdb/wiki/Checkpoints" target="_blank" rel="noopener">Checkpoints</a></li><li>20 <a href="https://www.tuicool.com/articles/7ju2UfI" target="_blank" rel="noopener">LSM-Tree与RocksDB</a></li><li>21 <a href="https://www.jianshu.com/p/0fdeed70b36a" target="_blank" rel="noopener">自动调优 RocksDB</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;前沿&quot;&gt;&lt;a href=&quot;#前沿&quot; class=&quot;headerlink&quot; title=&quot;前沿&quot;&gt;&lt;/a&gt;前沿&lt;/h1&gt;&lt;p&gt;近日工作中使用了 RocksDB。RocksDB 的优点此处无需多说，它的一个 feature 是其有很多优化选项用于对 RocksDB 进行调优。欲熟悉这些参数，必须对其背后的原理有所了解，本文主要整理一些 RocksDB 的 wiki 文档，以备自己参考之用。&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据结构" scheme="http://blog.crazylaw.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
      <category term="数据库" scheme="http://blog.crazylaw.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库开发知识" scheme="http://blog.crazylaw.cn/tags/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%BC%80%E5%8F%91%E7%9F%A5%E8%AF%86/"/>
    
  </entry>
  
  <entry>
    <title>【Golang】- 使用golang重写tga服务</title>
    <link href="http://blog.crazylaw.cn/2020/12/17/Golang/%E4%BD%BF%E7%94%A8golang%E9%87%8D%E5%86%99tga%E6%9C%8D%E5%8A%A1/"/>
    <id>http://blog.crazylaw.cn/2020/12/17/Golang/%E4%BD%BF%E7%94%A8golang%E9%87%8D%E5%86%99tga%E6%9C%8D%E5%8A%A1/</id>
    <published>2020-12-17T01:46:51.000Z</published>
    <updated>2021-03-20T16:25:01.799Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>早期，在我们的部门中后端的技术栈语言主要有三种语言，分别是<code>php/python/erlang</code>，其中用于做服务的是 <code>php/erlang</code>。</p><p>在我们的体系中，日志采集服务体系目前都是用erlang写的，而php写的服务多是基于swoole写的一些基础服务。</p><p>在tga服务中，我们需要从<code>kafka</code> -&gt; <code>服务</code> -&gt; <code>本地文件</code>的模式。</p><p>业务据流图如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+---------------------------------------------------------------+</span><br><span class="line">|                                                               |</span><br><span class="line">|                                                               |</span><br><span class="line">|       +---------------------------------------------+         |</span><br><span class="line">|       |                                             |         |</span><br><span class="line">|       |                                             |         |</span><br><span class="line">|       |                  kafka服务                   |         |</span><br><span class="line">|       |                                             |         |</span><br><span class="line">|       |                                             |         |</span><br><span class="line">|       |                                             |         |</span><br><span class="line">|       +----------------------+----------------------+         |</span><br><span class="line">|                              |                                |</span><br><span class="line">|                              |                                |</span><br><span class="line">|       +----------------------v-----------------------+        |                     +----------------------------+</span><br><span class="line">|       |                                              |        |                     |                            |</span><br><span class="line">|       |                                              |        |                     |                            |</span><br><span class="line">|       |                                              |        |                     |                            |</span><br><span class="line">|       |               mthinkingdata服务               |        &lt;---------------------+          logbus服务        |</span><br><span class="line">|       |                                              |        |                     |                            |</span><br><span class="line">|       |                                              |        |                     |                            |</span><br><span class="line">|       |                                              |        |                     |                            |</span><br><span class="line">|       +----------------------+-----------------------+        |                     +----------------------------+</span><br><span class="line">|                              |                                |</span><br><span class="line">|                              |                                |</span><br><span class="line">|       +----------------------v------------------------+       |</span><br><span class="line">|       |                                               |       |</span><br><span class="line">|       |                                               |       |</span><br><span class="line">|       |                                               |       |</span><br><span class="line">|       |                   本地文件                     +-------+</span><br><span class="line">|       |                                               |       |</span><br><span class="line">|       |                                               |       |</span><br><span class="line">|       |                                               |       |</span><br><span class="line">|       +-----------------------------------------------+       |</span><br><span class="line">|                                                               |</span><br><span class="line">+---------------------------------------------------------------+</span><br></pre></td></tr></table></figure><p>于是我们试探性的自研基于swoole的kafka客户端，我们自己实现根据kafka协议的封包，解包，流程处理。(<code>swoole-kafka</code>)<br><code>mthinkingdata服务</code>就是我们基于<code>kafka-swoole</code>研发的业务服务。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">+------------+</span><br><span class="line">|            |</span><br><span class="line">|  message1  +------------+</span><br><span class="line">|            |            |</span><br><span class="line">+------------+            |</span><br><span class="line">                          |</span><br><span class="line">+------------+     +------+--------+</span><br><span class="line">|            |     |               |</span><br><span class="line">|  message2  +-----+   snappy压缩   |</span><br><span class="line">|            |     |               |</span><br><span class="line">+------------+     +------+--------+</span><br><span class="line">                           |</span><br><span class="line">+---------------+          |</span><br><span class="line">|               |          |</span><br><span class="line">| message[3..n] +----------+</span><br><span class="line">|               |</span><br><span class="line">+---------------+</span><br></pre></td></tr></table></figure><p>在这个过程中，我们发现php对于cpu密集型的处理存在瓶颈，因为我们在生产者一方如果发送多条协议的情况下，会经过 <code>snappy</code> 算法的压缩再推送以便减少network-io（增加cpu-io）。<br>消费者在接受消息的时候也是被压缩过的数据，所以我们需要解压，在这个解压的过程中，是个十分消费cpu的过程，即使我们当时是基于swoole4.3的协程版本来处理，<br>不行的是的抢占式协程当时并没有很好的完成，我们没办法达到快速的接受多个数据包的行为。消费速度也并不是特别理想。<br>在这个大环境下，我们还需要借助redis来作为中间的存储。而redis是单线程的，我们在这个过程中，试过使用<code>pipeline</code>等手段减少tcp中的响应包的带来的性能损耗。但是由于redis只能利用单核的缘故，批量处理一批指令后，最高的cpu利用率接近100%下无法再增长。<br>也因此，我们的服务注定无法达到很好的性能测试。</p><p>我们得出结论，当时服务的瓶颈在于：</p><ul><li><ol><li>php语言本身的性能</li></ol></li><li><ol start="2"><li>swoole协程不支持抢占式调度</li></ol></li><li><ol start="3"><li>未实现动态伸缩扩展worker数量（感兴趣可以去看看kafka-swoole的架构分享）</li></ol></li><li><ol start="4"><li>redis未能利用多核，cpu利用率达到峰值</li></ol></li></ul><a id="more"></a><h2 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h2><p>针对以上几个问题，我们逐一得出解决方案</p><ul><li><ol><li>使用golang语言（语言优先在公司内部博客有比较）</li></ol></li><li><ol start="2"><li>使用基于golang的嵌入式存储服务作为中间存储服务（<code>badger</code>）（<code>rocksdb</code>的使用在尝试中）</li></ol></li></ul><p>也因此催生出了使用golang重写tga服务（<code>mtga</code>）的需求。以其中一个项目开发为例子(项目编号：<code>24</code>)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">├── README.md</span><br><span class="line">├── _build                                 # 部署的目录结构</span><br><span class="line">│   ├── README.md</span><br><span class="line">│   ├── bin                                 </span><br><span class="line">│   │   └── mtga                           # 编译后的二进制文件</span><br><span class="line">│   ├── config</span><br><span class="line">│   │   ├── common.env</span><br><span class="line">│   │   ├── msource.yml</span><br><span class="line">│   │   ├── mtga.local.yml</span><br><span class="line">│   │   └── mtga.yml</span><br><span class="line">│   ├── mctl                               # 操作入口脚本</span><br><span class="line">│   ├── scripts</span><br><span class="line">│   │   └── init.sh                        # 第一次部署项目需要执行的初始化脚本</span><br><span class="line">│   ├── settings</span><br><span class="line">│   │   ├── mthinkingdata:filter.24.json</span><br><span class="line">│   │   └── mthinkingdata:metadata.24.json</span><br><span class="line">│   └── tmp</span><br><span class="line">├── _local_build                           # 本地开发部署的目录结构</span><br><span class="line">│   ├── README.md</span><br><span class="line">│   ├── bin</span><br><span class="line">│   │   └── mtga</span><br><span class="line">│   ├── config</span><br><span class="line">│   │   ├── common.env</span><br><span class="line">│   │   ├── msource.yml</span><br><span class="line">│   │   ├── mtga.local.yml</span><br><span class="line">│   │   └── mtga.yml</span><br><span class="line">│   ├── mctl</span><br><span class="line">│   ├── scripts</span><br><span class="line">│   │   └── init.sh</span><br><span class="line">│   ├── settings</span><br><span class="line">│   │   ├── mthinkingdata:filter.24.json</span><br><span class="line">│   │   └── mthinkingdata:metadata.24.json</span><br><span class="line">│   └── tmp</span><br><span class="line">├── build</span><br><span class="line">│   ├── Dockerfile</span><br><span class="line">│   ├── Jenkinsfile</span><br><span class="line">│   └── README.md</span><br><span class="line">└── src</span><br><span class="line">    ├── Makefile                         # 便于构建服务</span><br><span class="line">    ├── app                              # 业务代码</span><br><span class="line">    │   ├── bussiness.go                 # 消费者的业务核心代码</span><br><span class="line">    │   ├── config</span><br><span class="line">    │   │   ├── app.go</span><br><span class="line">    │   │   ├── redis_keys.go</span><br><span class="line">    │   │   ├── setting_tools.go</span><br><span class="line">    │   │   └── settings.go</span><br><span class="line">    │   ├── main.go</span><br><span class="line">    │   └── reporter</span><br><span class="line">    │       └── notify.go</span><br><span class="line">    ├── cmd                              # cli终端命令</span><br><span class="line">    │   ├── clear_failure_queue.go       # 清理失败队列</span><br><span class="line">    │   ├── failure_queue_count.go       # 失败队列数量</span><br><span class="line">    │   ├── kafka_lag.go                 # 当前消费者阻塞总数据量</span><br><span class="line">    │   ├── offset_checker.go            # topic中各个partition的当前offset以及阻塞情况</span><br><span class="line">    │   ├── restart.go                   # 重启服务</span><br><span class="line">    │   ├── root.go</span><br><span class="line">    │   ├── start.go                     # 启动服务</span><br><span class="line">    │   ├── start_not_daemon.go          # 以非守护进程的方式启动服务</span><br><span class="line">    │   └── stop.go                      # 暂停服务</span><br><span class="line">    ├── config                           # 配置目录</span><br><span class="line">    │   ├── msource.yml</span><br><span class="line">    │   ├── mtga.local.yml</span><br><span class="line">    │   └── mtga.yml</span><br><span class="line">    ├── go.mod</span><br><span class="line">    ├── go.sum</span><br><span class="line">    ├── main.go</span><br><span class="line">    ├── settings</span><br><span class="line">    │   ├── mthinkingdata:filter.24.json</span><br><span class="line">    │   └── mthinkingdata:metadata.24.json</span><br><span class="line">    └── test</span><br><span class="line">        ├── setting_test.go</span><br><span class="line">        └── setting_tools_test.go</span><br></pre></td></tr></table></figure><blockquote><p>早期我们还没抛弃redis的时候，持续占用cpu100%的话就会出现这个。 如果不用pipe模式的话，tps测试只有2500-3500之间。<br>抛弃redis使用了badger之后， 结合业务逻辑，tps:1w/s左右</p></blockquote><p>其中用到内部的组件包含如下：</p><ul><li>commentjson</li><li>go-graceful-daemon</li><li>logbdev</li><li>metl-sdk</li><li>msink</li><li>msource</li></ul><h2 id="mtga-amp-amp-msource"><a href="#mtga-amp-amp-msource" class="headerlink" title="mtga &amp;&amp; msource"></a>mtga &amp;&amp; msource</h2><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 转换前</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="attr">"headers"</span>:&#123;</span><br><span class="line">        <span class="attr">"app_id"</span>:<span class="number">24</span>,</span><br><span class="line">        <span class="attr">"log_name"</span>:<span class="string">"log_item"</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"logs"</span>:&#123;</span><br><span class="line">        <span class="attr">"account_name"</span>:<span class="string">"4866014107"</span>,</span><br><span class="line">        <span class="attr">"action"</span>:<span class="number">2146</span>,</span><br><span class="line">        <span class="attr">"agent_id"</span>:<span class="number">36</span>,</span><br><span class="line">        <span class="attr">"amount"</span>:<span class="number">1</span>,</span><br><span class="line">        <span class="attr">"bag_amount"</span>:<span class="number">0</span>,</span><br><span class="line">        <span class="attr">"bag_type"</span>:<span class="number">0</span>,</span><br><span class="line">        <span class="attr">"bind_type"</span>:<span class="number">1</span>,</span><br><span class="line">        <span class="attr">"client_version"</span>:<span class="string">""</span>,</span><br><span class="line">        <span class="attr">"device_id"</span>:<span class="string">""</span>,</span><br><span class="line">        <span class="attr">"end_time"</span>:<span class="number">0</span>,</span><br><span class="line">        <span class="attr">"idfa"</span>:<span class="string">""</span>,</span><br><span class="line">        <span class="attr">"imei"</span>:<span class="string">""</span>,</span><br><span class="line">        <span class="attr">"is_internal"</span>:<span class="number">0</span>,</span><br><span class="line">        <span class="attr">"item_id"</span>:<span class="number">31103003</span>,</span><br><span class="line">        <span class="attr">"mac"</span>:<span class="string">""</span>,</span><br><span class="line">        <span class="attr">"mtime"</span>:<span class="number">1608184243</span>,</span><br><span class="line">        <span class="attr">"pid"</span>:<span class="number">1608184244000008</span>,</span><br><span class="line">        <span class="attr">"platform"</span>:<span class="number">3100</span>,</span><br><span class="line">        <span class="attr">"quality"</span>:<span class="number">0</span>,</span><br><span class="line">        <span class="attr">"regrow"</span>:<span class="number">14</span>,</span><br><span class="line">        <span class="attr">"role_id"</span>:<span class="number">6001310009300</span>,</span><br><span class="line">        <span class="attr">"role_level"</span>:<span class="number">800</span>,</span><br><span class="line">        <span class="attr">"server_id"</span>:<span class="number">5001</span>,</span><br><span class="line">        <span class="attr">"server_version"</span>:<span class="string">""</span>,</span><br><span class="line">        <span class="attr">"sn"</span>:<span class="string">"205914E03BF640CB76"</span>,</span><br><span class="line">        <span class="attr">"star"</span>:<span class="number">0</span>,</span><br><span class="line">        <span class="attr">"start_time"</span>:<span class="number">1608184243</span>,</span><br><span class="line">        <span class="attr">"upf"</span>:<span class="number">3100</span>,</span><br><span class="line">        <span class="attr">"via"</span>:<span class="string">"36|3100"</span>,</span><br><span class="line">        <span class="attr">"zero_dateline"</span>:<span class="number">1608134400</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 转换后</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="attr">"#account_id"</span>:<span class="string">"10289100005300x"</span>,</span><br><span class="line">    <span class="attr">"#distinct_id"</span>:<span class="string">""</span>,</span><br><span class="line">    <span class="attr">"#event_name"</span>:<span class="string">"t_log_item"</span>,</span><br><span class="line">    <span class="attr">"#ip"</span>:<span class="string">""</span>,</span><br><span class="line">    <span class="attr">"#time"</span>:<span class="string">"2020-12-17 13:52:39"</span>,</span><br><span class="line">    <span class="attr">"#type"</span>:<span class="string">"track"</span>,</span><br><span class="line">    <span class="attr">"properties"</span>:&#123;</span><br><span class="line">        <span class="attr">"account_name"</span>:<span class="string">"1608153237001005108"</span>,</span><br><span class="line">        <span class="attr">"action"</span>:<span class="number">1005</span>,</span><br><span class="line">        <span class="attr">"agent_id"</span>:<span class="number">11</span>,</span><br><span class="line">        <span class="attr">"amount"</span>:<span class="number">-1</span>,</span><br><span class="line">        <span class="attr">"bag_amount"</span>:<span class="number">1</span>,</span><br><span class="line">        <span class="attr">"bag_type"</span>:<span class="number">0</span>,</span><br><span class="line">        <span class="attr">"bind_type"</span>:<span class="number">1</span>,</span><br><span class="line">        <span class="attr">"client_version"</span>:<span class="string">""</span>,</span><br><span class="line">        <span class="attr">"device_id"</span>:<span class="string">""</span>,</span><br><span class="line">        <span class="attr">"end_time"</span>:<span class="number">0</span>,</span><br><span class="line">        <span class="attr">"idfa"</span>:<span class="string">""</span>,</span><br><span class="line">        <span class="attr">"imei"</span>:<span class="string">""</span>,</span><br><span class="line">        <span class="attr">"is_internal"</span>:<span class="number">0</span>,</span><br><span class="line">        <span class="attr">"item_id"</span>:<span class="number">10203101</span>,</span><br><span class="line">        <span class="attr">"mac"</span>:<span class="string">""</span>,</span><br><span class="line">        <span class="attr">"mtime"</span>:<span class="number">1608184359</span>,</span><br><span class="line">        <span class="attr">"pid"</span>:<span class="number">1608184359000031</span>,</span><br><span class="line">        <span class="attr">"platform"</span>:<span class="number">101</span>,</span><br><span class="line">        <span class="attr">"quality"</span>:<span class="number">0</span>,</span><br><span class="line">        <span class="attr">"regrow"</span>:<span class="number">1</span>,</span><br><span class="line">        <span class="attr">"role_id"</span>:<span class="number">110289100005300</span>,</span><br><span class="line">        <span class="attr">"role_level"</span>:<span class="number">160</span>,</span><br><span class="line">        <span class="attr">"server_id"</span>:<span class="number">289</span>,</span><br><span class="line">        <span class="attr">"server_version"</span>:<span class="string">""</span>,</span><br><span class="line">        <span class="attr">"sn"</span>:<span class="string">""</span>,</span><br><span class="line">        <span class="attr">"star"</span>:<span class="number">0</span>,</span><br><span class="line">        <span class="attr">"start_time"</span>:<span class="number">1608184359</span>,</span><br><span class="line">        <span class="attr">"upf"</span>:<span class="number">101</span>,</span><br><span class="line">        <span class="attr">"via"</span>:<span class="string">"11|101"</span>,</span><br><span class="line">        <span class="attr">"zero_dateline"</span>:<span class="number">1608134400</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>数据在<code>msource</code>到<code>mtga</code>的交互</p><p>改版前：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">                       +-----------------------------------------+</span><br><span class="line">                       | +---------+                             |</span><br><span class="line">                       | | msource |                             |</span><br><span class="line">                       | +---------+                             |</span><br><span class="line">                       |                                         |</span><br><span class="line">                       |    +--------------------------------+   |</span><br><span class="line">                       |    |                                |   |</span><br><span class="line">                       |    |       spoutKafka(channel)      |   |</span><br><span class="line">                       |    |                                +-&gt;X+-------------------------+</span><br><span class="line">                       |    |                                |   |                         |</span><br><span class="line">                       |    +-------------^----------^-------+   |           +-------------v----------------+</span><br><span class="line">                       |                  |          |           |           |-----------+                  |</span><br><span class="line">                       |                  |          |           |           || 业务服务 |                   |</span><br><span class="line">                       |                  |          |           |           +-----------+                  |</span><br><span class="line">+------------+         |                  |          |           |           |                              |</span><br><span class="line">|            |         |    +--------------------------------+   |           |      XXXXXXXXXXXXXXX         |</span><br><span class="line">|   kafka    +---------&gt;X+  | +-------+   |          |       |   |           |      X1.过滤的数据 X          |</span><br><span class="line">|            |         | |  | | badge |   |          |       |   |           |      XXXXXXXXXXXXXXX         |</span><br><span class="line">+------------+         | |  | +-------+   |          |       |   |           |                              |</span><br><span class="line">                       | |  |             |          |       |   |           |                              |</span><br><span class="line">                       | |  |      +------+-------+  |       |   |           |      +---------------+       |</span><br><span class="line">                       | ^---------&gt;              |  |       | ^--------------------+2.失败的数据     |       |</span><br><span class="line">                       |    |      |  等待ack队列   &lt;-----^   | | |           |      +---------------+       |</span><br><span class="line">                       |    |      |              |  |  |    | | |           |                              |</span><br><span class="line">                       |    |      +--------------+  |  |    | | |           |                              |</span><br><span class="line">                       |    |                        |  |    | | |           |                              |</span><br><span class="line">                       |    |      +--------------+  |  |    | | |           |     +------------------+     |</span><br><span class="line">                       |    |      |              +--^  &lt;--------------------------+3.正常处理完的数据   |     |</span><br><span class="line">                       |    |      |  业务失败队列  |          | | |           |     +------------------+     |</span><br><span class="line">                       |    |      |              &lt;------------v |           |                              |</span><br><span class="line">                       |    |      +--------------+          |   |           +------------------------------+</span><br><span class="line">                       |    +--------------------------------+   |</span><br><span class="line">                       +-----------------------------------------+</span><br></pre></td></tr></table></figure><p>改版后：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">                       +-----------------------------------------+</span><br><span class="line">                       | +---------+                             |           +-----------------------+</span><br><span class="line">                       | | msource |                             |           |                       |</span><br><span class="line">                       | +---------+                             |           |  特定条件下提交offse    &lt;----------+</span><br><span class="line">                       |                                         |           |                       |          |</span><br><span class="line">                       |    +--------------------------------+   |           +-----------------------+          |</span><br><span class="line">                       |    |                                |   |                                              |</span><br><span class="line">                       | +--&gt;       spoutKafka(channel)      |   |                                              |</span><br><span class="line">                       | |  |        （有缓冲）                +-&gt;X+-------------------------+                    |</span><br><span class="line">                       | |  |                                |   |                         |                    |</span><br><span class="line">                       | |  +------------------------^-------+   |           +-------------v----------------+   |</span><br><span class="line">                       | |                           |           |           |-----------+                  |   |</span><br><span class="line">                       | |                           |           |           || 业务服务  |                  |   |</span><br><span class="line">                       | |                           |           |           +-----------+                  |   |</span><br><span class="line">+------------+         | |                           |           |           |                              |   |</span><br><span class="line">|            |         | |  +--------------------------------+   |           |      XXXXXXXXXXXXXXX         |   |</span><br><span class="line">|   kafka    +---------&gt;X+  | +-------+              |       |   |           |      X1.过滤的数据   X         |   |</span><br><span class="line">|            |         |    | | badge |              |       |   |           |      XXXXXXXXXXXXXXX         |   |</span><br><span class="line">+------------+         |    | +-------+     ^--------&gt;       |   |           |                              |   |</span><br><span class="line">                       |    |               |                |   |           |                              |   |</span><br><span class="line">                       |    |       +-------+------+         |   |           |      +---------------+       |   |</span><br><span class="line">                       |    |       |              &lt;-----------+X+------------------+2.失败的数据     |       |   |</span><br><span class="line">                       |    |       |  业务失败队列  |         |   |           |      +---------------+       |   |</span><br><span class="line">                       |    |       |              |         |   |           |                              |   |</span><br><span class="line">                       |    |       +--------------+         |   |           |                              |   |</span><br><span class="line">                       |    |                                |   |           |                              |   |</span><br><span class="line">                       |    |                                |   |           |     +------------------+     |   |</span><br><span class="line">                       |    +--------------------------------+   |           |     |3.正常处理完的数据   +---------&gt;</span><br><span class="line">                       |                                         |           |     +------------------+     |</span><br><span class="line">                       |                                         |           |                              |</span><br><span class="line">                       |                                         |           +------------------------------+</span><br><span class="line">                       |                                         |</span><br><span class="line">                       +-----------------------------------------+</span><br></pre></td></tr></table></figure><ul><li>每10s记录一次本地offset</li><li>接收到信号量（<code>SIGINT/SIGTERM</code>）到时候也提交一次</li></ul><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 接收到信号量到时候也提交一次</span></span><br><span class="line"><span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">  ch := <span class="built_in">make</span>(<span class="keyword">chan</span> os.Signal, <span class="number">1</span>)</span><br><span class="line">  signal.Notify(ch, syscall.SIGINT, syscall.SIGTERM)</span><br><span class="line">  <span class="keyword">for</span> _ = <span class="keyword">range</span> ch &#123;</span><br><span class="line">    t.Stop()</span><br><span class="line">    logic.ackf.RLock()</span><br><span class="line">    <span class="keyword">for</span> _, partitionMessage := <span class="keyword">range</span> logic.acks &#123;</span><br><span class="line">      <span class="keyword">for</span> _,msg := <span class="keyword">range</span> partitionMessage &#123;</span><br><span class="line">        logic.Sk.Ack(msg)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    logic.ackf.RUnlock()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 每10s记录一次本地offset</span></span><br><span class="line">t := time.NewTicker(<span class="number">10</span> * time.Second)</span><br><span class="line"><span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">  <span class="keyword">for</span> &#123;</span><br><span class="line">    <span class="keyword">select</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> &lt;-t.C:</span><br><span class="line">      logic.ackf.RLock()</span><br><span class="line">      <span class="keyword">for</span> _, partitionMessage := <span class="keyword">range</span> logic.acks &#123;</span><br><span class="line">        <span class="keyword">for</span> _,msg := <span class="keyword">range</span> partitionMessage &#123;</span><br><span class="line">          logic.Sk.Ack(msg)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      logic.ackf.RUnlock()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;()</span><br></pre></td></tr></table></figure><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 服务worker启动核心逻辑</span></span><br><span class="line">wg := sync.WaitGroup&#123;&#125;</span><br><span class="line">logic := <span class="built_in">new</span>(CoreLogic)</span><br><span class="line">logic.Sk = sk</span><br><span class="line">logic.acks = <span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">map</span>[<span class="keyword">int32</span>]*kafka.Message&#123;&#125;</span><br><span class="line">logic.ackf = sync.RWMutex&#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i := <span class="number">0</span>; i &lt; config.AppConfig.Worker; &#123;</span><br><span class="line">wg.Add(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="keyword">defer</span> wg.Done()</span><br><span class="line">logic.Consume()</span><br><span class="line">&#125;()</span><br><span class="line">i++</span><br><span class="line">&#125;</span><br><span class="line">wg.Wait()</span><br></pre></td></tr></table></figure><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// bussiness.go 的核心代码</span></span><br><span class="line"><span class="keyword">type</span> CoreLogic <span class="keyword">struct</span> &#123;</span><br><span class="line">Sk *msource.SpoutKafka</span><br><span class="line"></span><br><span class="line">ackf sync.RWMutex</span><br><span class="line">acks <span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">map</span>[<span class="keyword">int32</span>]*kafka.Message</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(business *CoreLogic)</span> <span class="title">Consume</span><span class="params">()</span></span> &#123;</span><br><span class="line">  ...</span><br><span class="line">  <span class="comment">// 从正常队列来的数据</span></span><br><span class="line"><span class="keyword">if</span> msg.TopicPartition.Topic != <span class="literal">nil</span> &#123;</span><br><span class="line">business.ackf.Lock()</span><br><span class="line"><span class="keyword">if</span> _, ok := business.acks[*msg.TopicPartition.Topic]; !ok &#123;</span><br><span class="line">business.acks[*msg.TopicPartition.Topic] = <span class="keyword">map</span>[<span class="keyword">int32</span>]*kafka.Message&#123;&#125;</span><br><span class="line">&#125;</span><br><span class="line">business.acks[*msg.TopicPartition.Topic][msg.TopicPartition.Partition] = msg</span><br><span class="line">business.ackf.Unlock()</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="comment">// 从失败队列来的数据，独立写入，独立Ack</span></span><br><span class="line">err = p.Write(fileName, <span class="keyword">string</span>(sinkBuffer))</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">logbdev.Error(err)</span><br><span class="line">&#125;</span><br><span class="line">business.Sk.Ack(msg)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>由于我们的消费者需要从<code>msource</code>中把消息拉出来，所以设置了一个<code>Corelogic</code>的结构体，其中包含了 <code>sk</code>,<code>acks</code>,<code>ackf</code>三个属性。</p><blockquote><p>由于我们需要异步的提交offset，所以需要设置 ackf 的锁来确保数据的完整性</p></blockquote><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 读取消息的方式</span></span><br><span class="line"><span class="keyword">for</span> msg := <span class="keyword">range</span> business.Sk.MessageChan() &#123;</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>golang的格式化时间戳很奇葩，需要以<code>&quot;2006-01-02 15:04:05&quot;</code>为格式进行格式化。</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// yyyy-MM-dd hh:mm:ss</span></span><br><span class="line"><span class="comment">// yyyy-MM-dd H:i:s</span></span><br><span class="line"></span><br><span class="line">jsonArray[<span class="string">"#time"</span>] = time.Unix(time.Now().Unix(), <span class="number">0</span>).Format(<span class="string">"2006-01-02 15:04:05"</span>)</span><br></pre></td></tr></table></figure><h2 id="msource"><a href="#msource" class="headerlink" title="msource"></a>msource</h2><p><code>msource</code>是<code>kafka</code>和<code>本地存储</code>的桥梁(通信服务)，间接得做着服务可靠性的保证。（数据不丢，不重，方便查看堵塞情况）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;mctl</span><br><span class="line"></span><br><span class="line">Usage:</span><br><span class="line">  mtga [command]</span><br><span class="line"></span><br><span class="line">Available Commands:</span><br><span class="line">  clear_failure_queue 清空失败队列</span><br><span class="line">  failure_queue_count 失败队列数量</span><br><span class="line">  kafka_lag           消费阻塞</span><br><span class="line">  offset_checker      offset的情况</span><br></pre></td></tr></table></figure><p>以下命令都是msource提供出来的api，再由业务服务封装成命令</p><p>例如：offset_checker</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">.---.----------------.-----------.------------.------------.------------.-----.</span><br><span class="line">| # |     Topic      | Partition |    Low     |    High    |  Current   | Lag |</span><br><span class="line">+---+----------------+-----------+------------+------------+------------+-----+</span><br><span class="line">| 0 | mulog_clean_24 | 0         | 6171720626 | 6197511755 | 6197511494 | 261 |</span><br><span class="line">| 1 | mulog_clean_24 | 1         | 6169152656 | 6197196879 | 6197196555 | 324 |</span><br><span class="line">| 2 | mulog_clean_24 | 2         | 6169656725 | 6195509715 | 6195509483 | 232 |</span><br><span class="line">| 3 | mulog_clean_24 | 3         | 6172416843 | 6197496752 | 6197496518 | 234 |</span><br><span class="line">| 4 | mulog_clean_24 | 4         | 6170706518 | 6197423974 | 6197423659 | 315 |</span><br><span class="line">| 5 | mulog_clean_24 | 5         | 6168091223 | 6196757252 | 6196756991 | 261 |</span><br><span class="line">&#39;---&#39;----------------&#39;-----------&#39;------------&#39;------------&#39;------------&#39;-----&#39;</span><br></pre></td></tr></table></figure><p>例如：kafka_lag/failure_queue_count</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1234</span><br></pre></td></tr></table></figure><p>msource如何做到由业务系统控制指定offset中消费呢？</p><p>我们还是通过了<code>badger</code>来存储各个topic-partition的offset。服务在启动的时候会取各个partition的offset，如果存在的话，就设置需要从对应的offset开始读取数据。如果不存在对应的offset的话，那么就根据你的策略从<code>最早</code>,<code>最近</code>开始选择拉取数据。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">+-------------------+</span><br><span class="line">|---------+         |</span><br><span class="line">|| badger |         |</span><br><span class="line">+---------+         +---------------+</span><br><span class="line">|                   |               |</span><br><span class="line">|      offset存储   |               |</span><br><span class="line">|                   |               |</span><br><span class="line">+-------------------+     +---------v-------------+</span><br><span class="line">                          |                       |      +------------+</span><br><span class="line">                          |  从partition拉取数据    +-----+   。。。。。。|</span><br><span class="line">                          |                       |      +------------+</span><br><span class="line">                          +---------+-------------+</span><br><span class="line">                                    |</span><br><span class="line">                                    |</span><br><span class="line">+-------------------+               |</span><br><span class="line">|                   |               |</span><br><span class="line">|      Kafka        &lt;---------------+</span><br><span class="line">|                   |</span><br><span class="line">+-------------------+</span><br></pre></td></tr></table></figure><p>msource的工作原理在介绍mtga的时候基本也差不多介绍完了。至于这些api的实现是基于<code>Unix Socket</code>实现的。</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(sk *SpoutKafka)</span> <span class="title">unixSocketListen</span><span class="params">()</span></span> &#123;</span><br><span class="line">start:</span><br><span class="line">lis, err := net.Listen(<span class="string">"unix"</span>, UNIX_SOCKET_FILE)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">logbdev.Info(<span class="string">"UNIX Domain Socket 创建失败，正在尝试重新创建 -&gt; "</span>, err)</span><br><span class="line">err = os.Remove(UNIX_SOCKET_FILE)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">logbdev.Info(<span class="string">"删除 sock 文件失败！程序退出 -&gt; "</span>, err)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">goto</span> start</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">logbdev.Info(<span class="string">"创建 UNIX Domain Socket 成功"</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">sigs := <span class="built_in">make</span>(<span class="keyword">chan</span> os.Signal, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">signal.Notify(sigs, syscall.SIGINT, syscall.SIGTERM)</span><br><span class="line"></span><br><span class="line"><span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">&lt;-sigs</span><br><span class="line"><span class="keyword">if</span> sk.SigTermCbNeed &#123;</span><br><span class="line">sk.SigTermCb(<span class="literal">true</span>)</span><br><span class="line">&#125;</span><br><span class="line">&#125;()</span><br><span class="line"></span><br><span class="line"><span class="keyword">defer</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">lis.Close()</span><br><span class="line">os.Remove(UNIX_SOCKET_FILE)</span><br><span class="line">&#125;()</span><br><span class="line"></span><br><span class="line">invokeObjectMethod := <span class="function"><span class="keyword">func</span><span class="params">(conn net.Conn, object <span class="keyword">interface</span>&#123;&#125;, methodName <span class="keyword">string</span>, args ...<span class="keyword">interface</span>&#123;&#125;)</span></span> &#123;</span><br><span class="line">inputs := <span class="built_in">make</span>([]reflect.Value, <span class="built_in">len</span>(args))</span><br><span class="line"><span class="keyword">for</span> i, _ := <span class="keyword">range</span> args &#123;</span><br><span class="line">inputs[i] = reflect.ValueOf(args[i])</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">intCb := <span class="function"><span class="keyword">func</span><span class="params">(v reflect.Value)</span></span> &#123;</span><br><span class="line">n, err := conn.Write([]<span class="keyword">byte</span>(fmt.Sprintf(<span class="string">"%d\n"</span>, v.Int())))</span><br><span class="line"><span class="keyword">if</span> n &gt; <span class="number">0</span> &#123;</span><br><span class="line">logbdev.Info(fmt.Sprintf(<span class="string">"Cmd: %s ,成功响应结果: %d"</span>, methodName, v.Int()))</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">logbdev.Error(fmt.Sprintf(<span class="string">"Cmd: %s ,响应失败 %s"</span>, methodName, err))</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">strCb := <span class="function"><span class="keyword">func</span><span class="params">(v reflect.Value)</span></span> &#123;</span><br><span class="line">n, err := conn.Write([]<span class="keyword">byte</span>(fmt.Sprintf(<span class="string">"%s\n"</span>, v.String())))</span><br><span class="line"><span class="keyword">if</span> n &gt; <span class="number">0</span> &#123;</span><br><span class="line">logbdev.Info(fmt.Sprintf(<span class="string">"Cmd: %s ,成功响应结果: %s"</span>, methodName, v.String()))</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">logbdev.Error(fmt.Sprintf(<span class="string">"Cmd: %s ,响应失败 %s"</span>, methodName, err))</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _, v := <span class="keyword">range</span> reflect.ValueOf(object).MethodByName(methodName).Call(inputs) &#123;</span><br><span class="line"><span class="keyword">switch</span> v.Kind() &#123;</span><br><span class="line"><span class="keyword">case</span> reflect.Int:</span><br><span class="line"><span class="keyword">case</span> reflect.Int64:</span><br><span class="line">intCb(v)</span><br><span class="line"><span class="keyword">break</span></span><br><span class="line"><span class="keyword">case</span> reflect.String:</span><br><span class="line">strCb(v)</span><br><span class="line"><span class="keyword">break</span></span><br><span class="line"><span class="keyword">default</span>:</span><br><span class="line">_, err := conn.Write([]<span class="keyword">byte</span>(fmt.Sprintf(<span class="string">"%s\n"</span>, <span class="string">"不支持的响应数据类型"</span>)))</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">handleError(err.Error())</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">handle := <span class="function"><span class="keyword">func</span><span class="params">(conn net.Conn)</span></span> &#123;</span><br><span class="line"><span class="keyword">defer</span> conn.Close()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> &#123;</span><br><span class="line"><span class="keyword">var</span> buf = <span class="built_in">make</span>([]<span class="keyword">byte</span>, <span class="number">1024</span>)</span><br><span class="line">n, err := conn.Read(buf)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 如果已经没数据了，则结束</span></span><br><span class="line"><span class="keyword">if</span> err == io.EOF &#123;</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">logbdev.Info(<span class="string">"Socket conn read error:"</span>, err)</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> cmd Cmd</span><br><span class="line"><span class="keyword">if</span> n &gt; <span class="number">0</span> &#123;</span><br><span class="line">err := json.Unmarshal(buf[:n], &amp;cmd)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">_, err := conn.Write([]<span class="keyword">byte</span>(fmt.Sprintf(<span class="string">"Rpc-Json解析失败, err: %s"</span>, err) + <span class="string">"\n"</span>))</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">logbdev.Warn(err)</span><br><span class="line"><span class="keyword">continue</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">invokeObjectMethod(conn, sk, cmd.RpcFuncName, cmd.Params...)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> &#123;</span><br><span class="line">conn, err := lis.Accept()</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">logbdev.Info(<span class="string">"请求接收错误 -&gt; "</span>, err)</span><br><span class="line"><span class="keyword">continue</span> <span class="comment">// 一个连接错误，不会影响整体的稳定性，忽略就好</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">go</span> handle(conn) <span class="comment">//开始处理数据</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>之前在mtga中出现过一个问题，那就是rpc超时问题的问题（socket超时）。这个问题导致了我们在执行各种命令的时候，如果出现问题会一直<code>hang up</code>的状态，得不到结果的同时一直卡住。影响到了我们对服务的监控。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">+----------------------------------------------------------+</span><br><span class="line">|   +-------------+                                        |</span><br><span class="line">|   |   msource   |                                        |</span><br><span class="line">|   +-------------+                                        |</span><br><span class="line">|                                                          |</span><br><span class="line">|   +---------------------+     +---------------------+    |</span><br><span class="line">|   |                     |     |                     |    |</span><br><span class="line">|   | unix socket server  |     | unix socket client  |    |</span><br><span class="line">|   |                     |     |                     |    |</span><br><span class="line">|   +---------------------+     +---------------------+    |</span><br><span class="line">+----------------------------------------------------------+</span><br></pre></td></tr></table></figure><p>为了解决这个问题。我们在<code>unix socket client</code> 这里加了一个超时的机制。借助是<code>context</code>的机制，实现协程之间的超时通信。</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> (</span><br><span class="line">TimeoutMsg = <span class="string">"操作超时, 请检查服务状态!"</span></span><br><span class="line">TimeoutInt = <span class="number">-1</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">rcn := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">bool</span>)</span><br><span class="line">ctx, cancel := context.WithTimeout(context.Background(), time.Second*<span class="number">10</span>)</span><br><span class="line"><span class="keyword">defer</span> cancel()</span><br><span class="line"></span><br><span class="line"><span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">  <span class="keyword">if</span> exists(UNIX_SOCKET_FILE) &#123;</span><br><span class="line">    conn, err := net.Dial(<span class="string">"unix"</span>, UNIX_SOCKET_FILE)</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">      handleError(err.Error())</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">defer</span> conn.Close()</span><br><span class="line"></span><br><span class="line">    rpc := Cmd&#123;RpcFuncName: <span class="string">"FailureQueueCount"</span>&#125;</span><br><span class="line">    data, err := json.Marshal(rpc)</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">      handleError(fmt.Sprintf(<span class="string">"encode-json失败"</span>))</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    n, err := conn.Write(data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> n &gt; <span class="number">0</span> &amp;&amp; err == <span class="literal">nil</span> &#123;</span><br><span class="line">      reader := bufio.NewReader(conn)</span><br><span class="line">      msg, err := reader.ReadString(<span class="string">'\n'</span>)</span><br><span class="line">      <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        logbdev.Info(err)</span><br><span class="line">      &#125;</span><br><span class="line">      msg = msg[:<span class="built_in">len</span>(msg)-<span class="built_in">len</span>(<span class="string">"\n"</span>)]</span><br><span class="line">      lagInt, err := strconv.Atoi(msg)</span><br><span class="line">      <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        handleError(err.Error())</span><br><span class="line">      &#125;</span><br><span class="line">      lag = <span class="keyword">int64</span>(lagInt)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    sk := SourceToolRun(configOpts)</span><br><span class="line">    lag = sk.FailureQueueCount()</span><br><span class="line">  &#125;</span><br><span class="line">  rcn &lt;- <span class="literal">true</span></span><br><span class="line">&#125;()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> &#123;</span><br><span class="line">  <span class="keyword">select</span> &#123;</span><br><span class="line">  <span class="keyword">case</span> &lt;-ctx.Done():</span><br><span class="line">    <span class="keyword">if</span> ctx.Err() != <span class="literal">nil</span> &#123;</span><br><span class="line">      logbdev.Warn(ctx.Err())</span><br><span class="line">      <span class="keyword">return</span> TimeoutInt</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">return</span> lag</span><br><span class="line">    &#125;</span><br><span class="line">  <span class="keyword">case</span> &lt;- rcn:</span><br><span class="line">    <span class="keyword">return</span> lag</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里需要注意的是，这个<code>context.WithTimeout</code> 是不管你内部是否处理完毕，一定会在指定的时间内<code>timeout</code>，所以如果提前完成了必须通过自己的手段提前结束。</p><h2 id="msink"><a href="#msink" class="headerlink" title="msink"></a>msink</h2><p>这个组件的作用主要是用于把指定的消息进行sink到各种<code>终端</code>，例如<code>msink_file</code>,<code>msink_kafka</code>等等。</p><p>目前我们是封装在同一个仓库中，以不同文件保存，后续，我们或许会考虑拆分开来便于维护发版。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">├── Dockerfile</span><br><span class="line">├── README.MD</span><br><span class="line">├── config.go</span><br><span class="line">├── config.yml</span><br><span class="line">├── config_test.go</span><br><span class="line">├── error.go</span><br><span class="line">├── go.mod</span><br><span class="line">├── go.sum</span><br><span class="line">├── msource_spout.go</span><br><span class="line">├── msource_spout_test.go</span><br><span class="line">└── rpc.go</span><br></pre></td></tr></table></figure><p>这里主要和大家说以下<code>msink_file</code>吧，<code>msink_kafka</code>的实现原理差不多。</p><ul><li>支持自定义回调函数，判断是否写入成功</li><li>支持批处理和流式处理</li></ul><p>流式处理有独立的Api</p><ul><li><code>func (f *FileClient) WriteWithoutEol(filePath string, message string) error</code></li><li><code>func (f *FileClient) Write(filePath string, message string) error</code></li><li><code>(f *FileClient) BatchLineDataChannel() chan&lt;- map[Filename]ChannelMessage</code></li></ul><p>批处理的Api</p><ul><li><code>(f *FileClient) BatchLineDataChannel() chan&lt;- map[Filename]ChannelMessage</code></li></ul><p>可能细心的朋友已经发现了，流式处理的Api包含了批处理的Api。<br>是的，被包含在一起了，为什么会这样子？</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">NewFileClient</span><span class="params">(client afero.Fs, opts ...DialOption)</span> *<span class="title">FileClient</span></span> &#123;</span><br><span class="line">p := <span class="built_in">new</span>(FileClient)</span><br><span class="line">p.client = client</span><br><span class="line">p.dopts = defaultOptions()</span><br><span class="line">p.batchLineDataChannel = <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">map</span>[Filename]ChannelMessage, <span class="number">1000</span>)</span><br><span class="line">p.events = <span class="built_in">make</span>(<span class="keyword">chan</span> FileMetaMessage, <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//循环调用opts</span></span><br><span class="line"><span class="keyword">for</span> _, opt := <span class="keyword">range</span> opts &#123;</span><br><span class="line">opt.apply(&amp;p.dopts)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> writer <span class="function"><span class="keyword">func</span><span class="params">(fileClient *FileClient)</span></span></span><br><span class="line"><span class="keyword">if</span> p.dopts.batchWrite &#123;</span><br><span class="line">writer = channelBatchWriter</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">writer = channelWriter</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">p.wg.Add(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">writer(p)</span><br><span class="line">p.wg.Done()</span><br><span class="line">&#125;()</span><br><span class="line"></span><br><span class="line">p.wg.Add(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">p.run()</span><br><span class="line">p.wg.Done()</span><br><span class="line">&#125;()</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> p</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 流式</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">channelWriter</span><span class="params">(client *FileClient)</span></span> &#123;</span><br><span class="line"><span class="keyword">for</span> m := <span class="keyword">range</span> client.batchLineDataChannel &#123;</span><br><span class="line"><span class="keyword">for</span> fn, msg := <span class="keyword">range</span> m &#123;</span><br><span class="line">err := client.WriteWithoutEol(<span class="keyword">string</span>(fn), <span class="keyword">string</span>(msg))</span><br><span class="line">client.events &lt;- FileMetaMessage&#123;Message: <span class="keyword">string</span>(msg), Error: err&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 批量</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">channelBatchWriter</span><span class="params">(client *FileClient)</span></span> &#123;</span><br><span class="line"><span class="keyword">var</span> buffered = <span class="built_in">make</span>(<span class="keyword">map</span>[Filename][]ChannelMessage)</span><br><span class="line">bufferedCnt := <span class="number">0</span></span><br><span class="line">batchSize := client.dopts.batchLineDataChannelCount</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> m := <span class="keyword">range</span> client.batchLineDataChannel &#123;</span><br><span class="line"><span class="keyword">for</span> fn, msg := <span class="keyword">range</span> m &#123;</span><br><span class="line">buffered[fn] = <span class="built_in">append</span>(buffered[fn], msg)</span><br><span class="line">bufferedCnt++</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">loop:</span><br><span class="line"><span class="keyword">for</span> <span class="literal">true</span> &#123;</span><br><span class="line"><span class="keyword">select</span> &#123;</span><br><span class="line"><span class="keyword">case</span> m, ok := &lt;-client.batchLineDataChannel:</span><br><span class="line"><span class="keyword">if</span> !ok &#123;</span><br><span class="line"><span class="keyword">break</span> loop</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> m == <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="built_in">panic</span>(<span class="string">"nil message received on batchLineDataChannel"</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> fn, msg := <span class="keyword">range</span> m &#123;</span><br><span class="line">buffered[fn] = <span class="built_in">append</span>(buffered[fn], msg)</span><br><span class="line">bufferedCnt++</span><br><span class="line"><span class="keyword">if</span> bufferedCnt &gt; batchSize &#123;</span><br><span class="line"><span class="keyword">break</span> loop</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">default</span>:</span><br><span class="line"><span class="keyword">break</span> loop</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> tmpLongMsg = <span class="built_in">make</span>(<span class="keyword">map</span>[Filename]<span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">for</span> Filename, ChannelMessageList := <span class="keyword">range</span> buffered &#123;</span><br><span class="line">tmpMsg := <span class="string">""</span></span><br><span class="line"><span class="keyword">for</span> _, cm := <span class="keyword">range</span> ChannelMessageList &#123;</span><br><span class="line">tmpMsg += <span class="keyword">string</span>(cm)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">tmpLongMsg[Filename] = tmpMsg</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> Filename, longMsg := <span class="keyword">range</span> tmpLongMsg &#123;</span><br><span class="line">err := client.WriteWithoutEol(<span class="keyword">string</span>(Filename), longMsg)</span><br><span class="line"><span class="keyword">for</span> _, ChannelMessageList := <span class="keyword">range</span> buffered &#123;</span><br><span class="line"><span class="keyword">for</span> _, cm := <span class="keyword">range</span> ChannelMessageList &#123;</span><br><span class="line">client.events &lt;- FileMetaMessage&#123;Message: <span class="keyword">string</span>(cm), Error: err&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">buffered = <span class="built_in">make</span>(<span class="keyword">map</span>[Filename][]ChannelMessage)</span><br><span class="line">bufferedCnt = <span class="number">0</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>看到这里大家应该也知道Api应该就可以理解为什么会被包含在一起了。</p><h2 id="logdev"><a href="#logdev" class="headerlink" title="logdev"></a>logdev</h2><p>这是我们的日志组件，在上面的代码中，多多少少已经有了这个组件的身影。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">├── README.md</span><br><span class="line">├── exported.go</span><br><span class="line">├── go.mod</span><br><span class="line">├── go.sum</span><br><span class="line">├── hooks</span><br><span class="line">│   ├── reporter</span><br><span class="line">│   │   ├── metl.go</span><br><span class="line">│   │   ├── reporter.go</span><br><span class="line">│   │   └── reporter_test.go</span><br><span class="line">│   └── stacker</span><br><span class="line">│       ├── stacker.go</span><br><span class="line">│       └── stacker_test.go</span><br><span class="line">├── logbdev.go</span><br><span class="line">├── logbdev_test.go</span><br><span class="line">├── logger.go</span><br><span class="line">├── mc_formatter.go</span><br></pre></td></tr></table></figure><p>常规设置，主要是我们在这个组件中添加了几个<code>hook</code>，分别是<code>reporter</code>,<code>stacker</code></p><ul><li><p>reporter （设置需要上报日志的级别，用于如果发现了<code>error</code>级别的错误就推送日志到我们的<code>告警服务</code>中。）</p></li><li><p>stacker （设置需要输出堆栈的级别日志级别）</p></li></ul><p>日志存储的方式有几种</p><ul><li><ol><li>常规的单日志存储</li></ol></li><li><ol start="2"><li>日志按照大小轮转</li></ol></li><li><ol start="3"><li>日志按照日期轮转</li></ol></li></ul><h2 id="commentjson"><a href="#commentjson" class="headerlink" title="commentjson"></a>commentjson</h2><p>这个是用来解析包含了<code>换行符</code>,<code>空白行</code>,<code>行注释</code>,<code>段注释</code>等等的<code>json</code>字符串</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">├── README.md</span><br><span class="line">├── hjson.go</span><br><span class="line">├── hjson_test.go</span><br><span class="line">└── test.json</span><br></pre></td></tr></table></figure><p>这个原理也比较简单，就是一个个字符去匹配，重新构造出一个新的合法的json格式。里面的单元测试也做得比较完善了。</p><h2 id="go-graceful-daemon"><a href="#go-graceful-daemon" class="headerlink" title="go-graceful-daemon"></a>go-graceful-daemon</h2><p>这个组件是用来将服务变成守护进程用的。</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">├── README.md</span><br><span class="line">├── daemon.<span class="keyword">go</span></span><br><span class="line">├── <span class="keyword">go</span>.mod</span><br><span class="line">├── <span class="keyword">go</span>.mod2</span><br><span class="line">├── signal.<span class="keyword">go</span></span><br><span class="line">└── test</span><br><span class="line">    └── test.<span class="keyword">go</span></span><br></pre></td></tr></table></figure><p>这里的核心主要是借助了<code>syscall</code>的<code>ForkExec</code>实现。</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">forkDaemon</span><span class="params">()</span> <span class="title">error</span></span> &#123;</span><br><span class="line">args := os.Args</span><br><span class="line">os.Setenv(<span class="string">"__Daemon"</span>, <span class="string">"true"</span>)</span><br><span class="line">procAttr := &amp;syscall.ProcAttr&#123;</span><br><span class="line">Env:   os.Environ(),</span><br><span class="line">&#125;</span><br><span class="line">pid, err := syscall.ForkExec(os.Args[<span class="number">0</span>], args, procAttr)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line">&#125;</span><br><span class="line">log.Printf(<span class="string">"[%d] %s start daemon\n"</span>, pid, AppName)</span><br><span class="line">savePid(pid)</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>并且支持自定实现信号量的捕捉处理逻辑</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> ErrStop = errors.New(<span class="string">"stop serve signals"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> SignalHandlerFunc <span class="function"><span class="keyword">func</span><span class="params">(sig os.Signal)</span> <span class="params">(err error)</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">SetSigHandler</span><span class="params">(handler SignalHandlerFunc, signals ...os.Signal)</span></span> &#123;</span><br><span class="line"><span class="keyword">for</span> _, sig := <span class="keyword">range</span> signals &#123;</span><br><span class="line">handlers[sig] = handler</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ServeSignals calls handlers for system signals.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">ServeSignals</span><span class="params">()</span> <span class="params">(err error)</span></span> &#123;</span><br><span class="line">signals := <span class="built_in">make</span>([]os.Signal, <span class="number">0</span>, <span class="built_in">len</span>(handlers))</span><br><span class="line"><span class="keyword">for</span> sig := <span class="keyword">range</span> handlers &#123;</span><br><span class="line">signals = <span class="built_in">append</span>(signals, sig)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">ch := <span class="built_in">make</span>(<span class="keyword">chan</span> os.Signal, <span class="number">8</span>)</span><br><span class="line">signal.Notify(ch, signals...)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> sig := <span class="keyword">range</span> ch &#123;</span><br><span class="line">err = handlers[sig](sig)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">break</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">signal.Stop(ch)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> err == ErrStop &#123;</span><br><span class="line">err = <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> handlers = <span class="built_in">make</span>(<span class="keyword">map</span>[os.Signal]SignalHandlerFunc)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">init</span><span class="params">()</span></span> &#123;</span><br><span class="line">handlers[syscall.SIGINT] = sigtermDefaultHandler</span><br><span class="line">handlers[syscall.SIGTERM] = sigtermDefaultHandler</span><br><span class="line">handlers[syscall.SIGHUP] = sighupDefaultHandler</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">sigtermDefaultHandler</span><span class="params">(sig os.Signal)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">log.Printf(<span class="string">"[%d] %s stop graceful"</span>, os.Getpid(), AppName)</span><br><span class="line">log.Printf(<span class="string">"[%d] %s stopped."</span>, os.Getpid(), AppName)</span><br><span class="line">os.Remove(PidFile)</span><br><span class="line">os.Exit(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">return</span> ErrStop</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">sighupDefaultHandler</span><span class="params">(sig os.Signal)</span> <span class="title">error</span></span> &#123;</span><br><span class="line"><span class="comment">//only deamon时不支持kill -HUP,因为可能监听地址会占用</span></span><br><span class="line">log.Printf(<span class="string">"[%d] %s stopped."</span>, os.Getpid(), AppName)</span><br><span class="line">os.Remove(PidFile)</span><br><span class="line">os.Exit(<span class="number">2</span>)</span><br><span class="line"><span class="keyword">return</span> ErrStop</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>目前内置了<code>SIGINT</code>,<code>SIGTERM</code>,<code>SIGHUP</code>的默认行为</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;早期，在我们的部门中后端的技术栈语言主要有三种语言，分别是&lt;code&gt;php/python/erlang&lt;/code&gt;，其中用于做服务的是 &lt;code&gt;php/erlang&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;在我们的体系中，日志采集服务体系目前都是用erlang写的，而php写的服务多是基于swoole写的一些基础服务。&lt;/p&gt;
&lt;p&gt;在tga服务中，我们需要从&lt;code&gt;kafka&lt;/code&gt; -&amp;gt; &lt;code&gt;服务&lt;/code&gt; -&amp;gt; &lt;code&gt;本地文件&lt;/code&gt;的模式。&lt;/p&gt;
&lt;p&gt;业务据流图如下：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;27&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;29&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;30&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;31&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;32&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;33&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;34&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;35&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;36&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;+---------------------------------------------------------------+&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|                                                               |&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|                                                               |&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|       +---------------------------------------------+         |&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|       |                                             |         |&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|       |                                             |         |&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|       |                  kafka服务                   |         |&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|       |                                             |         |&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|       |                                             |         |&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|       |                                             |         |&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|       +----------------------+----------------------+         |&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|                              |                                |&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|                              |                                |&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|       +----------------------v-----------------------+        |                     +----------------------------+&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|       |                                              |        |                     |                            |&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|       |                                              |        |                     |                            |&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|       |                                              |        |                     |                            |&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|       |               mthinkingdata服务               |        &amp;lt;---------------------+          logbus服务        |&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|       |                                              |        |                     |                            |&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|       |                                              |        |                     |                            |&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|       |                                              |        |                     |                            |&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|       +----------------------+-----------------------+        |                     +----------------------------+&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|                              |                                |&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|                              |                                |&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|       +----------------------v------------------------+       |&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|       |                                               |       |&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|       |                                               |       |&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|       |                                               |       |&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|       |                   本地文件                     +-------+&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|       |                                               |       |&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|       |                                               |       |&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|       |                                               |       |&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|       +-----------------------------------------------+       |&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|                                                               |&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;+---------------------------------------------------------------+&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;


&lt;p&gt;于是我们试探性的自研基于swoole的kafka客户端，我们自己实现根据kafka协议的封包，解包，流程处理。(&lt;code&gt;swoole-kafka&lt;/code&gt;)&lt;br&gt;&lt;code&gt;mthinkingdata服务&lt;/code&gt;就是我们基于&lt;code&gt;kafka-swoole&lt;/code&gt;研发的业务服务。&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;+------------+&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|            |&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|  message1  +------------+&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|            |            |&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;+------------+            |&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                          |&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;+------------+     +------+--------+&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|            |     |               |&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|  message2  +-----+   snappy压缩   |&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|            |     |               |&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;+------------+     +------+--------+&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                           |&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;+---------------+          |&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|               |          |&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;| message[3..n] +----------+&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|               |&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;+---------------+&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;p&gt;在这个过程中，我们发现php对于cpu密集型的处理存在瓶颈，因为我们在生产者一方如果发送多条协议的情况下，会经过 &lt;code&gt;snappy&lt;/code&gt; 算法的压缩再推送以便减少network-io（增加cpu-io）。&lt;br&gt;消费者在接受消息的时候也是被压缩过的数据，所以我们需要解压，在这个解压的过程中，是个十分消费cpu的过程，即使我们当时是基于swoole4.3的协程版本来处理，&lt;br&gt;不行的是的抢占式协程当时并没有很好的完成，我们没办法达到快速的接受多个数据包的行为。消费速度也并不是特别理想。&lt;br&gt;在这个大环境下，我们还需要借助redis来作为中间的存储。而redis是单线程的，我们在这个过程中，试过使用&lt;code&gt;pipeline&lt;/code&gt;等手段减少tcp中的响应包的带来的性能损耗。但是由于redis只能利用单核的缘故，批量处理一批指令后，最高的cpu利用率接近100%下无法再增长。&lt;br&gt;也因此，我们的服务注定无法达到很好的性能测试。&lt;/p&gt;
&lt;p&gt;我们得出结论，当时服务的瓶颈在于：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;ol&gt;
&lt;li&gt;php语言本身的性能&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;ol start=&quot;2&quot;&gt;
&lt;li&gt;swoole协程不支持抢占式调度&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;ol start=&quot;3&quot;&gt;
&lt;li&gt;未实现动态伸缩扩展worker数量（感兴趣可以去看看kafka-swoole的架构分享）&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;ol start=&quot;4&quot;&gt;
&lt;li&gt;redis未能利用多核，cpu利用率达到峰值&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Golang" scheme="http://blog.crazylaw.cn/categories/Golang/"/>
    
    
      <category term="Golang" scheme="http://blog.crazylaw.cn/tags/Golang/"/>
    
  </entry>
  
  <entry>
    <title>【DevOps】Jenkins的share-libraries明朝的运用</title>
    <link href="http://blog.crazylaw.cn/2020/11/10/DevOps/Jenkins%E5%9C%A8%E6%98%8E%E6%9C%9D%E7%9A%84%E8%BF%90%E7%94%A8/"/>
    <id>http://blog.crazylaw.cn/2020/11/10/DevOps/Jenkins%E5%9C%A8%E6%98%8E%E6%9C%9D%E7%9A%84%E8%BF%90%E7%94%A8/</id>
    <published>2020-11-10T08:44:30.000Z</published>
    <updated>2021-03-20T16:25:01.798Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本次我们不从Jenkins的部署架构和具体配置说明，我们主要围绕 <code>jenkinsfile</code> 的内部来讲解。</p><a id="more"></a><h2 id="Jenkinsfile"><a href="#Jenkinsfile" class="headerlink" title="Jenkinsfile"></a>Jenkinsfile</h2><p>目前我们的 jenkins 服务用过<code>普通的pipeline</code>和 <code>多分支的pipeline</code>，其主要区别就是<code>多分支pipeline</code>更加灵活一些，能够让我们少做一些逻辑处理。<br>但是这引入了一个问题，那就是我们的<code>jenkins</code>，必须存在与对应的分支中，也因此，pipeline 的流程变成了代码管理，并且由于我们存在多个项目，那么将会有<code>项目数 * 2(master/pre-develop)</code> 那么多的 jenkins 需要管理，于是就导致了我们有很多这种冗余的写法，这还不是最糟糕的，最糟糕的是如果我们需要修改一些通用的内容的时候，就需要一个个去修改，这绝对是灾难级别的需求。</p><p>Jenkinsfile的实现分为<code>2</code>种，分别是<code>声明式pipeline</code>和<code>脚本式pipeline</code>，这2种各有优缺点</p><p>jenkins需要做的事情，可以概括为如下几种：</p><ol><li>拉取代码</li><li>版本管理</li><li>单元测试</li><li>构建部署</li><li>通知结果</li></ol><h3 id="声明式pipeline"><a href="#声明式pipeline" class="headerlink" title="声明式pipeline"></a>声明式pipeline</h3><p>可以为我们提供<code>关键字</code> 和 <code>顺序结构</code>来辅助我们完成如上的5个阶段。</p><p>但是他的缺点也是明显的，就是<code>不能很灵活</code>根据自己的想法去做处理逻辑。</p><p>还有一种情况是比较蛋疼的，例如项目A和项目B，独立维护一个jenkinsfile，但是其实项目A和项目B的jenkinsfile其实是一致的，如果某一条要优化这个jenkinsfile的部署逻辑了。那么项目B和项目A都需要一起处理，无法实现<code>复用</code>的概念，并没有提供<code>抽象</code>的概念，不利于维护。</p><p>对于一些<code>相对简单，没有太复杂流程</code>的构建过程来说，声明式一定是大家的首要选择。</p><h3 id="脚本式pipeline"><a href="#脚本式pipeline" class="headerlink" title="脚本式pipeline"></a>脚本式pipeline</h3><p>可以给我们提供类似于写代码脚本的方式来组织部署逻辑，我把<code>脚本式pipeline</code>定义为<code>升级版的声明式pipeline</code>。</p><p>虽然脚本式的pipeline可以给我们提供灵活的部署逻辑，但是依旧无法解决我们的<code>复用</code>的问题。</p><p>我们需要把这种能<code>复用</code>的东西放在同一个地方进行统一管理，类似于<code>依赖库</code>的概念，独立与jenkinsfile之外的一种存在。于是乎，这便有了<code>jenkins-shard-libaray</code>。</p><h2 id="Jenkins-shard-libaray"><a href="#Jenkins-shard-libaray" class="headerlink" title="Jenkins-shard-libaray"></a>Jenkins-shard-libaray</h2><p>这是一个jenkins中的共享类库，只需要在jenkinsfile中对其引入，就可以引用共享库的代码，从而实现我们的<code>复用</code>。</p><p>但是有一点是这个共享库，需要我们储备点<code>groovy</code>语言的知识，因为他是用groovy作为”外挂”的方式加载运行的。</p><p>我们在写共享库的时候，<code>必须</code> 依照要有2个基本目录，否则jenkins无法类库。</p><ul><li>src (必要，核心代码)</li><li>vars (非必要，声明自己的语法)</li><li>resources (非必要，配置存放的目录)</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">./</span><br><span class="line">├── Jenkinsfile.example                                   jenkinsfile的DEMO样式</span><br><span class="line">├── README.MD</span><br><span class="line">├── jars</span><br><span class="line">│   ├── groovy-cps-1.1.jar</span><br><span class="line">│   └── pipeline-model-definition-1.7.1.jar</span><br><span class="line">├── resources                                              配置文件和基础库分开管理了，所以这里不会存在静态资源文件</span><br><span class="line">├── src</span><br><span class="line">│   └── com</span><br><span class="line">│       └── company</span><br><span class="line">│           └── jenkins</span><br><span class="line">│               ├── BasePipeline.groovy                    基础管道</span><br><span class="line">│               ├── CommonPipeline.groovy                  公共管道</span><br><span class="line">│               ├── Constant.groovy                        常量类</span><br><span class="line">│               ├── Deploy.groovy                          部署基类</span><br><span class="line">│               ├── Git.groovy                             git相关的操作类</span><br><span class="line">│               ├── GolangPipeline.groovy                  golang项目管道</span><br><span class="line">│               ├── LaravelPipeline.groovy                 laravel项目管道</span><br><span class="line">│               ├── MPipeline.groovy                       初始化类</span><br><span class="line">│               ├── MetlNotify.groovy                      用于发送通知的类</span><br><span class="line">│               ├── Repo.groovy                            用于实例化代码仓库信息</span><br><span class="line">│               ├── SummaryNotify.groovy                   用于结构化通知内容的类</span><br><span class="line">│               └── deployments                            具体的部署类</span><br><span class="line">│                   ├── AbstractDeployment.groovy          抽象部署类</span><br><span class="line">│                   ├── BaseDeployment.groovy              基础部署类</span><br><span class="line">│                   ├── DefaultDeployment.groovy           默认部署类</span><br><span class="line">│                   └── GolangDeployment.groovy            Go部署类</span><br><span class="line">├── tests                                                  单元测试目录</span><br><span class="line">│   ├── RepoTest.groovy</span><br><span class="line">│   └── classfiles</span><br><span class="line">│       └── com</span><br><span class="line">│           └── company</span><br><span class="line">│               └── jenkins</span><br><span class="line">│                   ├── Constant.class</span><br><span class="line">│                   ├── Git.class</span><br><span class="line">│                   ├── MPipeline.class</span><br><span class="line">│                   ├── MetlNotify.class</span><br><span class="line">│                   ├── Repo.class</span><br><span class="line">│                   └── SummaryNotify.class</span><br><span class="line">└── vars                                                   全局函数定义目录</span><br><span class="line">    ├── mpipeline.groovy                                   自定义入口语法糖</span><br><span class="line">    └── notify.groovy                                      定义通知全局调用函数</span><br></pre></td></tr></table></figure><h3 id="Jenkinsfile-1"><a href="#Jenkinsfile-1" class="headerlink" title="Jenkinsfile"></a>Jenkinsfile</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">mpipeline&#123;</span><br><span class="line">    script&#x3D;this</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import com.company.jenkins.LaravelPipeline</span><br><span class="line">def laravelPipeline &#x3D; new LaravelPipeline(this)</span><br><span class="line"></span><br><span class="line">node &#123;</span><br><span class="line">    laravelPipeline.preBuild().build().debug().reviews().production().execute()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这是一个最基本的jenkinsfile的demo。我们把他分为2个部分。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 第一部分</span><br><span class="line">mpipeline&#123;</span><br><span class="line">    script&#x3D;this</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 第二部分</span><br><span class="line">import com.company.jenkins.LaravelPipeline</span><br><span class="line">def laravelPipeline &#x3D; new LaravelPipeline(this)</span><br><span class="line"></span><br><span class="line">node &#123;</span><br><span class="line">    laravelPipeline.preBuild().build().debug().reviews().production().execute()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这2个部分各司其职。</p><p>第一部分 <code>mpipeline</code> 的语法糖，来自于我们<code>vars/mpipeline.groovy</code>文件，这是我们自定义的语法糖。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">import com.company.jenkins.*</span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line"> *</span><br><span class="line"> * @param body</span><br><span class="line"> *      repo 代码仓库地址 default: null</span><br><span class="line"> *      autoCheckout 是否需要自动checkout代码，default: true</span><br><span class="line"> * @return</span><br><span class="line"> *&#x2F;</span><br><span class="line">def call(body) &#123;</span><br><span class="line">    def config &#x3D; [:]</span><br><span class="line">    body.resolveStrategy &#x3D; Closure.DELEGATE_FIRST</span><br><span class="line">    body.delegate &#x3D; config</span><br><span class="line">    body()</span><br><span class="line"></span><br><span class="line">    config.repo &#x3D; config.repo ?: null</span><br><span class="line">    config.script &#x3D; config.script ?: null</span><br><span class="line">    config.autoCheckout &#x3D; config.autoCheckout ?: true</span><br><span class="line"></span><br><span class="line">    if (config.script &#x3D;&#x3D; null) &#123;</span><br><span class="line">        throw new Exception(&quot;&lt;script&#x3D;this&gt;是必填参数&quot;)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    if (config.repo &#x3D;&#x3D; null) &#123;</span><br><span class="line">        config.repo &#x3D;  config.script.scm.getUserRemoteConfigs()[0].getUrl()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    node &#123;</span><br><span class="line">        def mpipe &#x3D; new MPipeline(config.script, config.repo)</span><br><span class="line">        mpipe.initializeResources()</span><br><span class="line">        def credentialsId &#x3D; config.credentialsId ?: config.script.gitCredentialsId</span><br><span class="line"></span><br><span class="line">        if (config.autoCheckout) &#123;</span><br><span class="line">            stage(&quot;Checkout&quot;) &#123;</span><br><span class="line">                &#x2F;&#x2F; git clone repo</span><br><span class="line">                git credentialsId: credentialsId, url: mpipe.repo.getGitLink(), branch: env.BRANCH_NAME</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        mpipe.initializeEnvironment()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    return this</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个语法糖做的内容定义了 <code>mpipeline</code> 做的内容，其中有三个有效参数 <code>repo</code>, <code>script</code>, <code>autoCheckout</code>，分别的意思是代表<code>该任务下的操作仓库地址</code>，<code>当前任务上下文</code>, <code>是否自动拉取代码</code>。其中 <code>script</code>参数为必填参数，其他2个都是选填参数，其都存在默认值。这个<code>script</code>必填的原因是因为一定要从jenkinsfile把对象传递进来，否则作用域有所不同。因为我们在第二部分要把这个对象继续传递下去，因为他携带了贯穿整个任务的上下文。</p><p>我们这里看到了 <code>node</code> 结构块，这是因为在<code>脚本式pipeline</code>中，他其实也是jenkins内置的一个自定义语法糖，所以他可以直接被嵌入在我们的代码中。因为我们这里实例化了共享库中的<code>Mpipeline类</code>，所以需要在 <code>node</code> 结构块中编写代码。</p><p>在这一块，我们可以看到，我们做了3件事</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">+--------------------------------------+                +--------------------------------+              +----------------------------------------------------+</span><br><span class="line">|                                      |                |                                |              |                                                    |</span><br><span class="line">|  1. Initialize the custom resource   +---------------&gt;+   2. Pull the warehouse code   +-------------&gt;+   3. Initialize the custom environment variable    |</span><br><span class="line">|                                      |                |                                |              |                                                    |</span><br><span class="line">+--------------------------------------+                +--------------------------------+              +----------------------------------------------------+</span><br></pre></td></tr></table></figure><ul><li><p>初始化自定义资源（接下来要说的resource-libaray共享库）</p></li><li><p>拉取仓库代码（这是jenkins自带的一个git插件，调用git函数，加上一些参数，就会拉取代码到一个目录，如果目录不存在，则会创建）</p></li><li><p>初始化自定义环境变量</p></li></ul><p>初始化自定义资源的代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;**</span><br><span class="line"> * 初始化资源库</span><br><span class="line"> *&#x2F;</span><br><span class="line">def initializeResources() &#123;</span><br><span class="line">    this.script.mresource_load([</span><br><span class="line">            script: this.script,</span><br><span class="line">            groupRepo: this.getRepoGroupRepo(),</span><br><span class="line">            shortJobName: this.getShortName(),</span><br><span class="line">            host: this.getHost()</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">    return this</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里，我们可以看到我们通过上下文变量，调用了 <code>mresource_load</code> 函数，这个是我们在 <code>resource-libaray共享库</code> 定义的函数，用于加载对应仓库的具体资源配置用，相关参数依旧会加载到上下文之中。</p><p>自定义环境变量部分代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;**</span><br><span class="line"> * 初始化环境变量</span><br><span class="line"> *&#x2F;</span><br><span class="line">def initializeEnvironment() &#123;</span><br><span class="line">    this.initBaseEnv().initEnvDependGit().initEnvDependRepo()</span><br><span class="line"></span><br><span class="line">    return this</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">def initBaseEnv() &#123;</span><br><span class="line">    &#x2F;&#x2F; SHORT_JOB_NAME</span><br><span class="line">    this.script.env.SHORT_JOB_NAME &#x3D; this.getShortName()</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; 是否仅仅显示stage节点而不执行内容</span><br><span class="line">    this.script.mStageShow &#x3D; false</span><br><span class="line"></span><br><span class="line">    return this</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">def initEnvDependGit() &#123;</span><br><span class="line">    &#x2F;&#x2F; COMMIT_ID</span><br><span class="line">    this.script.env.COMMIT_ID &#x3D; this.git.commitId()</span><br><span class="line">    &#x2F;&#x2F; COMMIT_AUTHOR</span><br><span class="line">    this.script.env.COMMIT_AUTHOR &#x3D; this.git.commitAuthor()</span><br><span class="line">    &#x2F;&#x2F; COMMIT_TIME</span><br><span class="line">    this.script.env.COMMIT_TIME &#x3D; this.git.commitTime()</span><br><span class="line">    &#x2F;&#x2F; COMMIT_COMMENT</span><br><span class="line">    this.script.env.COMMIT_COMMENT &#x3D; this.git.commitMessage()</span><br><span class="line"></span><br><span class="line">    return this</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">def initEnvDependRepo() &#123;</span><br><span class="line">    &#x2F;&#x2F; REPO</span><br><span class="line">    this.script.env.REPO &#x3D; this.repo.getRepo()</span><br><span class="line">    &#x2F;&#x2F; REPOSITORY</span><br><span class="line">    this.script.env.REPOSITORY &#x3D; this.repo.getFullRepoString()</span><br><span class="line">    &#x2F;&#x2F; Notify</span><br><span class="line">    this.script.env.COMMIT_ID_PATH &#x3D; this.repo.getCommitIdHostPath()</span><br><span class="line">    &#x2F;&#x2F; REPOSITORY_LINK</span><br><span class="line">    this.script.env.REPOSITORY_LINK &#x3D; this.repo.getGitLink()</span><br><span class="line">    &#x2F;&#x2F; REPOSITORY_GROUP</span><br><span class="line">    this.script.env.REPOSITORY_GROUP &#x3D; this.repo.getGroup()</span><br><span class="line">    &#x2F;&#x2F; REPOSITORY_GROUP_REPO</span><br><span class="line">    this.script.env.REPOSITORY_GROUP_REPO &#x3D; this.getRepoGroupRepo()</span><br><span class="line"></span><br><span class="line">    return this</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个，我们把环境变量也拆分封装到对应的方法了，分别是git相关的环境变量和仓库信息的环境变量等等。</p><p>接下来，我们看下 <code>第二部分</code> 的代码。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 第二部分</span><br><span class="line">import com.company.jenkins.LaravelPipeline</span><br><span class="line">def laravelPipeline &#x3D; new LaravelPipeline(this)</span><br><span class="line"></span><br><span class="line">node &#123;</span><br><span class="line">    laravelPipeline.preBuild().build().testing().debug().reviews().production().execute()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里，我们import进来了Laravel的具体实现管道，并且实例化这个对象，传入了当前上下文对象。</p><p>并且在下面的node结构中，调用laravelpipeline对象的相关封装方法。</p><p>经过我们对部署流程的抽象和统一，我们的部署流程大致分为 <code>5个阶段</code>。分别如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">+-----------------------------+</span><br><span class="line">|  1. Pre-construction phase  |</span><br><span class="line">+------------+----------------+</span><br><span class="line">             |</span><br><span class="line">             |             +-----------------------+</span><br><span class="line">             +------------&gt;+2. Construction phase  +------------------+</span><br><span class="line">                           +-----------------------+                  |</span><br><span class="line">                                                                      |</span><br><span class="line">                                                                      |</span><br><span class="line">                                                +---------------------v-----------------------------+</span><br><span class="line">                                        +-------+3. Deployment development environment phase (Debug)|</span><br><span class="line">                                        |       +---------------------------------------------------+</span><br><span class="line">                                        |</span><br><span class="line">                                        v</span><br><span class="line">                           +------------+---------------------------------------+</span><br><span class="line">             +-------------+4. Grayscale Environment Deployment Stage（ reviews） |</span><br><span class="line">             |             +----------------------------------------------------+</span><br><span class="line">             |</span><br><span class="line">+------------v----------------------------------------------+</span><br><span class="line">| 5. Deployment of production environment phase（ prodution） |</span><br><span class="line">+-----------------------------------------------------------+</span><br></pre></td></tr></table></figure><ul><li>预构建阶段</li><li>构建阶段</li><li>部署开发环境阶段（debug）</li><li>部署灰度环境阶段（reviews）</li><li>部署生产环境阶段（production）</li></ul><p>因此，我们把所有的仓库jenkinsfile都按照这样子的不是来写，这样子就可以做到，当我们需要修改部署的内容的时候，代码仓库的jenkinsfile不需要做任务变动，只需要修改我们的共享库即可。</p><p>部分代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">def preBuild() &#123;</span><br><span class="line">    def callback &#x3D; &#123;</span><br><span class="line">        this.startNotify()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    def apiName &#x3D; &#39;preBuild&#39;</span><br><span class="line">    this.addCallback(apiName, callback)</span><br><span class="line"></span><br><span class="line">    return this</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">def build(isParallel &#x3D; true) &#123;</span><br><span class="line">    def callback &#x3D; &#123;</span><br><span class="line">        this.script.stage(&#39;Build&#39;) &#123;</span><br><span class="line">            if (isParallel) &#123;</span><br><span class="line">                def stages &#x3D; [failFast: true]</span><br><span class="line"></span><br><span class="line">                stages[&quot;Composer Install&quot;] &#x3D; this.composerInstall()</span><br><span class="line">                stages[&quot;Client Build&quot;] &#x3D; this.clientBuild()</span><br><span class="line">                this.script.parallel stages</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">                this.composerInstall()()</span><br><span class="line">                this.clientBuild()()</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    def apiName &#x3D; &#39;build&#39;</span><br><span class="line">    this.addCallback(apiName, callback)</span><br><span class="line"></span><br><span class="line">    return this</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里看到，我们几乎所有的代码都会通过回调函数来编写的，所以，我们在实际调用的时候，必须加上 <code>execute()</code> 方法来实际触发链式过程。</p><p>因为在我们这个例子中，laravel是php的项目，默认情况下，我们是有前端和后端的代码。分别是<code>composer依赖</code>，<code>npm依赖</code>。</p><p>由于这2个部分的依赖，他们是没有相关性的，所以，我们在设计上，默认是支持 <code>并发</code> 执行的，所以我们这里调用了jenkins的一个内置函数 <code>parallel</code>来实现并发。实际效果如下展示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">                          +-----------------------+</span><br><span class="line">                          | +------------------+  |</span><br><span class="line">                          | | Composer Install |  |</span><br><span class="line">                          | +------------------+  |</span><br><span class="line">                          |                       |</span><br><span class="line">                          |                       |</span><br><span class="line">+----------------+        |                       |</span><br><span class="line">|   pre-build    +-------&gt;+        build          |</span><br><span class="line">+----------------+        |                       |</span><br><span class="line">                          |                       |</span><br><span class="line">                          |                       |</span><br><span class="line">                          | +------------------+  |</span><br><span class="line">                          | | Client Build     |  |</span><br><span class="line">                          | +------------------+  |</span><br><span class="line">                          |                       |</span><br><span class="line">                          +-----------------------+</span><br></pre></td></tr></table></figure><p>以npm依赖安装为例子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">public npmInstall(def packageFile &#x3D; &#39;client&#x2F;package.json&#39;, def clientDir &#x3D; &#39;client&#39;, def version &#x3D; &#39;latest&#39;) &#123;</span><br><span class="line">    def md5File &#x3D; &quot;$&#123;packageFile&#125;.md5&quot;</span><br><span class="line">    def callback &#x3D; &#123; isChange -&gt;</span><br><span class="line">        this.script.echo &quot;package.json是否有变化:$&#123;isChange&#125;&quot;</span><br><span class="line">        this.script.echo &quot;是否强制编译前端资源:$&#123;this.script.params.isBuild&#125;&quot;</span><br><span class="line">        if (isChange || this.script.params.isBuild) &#123;</span><br><span class="line">            this.script.docker.image(&quot;jenkins_docker_node:$&#123;version&#125;&quot;).inside(&#39;--dns-opt&#x3D;ndots:5 -v $HOME&#x2F;.npm:&#x2F;root&#x2F;.npm&#39;) &#123;</span><br><span class="line">                ...</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    this.checkChangeFile(packageFile, md5File, callback)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>另外在composer和npm依赖安装的过程中，我们做了一些细节上的优化，例如，我们会检测<code>package.json</code>以及<code>composer.json</code>，如果这2个文件没有变化的话，则不会更新依赖。避免了每次构建都需要去检查更新依赖包。<br>另外由于我们的jenkisn环境本身也是一个jenkins的docker容器，所以我们的jenkins环境下是不会存在<code>php</code>以及<code>node</code>的环境，所以我们这个时候，在安装依赖的时候，借助了上下文中的 <code>docker.image</code> 关键字来构建一个容器来触发我们的依赖安装。</p><p>接下来就是部署相关的内容了，我们以部署 <code>production</code> 环境来举例子</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">def production(def map &#x3D; [</span><br><span class="line">        sshId      : null,</span><br><span class="line">        deployClass: null</span><br><span class="line">]) &#123;</span><br><span class="line">    def callback &#x3D; &#123;</span><br><span class="line">        if (map[&#39;sshId&#39;] &#x3D;&#x3D; null) &#123;</span><br><span class="line">            map[&#39;sshId&#39;] &#x3D; this.getValue(&#39;deploySshId&#39;)</span><br><span class="line">        &#125;</span><br><span class="line">        if (map[&#39;deployClass&#39;] &#x3D;&#x3D; null) &#123;</span><br><span class="line">            map[&#39;deployClass&#39;] &#x3D; this.getDeployClass()</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        map[&#39;pipeline&#39;] &#x3D; this</span><br><span class="line"></span><br><span class="line">        this.deploy.production(map)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    def apiName &#x3D; &#39;production&#39;</span><br><span class="line">    this.addCallback(apiName, callback)</span><br><span class="line"></span><br><span class="line">    return this</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果不指定 <code>sshId</code> 和 <code>deployClass</code> 的话，都会存在默认值。由于我们整个过程都是通过回调的方式调用的（其实类似于中间件），所以我们这里的多了一个 <code>map[pipeline] = this</code> 的代码，意义在于把当前上下文传递进具体的部署类。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br></pre></td><td class="code"><pre><span class="line">def production(def map &#x3D; [</span><br><span class="line">        sshId      : null,</span><br><span class="line">        deployClass: null,</span><br><span class="line">        pipeline   : null</span><br><span class="line">]) &#123;</span><br><span class="line"></span><br><span class="line">    def keyword &#x3D; &#39;production&#39;</span><br><span class="line">    this.script.stage(&#39;Production Deploy&#39;) &#123;</span><br><span class="line">        if (this.git.isMasterBranch() &amp;&amp; this.script.currentResources[keyword].enabled !&#x3D; false) &#123;</span><br><span class="line"></span><br><span class="line">            &#x2F;&#x2F; 注入参数</span><br><span class="line">            def tags &#x3D; this.git.tagsList()</span><br><span class="line"></span><br><span class="line">            def buildParameters &#x3D; [</span><br><span class="line">                    [</span><br><span class="line">                            &#39;$class&#39;    : &#39;ChoiceParameter&#39;,</span><br><span class="line">                            choiceType  : &#39;PT_SINGLE_SELECT&#39;,</span><br><span class="line">                            description : &#39;&#39;&#39;</span><br><span class="line">选择构建类型：</span><br><span class="line">auto_deploy - 自动发布（直接发布最新代码）        </span><br><span class="line">deploy - 指定版本发布</span><br><span class="line">rollback - 版本回滚发布</span><br><span class="line">&#39;&#39;&#39;,</span><br><span class="line">                            filterLength: 1,</span><br><span class="line">                            filterable  : false,</span><br><span class="line">                            name        : &#39;buildType&#39;,</span><br><span class="line">                            randomName  : &#39;choice-parameter-69483043309720&#39;,</span><br><span class="line">                            script      : [</span><br><span class="line">                                    &#39;$class&#39;      : &#39;GroovyScript&#39;,</span><br><span class="line">                                    fallbackScript: [</span><br><span class="line">                                            classpath: [],</span><br><span class="line">                                            sandbox  : true,</span><br><span class="line">                                            script   : &#39;return[&quot;unknow&quot;]&#39;</span><br><span class="line">                                    ],</span><br><span class="line">                                    script        : [</span><br><span class="line">                                            classpath: [],</span><br><span class="line">                                            sandbox  : true,</span><br><span class="line">                                            script   : &#39;return[&quot;auto_deploy&quot;, &quot;deploy&quot;, &quot;rollback&quot;]&#39;,</span><br><span class="line">                                    ]</span><br><span class="line">                            ]</span><br><span class="line">                    ],</span><br><span class="line">                    [</span><br><span class="line">                            &#39;$class&#39;    : &#39;ChoiceParameter&#39;,</span><br><span class="line">                            choiceType  : &#39;PT_SINGLE_SELECT&#39;,</span><br><span class="line">                            description : &#39;&#39;&#39;</span><br><span class="line">选择Tag标签：</span><br><span class="line">auto_deploy时该参数无效</span><br><span class="line">&#39;&#39;&#39;,</span><br><span class="line">                            filterLength: 20,</span><br><span class="line">                            filterable  : true,</span><br><span class="line">                            name        : &#39;buildTag&#39;,</span><br><span class="line">                            randomName  : &#39;choice-parameter-69483043309721&#39;,</span><br><span class="line">                            script      : [</span><br><span class="line">                                    &#39;$class&#39;      : &#39;GroovyScript&#39;,</span><br><span class="line">                                    fallbackScript: [</span><br><span class="line">                                            classpath: [],</span><br><span class="line">                                            sandbox  : true,</span><br><span class="line">                                            script   : &#39;return[&quot;unknow&quot;]&#39;</span><br><span class="line">                                    ],</span><br><span class="line">                                    script        : [</span><br><span class="line">                                            classpath: [],</span><br><span class="line">                                            sandbox  : true,</span><br><span class="line">                                            script   : &quot;return [$&#123;tags&#125;]&quot;,</span><br><span class="line">                                    ]</span><br><span class="line">                            ]</span><br><span class="line">                    ]</span><br><span class="line">            ]</span><br><span class="line"></span><br><span class="line">            if (!this.script.mStageShow) &#123;</span><br><span class="line">                this.script.notify &#39;请确认是否发布到生产环境？&#39;</span><br><span class="line"></span><br><span class="line">                this.script.timeout(time: 10, unit: &#39;MINUTES&#39;) &#123;</span><br><span class="line">                    try &#123;</span><br><span class="line">                        this.script.input &#39;请确认是否要发布到生产环境？&#39;</span><br><span class="line">                    &#125; catch (e) &#123;</span><br><span class="line">                        map[&#39;pipeline&#39;].parameters +&#x3D; buildParameters</span><br><span class="line">                        throw e</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    def updateHistoryFile &#x3D; &#39;UPDATE_HISTORY&#39;</span><br><span class="line">                    def currentTag</span><br><span class="line"></span><br><span class="line">                    if (this.script.params.buildType &#x3D;&#x3D; &#39;deploy&#39; || this.script.params.buildType &#x3D;&#x3D; &#39;rollback&#39;) &#123;</span><br><span class="line">                        &#x2F;&#x2F; 发布指定tag</span><br><span class="line">                        this.script.sh &quot;git checkout $&#123;this.script.params.buildTag&#125;&quot;</span><br><span class="line">                        this.script.currentBuild.description &#x3D; &quot;【指定发布】Tag: $&#123;this.script.params.buildTag&#125;&quot;</span><br><span class="line">                        if (this.script.params.buildType &#x3D;&#x3D; &#39;rollback&#39;) &#123;</span><br><span class="line">                            this.script.currentBuild.description &#x3D; &quot;【回滚】Tag: $&#123;this.script.params.buildTag&#125;&quot;</span><br><span class="line">                        &#125;</span><br><span class="line">                        currentTag &#x3D; this.script.params.buildTag</span><br><span class="line">                    &#125; else &#123;</span><br><span class="line">                        &#x2F;&#x2F; 开始打tag</span><br><span class="line">                        ...</span><br><span class="line">                            try &#123;</span><br><span class="line">                                def tagRes</span><br><span class="line">                                if (link.startsWith(&quot;http&quot;)) &#123;</span><br><span class="line">                                    &#x2F;&#x2F; gitbucket</span><br><span class="line">                                    ...</span><br><span class="line">                                    &#125;</span><br><span class="line">                                &#125; else &#123;</span><br><span class="line">                                    &#x2F;&#x2F; gitlab</span><br><span class="line">                                    ...</span><br><span class="line">                                &#125;</span><br><span class="line">                                if (tagRes[&#39;exitCode&#39;] &gt; 0 &amp;&amp; !tagRes[&#39;stdout&#39;].contains(&#39;already exists&#39;)) &#123;</span><br><span class="line">                                    throw new Exception(tagRes[&#39;stdout&#39;])</span><br><span class="line">                                &#125;</span><br><span class="line">                            &#125; catch (e) &#123;</span><br><span class="line">                                throw e</span><br><span class="line">                            &#125;</span><br><span class="line">                            this.script.sh &quot;tee $&#123;preCommitIdFile&#125; &lt;&lt;&lt; $&#123;this.git.commitId()&#125;&quot;</span><br><span class="line"></span><br><span class="line">                            currentTag &#x3D; nextTag</span><br><span class="line"></span><br><span class="line">                            &#x2F;&#x2F; 最新tag加入下拉框</span><br><span class="line">                            buildParameters[1][&#39;script&#39;][&#39;script&#39;][&#39;script&#39;] &#x3D; &quot;return [\&quot;$&#123;currentTag&#125;\&quot;,$&#123;tags&#125;]&quot;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                    ...</span><br><span class="line">                    this.script.currentBuild.description &#x3D; &quot;【自动发布】Tag: $&#123;currentTag&#125;&quot;</span><br><span class="line">                    this.publishDeployment(keyword)(map[&#39;sshId&#39;], map[&#39;deployClass&#39;])</span><br><span class="line">                    ...</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">                this.script.echo &#39;Production Deploy Show&#39;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            map[&#39;pipeline&#39;].parameters +&#x3D; buildParameters</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            Utils.markStageSkippedForConditional(this.script.env.STAGE_NAME)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    return this</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里，我们再说明一下我们的自动化流程。</p><ul><li><ol><li>提交feature代码</li></ol></li><li><ol start="2"><li>合并到pre-develop分支，触发jenkins任务构建更新debug环境代码</li></ol></li><li><ol start="3"><li>最终合并到master分支，触发jenkins任务，如果代码有变化则对最新的master代码进行打tag，构建更新production环境代码</li></ol></li></ul><p>基于以上几点，我们就可以很直观的看到部署production类的时候，我们会检测master的代码，如果存在变化则进行打tag。然后再调用具体的部署逻辑。其中 <code>this.script.currentResources[keyword].enabled</code> 是来自于我们的 <code>resource-libaray</code> 的配置。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">abstract class AbstractDeployment &#123;</span><br><span class="line"></span><br><span class="line">    &#x2F;**</span><br><span class="line">     * 打包逻辑</span><br><span class="line">     * @return</span><br><span class="line">     *&#x2F;</span><br><span class="line">    abstract def parallelBefore(def packFiles, def packExclude)</span><br><span class="line"></span><br><span class="line">    &#x2F;**</span><br><span class="line">     * 并行逻辑</span><br><span class="line">     * @param ip</span><br><span class="line">     * @param port</span><br><span class="line">     * @param directory</span><br><span class="line">     * @return</span><br><span class="line">     *&#x2F;</span><br><span class="line">    abstract def parallel(def ip, def port, def directory, def backupExclude)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们的抽象具体部署类存在2个必须实现的抽象方法，分别是 <code>parallelBefore</code>, <code>parallel</code>。</p><ul><li>parallelBefore：并发部署前做的事情</li><li>parallel：并发部署到多台目标机器上</li></ul><h2 id="resources-libraries"><a href="#resources-libraries" class="headerlink" title="resources-libraries"></a>resources-libraries</h2><p>资源共享库</p><p>对外提供一个全局的函数 <code>mresource_load</code>，内部提供给上下文关键字拿到当前项目，当前环境的对应的部署内容信息。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">├── README.MD</span><br><span class="line">├── resources</span><br><span class="line">│   ├── git.xxxx.com</span><br><span class="line">│   │   ├── repo-group1</span><br><span class="line">│   │   │   ├── a.yaml</span><br><span class="line">│   │   │   └── b.yaml</span><br><span class="line">│   │   └── repo-group2</span><br><span class="line">│   │       └── c.yaml</span><br><span class="line">│   └── gitlab.xxx.com</span><br><span class="line">│       ├── repo-group3</span><br><span class="line">│       │   ├── d.yaml</span><br><span class="line">│       │   └── e.yaml</span><br><span class="line">│       └── repo-group4</span><br><span class="line">│           ├── f.yaml</span><br><span class="line">│           └── g.yaml</span><br><span class="line">├── tests</span><br><span class="line">│   └── test_mresource_load.groovy</span><br><span class="line">└── vars</span><br><span class="line">    └── mresource_load.groovy</span><br></pre></td></tr></table></figure><p>我们看到我们的基本结构和上一个 <code>jenkins-shard-libraries</code> 十分的类似，区别在于 <code>resources-libraries</code> 不存在 <code>src</code> 目录，但是 <code>resources</code> 全部都存在资源配置。</p><p>资源存放的目录，结构目录规则为 <code>&lt;FQDN&gt;/&lt;group&gt;/&lt;repo&gt;</code>，对应git仓库。</p><p>资源库采用<code>yaml</code>格式。目前支持的key有：</p><ul><li><p>type(normal|job_name)</p><ul><li>normal   常规项目，不区分项目</li><li>job_name 区分项目，一套仓库，多个项目区分部署</li></ul></li><li><p>debug 内测环境</p><ul><li>address   ip:port</li><li>directory 代码目录路径</li><li>deploy_ssh_id 部署的凭证</li><li>execute_user 执行的用户，目前仅仅在GolangPipline有效</li><li>ssh_username 部署代码的ssh的账号，目前在GolangPipline仅调试用</li></ul></li><li><p>reviews 灰度环境</p><ul><li>enabled:  false  跳过reviews步骤</li><li>address   ip:port</li><li>directory 代码目录路径</li><li>deploy_ssh_id 部署的凭证</li><li>execute_user 执行的用户，目前仅仅在GolangPipline有效</li><li>ssh_username 部署代码的ssh的账号，目前在GolangPipline仅调试用【非必填，默认采用deploy_ssh_id的信息】</li></ul></li><li><p>production 生产环境</p><ul><li>address   ip:port</li><li>directory 代码目录路径</li><li>deploy_ssh_id 部署的凭证</li><li>branch    生产环境对应的分支 【非必填】</li><li>execute_user 执行的用户，目前仅仅在GolangPipline有效【非必填】</li><li>ssh_username 部署代码的ssh的账号，目前在GolangPipline仅调试用【非必填，默认采用deploy_ssh_id的信息】</li></ul></li></ul><p>附加在上下文的变量：</p><ul><li>script.currentResources</li><li>script.gitCredentialsId</li><li>script.buildCredentialsId</li></ul><p>具体的yaml配置格式的demo</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">type: &#39;normal&#39;</span><br><span class="line"></span><br><span class="line">git_credentials_id: &#39;git_credentials_id&#39;</span><br><span class="line"></span><br><span class="line">common: &amp;common</span><br><span class="line">  deploy_ssh_id: &#39;deploy_ssh_id&#39;</span><br><span class="line"></span><br><span class="line">environment:</span><br><span class="line">  debug:</span><br><span class="line">    &lt;&lt;: *common</span><br><span class="line">    address:</span><br><span class="line">      - &#39;2.2.2.2:22&#39;</span><br><span class="line">      - &#39;1.1.1.1:22&#39;</span><br><span class="line">    directory:</span><br><span class="line">      &#39;2.2.2.2:22&#39;: &#39;&#x2F;a&#39;</span><br><span class="line">      &#39;1.1.1.1:22&#39;: &#39;&#x2F;b&#39;</span><br><span class="line"></span><br><span class="line">  reviews:</span><br><span class="line">    &lt;&lt;: *common</span><br><span class="line">    address: &#39;3.3.3.3:22&#39;</span><br><span class="line">    directory: &#39;&#x2F;c&#39;</span><br><span class="line"></span><br><span class="line">  production:</span><br><span class="line">    &lt;&lt;: *common</span><br><span class="line">    address: &#39;4.4.4.4:22&#39;</span><br><span class="line">    directory: &#39;&#x2F;d&#39;</span><br></pre></td></tr></table></figure><p>这是基本的配置文件，我们借助了yaml的灵活性，配置出尽可能简洁，可复用的项。<br>我们还有更加灵活的配置和格式，具体参考以下代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br></pre></td><td class="code"><pre><span class="line">@Grab(&#39;org.yaml:snakeyaml:1.26&#39;)</span><br><span class="line">import org.yaml.snakeyaml.*</span><br><span class="line"></span><br><span class="line">def call(def config &#x3D; [script: null, groupRepo: null, shortJobName: null, host: null]) &#123;</span><br><span class="line"></span><br><span class="line">    def script &#x3D; config.script</span><br><span class="line">    def groupRepo &#x3D; config.groupRepo</span><br><span class="line">    def shortJobName &#x3D; config.shortJobName</span><br><span class="line">    def host &#x3D; config.host</span><br><span class="line"></span><br><span class="line">    if (groupRepo &#x3D;&#x3D; null) &#123;</span><br><span class="line">        groupRepo &#x3D; script.env.REPOSITORY_GROUP_REPO</span><br><span class="line">    &#125;</span><br><span class="line">    if (shortJobName &#x3D;&#x3D; null) &#123;</span><br><span class="line">        shortJobName &#x3D; script.env.SHORT_JOB_NAME</span><br><span class="line">    &#125;</span><br><span class="line">    if (host &#x3D;&#x3D; null) &#123;</span><br><span class="line">        host &#x3D; &quot;git.xxxx.com&quot;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    configFile &#x3D; host + &#39;&#x2F;&#39; + groupRepo + &#39;.yaml&#39;</span><br><span class="line">    Yaml yaml &#x3D; new Yaml()</span><br><span class="line">    String resourceString &#x3D; libraryResource(configFile)</span><br><span class="line">    Map&lt;String, Object&gt; obj &#x3D; yaml.load(resourceString)</span><br><span class="line">    obj.type &#x3D; obj.type ?: null</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def resource &#x3D; [:]</span><br><span class="line">    def git_credentials_id &#x3D; &#39;&#39;</span><br><span class="line">    def build_credentials_id &#x3D; &#39;&#39;</span><br><span class="line"></span><br><span class="line">    if (obj.type &#x3D;&#x3D; &#39;job_name&#39;) &#123;</span><br><span class="line">        String pointJobName &#x3D; shortJobName</span><br><span class="line">        for (item in obj) &#123;</span><br><span class="line">            if (item.key &#x3D;&#x3D; pointJobName) &#123;</span><br><span class="line">                def value &#x3D; item.value</span><br><span class="line">                for (it1 in value) &#123;</span><br><span class="line">                    if (it1.key &#x3D;&#x3D; &#39;git_credentials_id&#39;) &#123;</span><br><span class="line">                        git_credentials_id &#x3D; it1.value</span><br><span class="line">                        continue</span><br><span class="line">                    &#125;</span><br><span class="line">                    if (it1.key &#x3D;&#x3D; &#39;build_credentials_id&#39;) &#123;</span><br><span class="line">                        build_credentials_id &#x3D; it1.value</span><br><span class="line">                        continue</span><br><span class="line">                    &#125;</span><br><span class="line">                    if (it1.key &#x3D;&#x3D; &#39;environment&#39;) &#123;</span><br><span class="line">                        &#x2F;&#x2F; 兼容 address，String &amp;&amp; List</span><br><span class="line">                        for (it in it1.value) &#123;</span><br><span class="line">                            for (values in it.value) &#123;</span><br><span class="line">                                if (values instanceof Map.Entry) &#123;</span><br><span class="line">                                    if (values.key &#x3D;&#x3D; &#39;address&#39;) &#123;</span><br><span class="line">                                        if (values.value instanceof String) &#123;</span><br><span class="line">                                            values.value &#x3D; [values.value]</span><br><span class="line">                                        &#125;</span><br><span class="line">                                    &#125;</span><br><span class="line">                                &#125;</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    if (it1.key &#x3D;&#x3D; &#39;environment&#39;) &#123;</span><br><span class="line">                        &#x2F;&#x2F; 兼容 directory</span><br><span class="line">                        for (it in it1.value) &#123;</span><br><span class="line">                            for (values in it.value) &#123;</span><br><span class="line">                                if (values instanceof Map.Entry) &#123;</span><br><span class="line">                                    if (values.key &#x3D;&#x3D; &#39;directory&#39;) &#123;</span><br><span class="line">                                        if (values.value instanceof String) &#123;</span><br><span class="line">                                            def dir &#x3D; values.value</span><br><span class="line">                                            values.value &#x3D; [:]</span><br><span class="line">                                            for (def addr in it.value.address) &#123;</span><br><span class="line">                                                values.value[addr] &#x3D; dir</span><br><span class="line">                                            &#125;</span><br><span class="line">                                        &#125;</span><br><span class="line">                                    &#125;</span><br><span class="line">                                &#125;</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                resource &#x3D; value</span><br><span class="line"></span><br><span class="line">                break</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; else &#123;  &#x2F;&#x2F; job.type &#x3D;&#x3D; normal</span><br><span class="line">        resource &#x3D; obj</span><br><span class="line">        resource.remove(&#39;type&#39;)</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; 兼容 address，String &amp;&amp; List</span><br><span class="line">        for (item in resource.environment) &#123;</span><br><span class="line">            for (values in item.value) &#123;</span><br><span class="line">                if (values instanceof Map.Entry) &#123;</span><br><span class="line">                    if (values.key &#x3D;&#x3D; &#39;address&#39;) &#123;</span><br><span class="line">                        if (values.value instanceof String) &#123;</span><br><span class="line">                            values.value &#x3D; [values.value]</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; 兼容 directory</span><br><span class="line">        for (item in resource.environment) &#123;</span><br><span class="line">            for (values in item.value) &#123;</span><br><span class="line">                if (values instanceof Map.Entry) &#123;</span><br><span class="line">                    if (values.key &#x3D;&#x3D; &#39;directory&#39;) &#123;</span><br><span class="line">                        if (values.value instanceof String) &#123;</span><br><span class="line">                            def dir &#x3D; values.value</span><br><span class="line">                            values.value &#x3D; [:]</span><br><span class="line">                            for (def addr in item.value.address) &#123;</span><br><span class="line">                                values.value[addr] &#x3D; dir</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        git_credentials_id &#x3D; resource.git_credentials_id</span><br><span class="line">        build_credentials_id &#x3D; resource.build_credentials_id</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    def environment &#x3D; resource.environment</span><br><span class="line"></span><br><span class="line">    script.currentResources &#x3D; environment ?: [:]</span><br><span class="line">    script.gitCredentialsId &#x3D; git_credentials_id ?: null</span><br><span class="line">    script.buildCredentialsId &#x3D; build_credentials_id ?: null</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>以上就是我们的jenkins对共享库应用场景了，后续也会随着服务器对架构而升级优化改变。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;本次我们不从Jenkins的部署架构和具体配置说明，我们主要围绕 &lt;code&gt;jenkinsfile&lt;/code&gt; 的内部来讲解。&lt;/p&gt;
    
    </summary>
    
    
      <category term="DevOps" scheme="http://blog.crazylaw.cn/categories/DevOps/"/>
    
    
      <category term="DevOps" scheme="http://blog.crazylaw.cn/tags/DevOps/"/>
    
      <category term="Jenkins" scheme="http://blog.crazylaw.cn/tags/Jenkins/"/>
    
  </entry>
  
  <entry>
    <title>【kubernetes】pause容器</title>
    <link href="http://blog.crazylaw.cn/2020/08/25/k8s/pause%E5%AE%B9%E5%99%A8/"/>
    <id>http://blog.crazylaw.cn/2020/08/25/k8s/pause%E5%AE%B9%E5%99%A8/</id>
    <published>2020-08-25T14:53:30.000Z</published>
    <updated>2021-03-20T16:25:01.807Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>k8s中有 <code>init容器</code>的概念，其中一个就是<code>pause容器</code>，<code>pause容器</code>是一个十分重要的概念，贯穿整个<code>Pod</code>的通信。</p><a id="more"></a><h1 id="Pause容器"><a href="#Pause容器" class="headerlink" title="Pause容器"></a>Pause容器</h1><p>Pause容器，又叫Infra容器，本文将探究该容器的作用与原理。</p><p>我们知道在kubelet的配置中有这样一个参数：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">KUBELET_POD_INFRA_CONTAINER=--pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure:latest</span><br></pre></td></tr></table></figure><p>上面是openshift中的配置参数，kubernetes中默认的配置参数是：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">KUBELET_POD_INFRA_CONTAINER=--pod-infra-container-image=gcr.io/google_containers/pause-amd64:3.0</span><br></pre></td></tr></table></figure><p>Pause容器，是可以自己来定义，官方使用的<code>gcr.io/google_containers/pause-amd64:3.0</code>容器的代码见<a href="https://github.com/kubernetes/kubernetes/tree/master/build/pause" target="_blank" rel="noopener">Github</a>，使用C语言编写。</p><h2 id="Pause容器特点"><a href="#Pause容器特点" class="headerlink" title="Pause容器特点"></a>Pause容器特点</h2><ul><li>镜像非常小，目前在700KB左右</li><li>永远处于Pause(暂停)状态</li></ul><h2 id="Pause容器背景"><a href="#Pause容器背景" class="headerlink" title="Pause容器背景"></a>Pause容器背景</h2><p>像 Pod 这样一个东西，本身是一个逻辑概念。那在机器上，它究竟是怎么实现的呢？这就是我们要解释的一个问题。</p><p>既然说 Pod 要解决这个问题，核心就在于如何让一个 Pod 里的多个容器之间最高效的共享某些资源和数据。</p><p>因为容器之间原本是被 Linux Namespace 和 cgroups 隔开的，所以现在实际要解决的是怎么去打破这个隔离，然后共享某些事情和某些信息。这就是 Pod 的设计要解决的核心问题所在。</p><p>所以说具体的解法分为两个部分：网络和存储。</p><p>Pause容器就是为解决Pod中的网络问题而生的。</p><h2 id="Pause容器实现"><a href="#Pause容器实现" class="headerlink" title="Pause容器实现"></a>Pause容器实现</h2><p>Pod 里的多个容器怎么去共享网络？下面是个例子：</p><p>比如说现在有一个 Pod，其中包含了一个容器 A 和一个容器 B，它们两个就要共享 Network Namespace。在 Kubernetes 里的解法是这样的：它会在每个 Pod 里，额外起一个 Infra container 小容器来共享整个 Pod 的 Network Namespace。</p><p>Infra container 是一个非常小的镜像，大概 700KB 左右，是一个C语言写的、永远处于“暂停”状态的容器。由于有了这样一个 Infra container 之后，其他所有容器都会通过 Join Namespace 的方式加入到 Infra container 的 Network Namespace 中。</p><p>所以说一个 Pod 里面的所有容器，它们看到的网络视图是完全一样的。即：它们看到的网络设备、IP地址、Mac地址等等，跟网络相关的信息，其实全是一份，这一份都来自于 Pod 第一次创建的这个 Infra container。这就是 Pod 解决网络共享的一个解法。</p><p>在 Pod 里面，一定有一个 IP 地址，是这个 Pod 的 Network Namespace 对应的地址，也是这个 Infra container 的 IP 地址。所以大家看到的都是一份，而其他所有网络资源，都是一个 Pod 一份，并且被 Pod 中的所有容器共享。这就是 Pod 的网络实现方式。</p><p>由于需要有一个相当于说中间的容器存在，所以整个 Pod 里面，必然是 Infra container 第一个启动。并且整个 Pod 的生命周期是等同于 Infra container 的生命周期的，与容器 A 和 B 是无关的。这也是为什么在 Kubernetes 里面，它是允许去单独更新 Pod 里的某一个镜像的，即：做这个操作，整个 Pod 不会重建，也不会重启，这是非常重要的一个设计。</p><h2 id="Pause容器的作用"><a href="#Pause容器的作用" class="headerlink" title="Pause容器的作用"></a>Pause容器的作用</h2><p>我们检查node节点的时候会发现每个node上都运行了很多的pause容器，例如如下。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ docker ps</span><br><span class="line">CONTAINER ID        IMAGE                                                                                                                    COMMAND                  CREATED             STATUS              PORTS               NAMES</span><br><span class="line">2c7d50f1a7be        docker.io/jimmysong/heapster-grafana-amd64@sha256:d663759b3de86cf62e64a43b021f133c383e8f7b0dc2bdd78115bc95db371c9a       <span class="string">"/run.sh"</span>                3 hours ago         Up 3 hours                              k8s_grafana_monitoring-influxdb-grafana-v4-5697c6b59-76zqs_kube-system_5788a3c5-29c0-11e8-9e88-525400005732_0</span><br><span class="line">5df93dea877a        docker.io/jimmysong/heapster-influxdb-amd64@sha256:a217008b68cb49e8f038c4eeb6029261f02adca81d8eae8c5c01d030361274b8      <span class="string">"influxd --config ..."</span>   3 hours ago         Up 3 hours                              k8s_influxdb_monitoring-influxdb-grafana-v4-5697c6b59-76zqs_kube-system_5788a3c5-29c0-11e8-9e88-525400005732_0</span><br><span class="line">9cec6c0ef583        jimmysong/pause-amd64:3.0                                                                                                <span class="string">"/pause"</span>                 3 hours ago         Up 3 hours                              k8s_POD_monitoring-influxdb-grafana-v4-5697c6b59-76zqs_kube-system_5788a3c5-29c0-11e8-9e88-525400005732_0</span><br><span class="line">54d06e30a4c7        docker.io/jimmysong/kubernetes-dashboard-amd64@sha256:668710d034c4209f8fa9a342db6d8be72b6cb5f1f3f696cee2379b8512330be4   <span class="string">"/dashboard --inse..."</span>   3 hours ago         Up 3 hours                              k8s_kubernetes-dashboard_kubernetes-dashboard-65486f5fdf-lshl7_kube-system_27c414a1-29c0-11e8-9e88-525400005732_0</span><br><span class="line">5a5ef33b0d58        jimmysong/pause-amd64:3.0                                                                                                <span class="string">"/pause"</span>                 3 hours ago         Up 3 hours                              k8s_POD_kubernetes-dashboard-65486f5fdf-lshl7_kube-system_27c414a1-29c0-11e8-9e88-525400005732_0</span><br></pre></td></tr></table></figure><p>kubernetes中的pause容器主要为每个业务容器提供以下功能：</p><ul><li>在pod中担任Linux命名空间共享的基础；</li><li>启用pid命名空间，开启init进程。</li></ul><p>在<a href="https://www.ianlewis.org/en/almighty-pause-container" target="_blank" rel="noopener">The Almighty Pause Container</a>这篇文章中做出了详细的说明，pause容器的作用可以从这个例子中看出，首先见下图：</p><p><img src="/images/k8s/pause-container.png" alt="Pause容器"></p><p>我们首先在节点上运行一个pause容器。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --name pause -p 8880:80 jimmysong/pause-amd64:3.0</span><br></pre></td></tr></table></figure><p>然后再运行一个nginx容器，nginx将为<code>localhost:2368</code>创建一个代理。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$ cat &lt;&lt;EOF &gt;&gt; nginx.conf</span><br><span class="line">error_log stderr;</span><br><span class="line">events &#123; worker_connections  1024; &#125;</span><br><span class="line">http &#123;</span><br><span class="line">    access_log /dev/stdout combined;</span><br><span class="line">    server &#123;</span><br><span class="line">        listen 80 default_server;</span><br><span class="line">        server_name example.com www.example.com;</span><br><span class="line">        location / &#123;</span><br><span class="line">            proxy_pass http://127.0.0.1:2368;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line">$ docker run -d --name nginx -v `<span class="built_in">pwd</span>`/nginx.conf:/etc/nginx/nginx.conf --net=container:pause --ipc=container:pause --pid=container:pause nginx</span><br></pre></td></tr></table></figure><p>突然间发现报错：</p><p><code>docker: Error response from daemon: can&#39;t join IPC of container 90208aca191fd04e86276bf29a1b1ff742f3d04c0c8c01f6cc61c3283909725b: non-shareable IPC (hint: use IpcMode:shareable for the donor container).</code></p><p>因为当前docker容器默认的<code>&quot;default-ipc-mode&quot;: &quot;host&quot;</code>，我们需要切换到<code>&quot;default-ipc-mode&quot;: &quot;shareable&quot;</code>, linux环境下写入到<code>/etc/docker/daemon.json</code>，<code>docker-for-mac</code> 写在配置中。然后重启即可。</p><p>然后再为<a href="https://github.com/TryGhost/Ghost" target="_blank" rel="noopener">ghost</a>创建一个应用容器，这是一款博客软件。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -d --name ghost --net=container:pause --ipc=container:pause --pid=container:pause ghost</span><br></pre></td></tr></table></figure><p>现在访问<a href="http://localhost:8880/" target="_blank" rel="noopener">http://localhost:8880/</a>就可以看到ghost博客的界面了。</p><p><strong>解析</strong></p><p>pause容器将内部的80端口映射到宿主机的8880端口，pause容器在宿主机上设置好了网络namespace后，nginx容器加入到该网络namespace中，我们看到nginx容器启动的时候指定了<code>--net=container:pause</code>，ghost容器同样加入到了该网络namespace中，这样三个容器就共享了网络，互相之间就可以使用<code>localhost</code>直接通信，<code>--ipc=contianer:pause --pid=container:pause</code>就是三个容器处于同一个namespace中，init进程为<code>pause</code>，这时我们进入到ghost容器中查看进程情况。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ps aux</span></span><br><span class="line">USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND</span><br><span class="line">root         1  0.0  0.0   1024     4 ?        Ss   13:49   0:00 /pause</span><br><span class="line">root         5  0.0  0.1  32432  5736 ?        Ss   13:51   0:00 nginx: master p</span><br><span class="line">systemd+     9  0.0  0.0  32980  3304 ?        S    13:51   0:00 nginx: worker p</span><br><span class="line">node        10  0.3  2.0 1254200 83788 ?       Ssl  13:53   0:03 node current/<span class="keyword">in</span></span><br><span class="line">root        79  0.1  0.0   4336   812 pts/0    Ss   14:09   0:00 sh</span><br><span class="line">root        87  0.0  0.0  17500  2080 pts/0    R+   14:10   0:00 ps aux</span><br></pre></td></tr></table></figure><p>在ghost容器中同时可以看到pause和nginx容器的进程，并且pause容器的PID是1。而在kubernetes中容器的PID=1的进程即为容器本身的业务进程。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">➜  k8s docker ps</span><br><span class="line">CONTAINER ID        IMAGE                  COMMAND                  CREATED             STATUS              PORTS                  NAMES</span><br><span class="line">56b408835478        nginx                  &quot;&#x2F;docker-entrypoint.…&quot;   About an hour ago   Up About an hour                           nginx</span><br><span class="line">b4b5ecc6ddf4        ghost                  &quot;docker-entrypoint.s…&quot;   2 hours ago         Up 2 hours                                 ghost</span><br><span class="line">2be4a03fea85        k8s.gcr.io&#x2F;pause:3.2   &quot;&#x2F;pause&quot;                 3 hours ago         Up 3 hours          0.0.0.0:8880-&gt;80&#x2F;tcp   pause</span><br></pre></td></tr></table></figure><p>当关闭<code>pause进程的时候，所有的容器都会停止</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker stop pause</span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://www.ianlewis.org/en/almighty-pause-container" target="_blank" rel="noopener">The Almighty Pause Container</a></li><li><a href="https://o-my-chenjian.com/2017/10/17/The-Pause-Container-Of-Kubernetes/" target="_blank" rel="noopener">Kubernetes之Pause容器</a></li><li><a href="https://edu.aliyun.com/lesson_1651_16895?spm=5176.10731542.0.0.41a620be3s3dmu#_16895" target="_blank" rel="noopener">CNCF&amp;Aliyun云原生课程</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;k8s中有 &lt;code&gt;init容器&lt;/code&gt;的概念，其中一个就是&lt;code&gt;pause容器&lt;/code&gt;，&lt;code&gt;pause容器&lt;/code&gt;是一个十分重要的概念，贯穿整个&lt;code&gt;Pod&lt;/code&gt;的通信。&lt;/p&gt;
    
    </summary>
    
    
      <category term="kubernetes" scheme="http://blog.crazylaw.cn/categories/kubernetes/"/>
    
    
      <category term="kubernetes" scheme="http://blog.crazylaw.cn/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>【爬虫】- scrapy</title>
    <link href="http://blog.crazylaw.cn/2020/08/24/%E7%88%AC%E8%99%AB/Tutorial/"/>
    <id>http://blog.crazylaw.cn/2020/08/24/%E7%88%AC%E8%99%AB/Tutorial/</id>
    <published>2020-08-24T01:46:51.000Z</published>
    <updated>2021-03-20T16:25:01.820Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近在和朋友写一个二手房的项目，需要用到爬虫，这里借助了scrapy来写。顺便整理了scrapy的用法.</p><p><a href="https://github.com/four-seas/source" target="_blank" rel="noopener">fous_seas/sources</a></p><a id="more"></a><h2 id="第一个-Spider"><a href="#第一个-Spider" class="headerlink" title="第一个 Spider"></a>第一个 Spider</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QuotesSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"quotes"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        urls = [</span><br><span class="line">            <span class="string">'http://quotes.toscrape.com/page/1/'</span>,</span><br><span class="line">            <span class="string">'http://quotes.toscrape.com/page/2/'</span>,</span><br><span class="line">        ]</span><br><span class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> urls:</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url=url, callback=self.parse)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        page = response.url.split(<span class="string">"/"</span>)[<span class="number">-2</span>]</span><br><span class="line">        filename = <span class="string">'quotes-%s.html'</span> % page</span><br><span class="line">        <span class="keyword">with</span> open(filename, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(response.body)</span><br><span class="line">        self.log(<span class="string">'Saved file %s'</span> % filename)</span><br></pre></td></tr></table></figure><p>可以看到，我们新建的 QuotesSpider 类是继承自 scrapy.Spider 类的；下面看看其属性和方法的意义.</p><ul><li>name</li></ul><p>是 Spider 的标识符，用于唯一标识该 Spider；它必须在整个项目中是全局唯一的.</p><ul><li>start_requests()</li></ul><p>必须定义并返回一组可以被 Spider 爬取的 Requests，Request 对象由一个 URL 和一个回调函数构成.</p><ul><li>parse()</li></ul><p>就是 Request 对象中的回调方法，用来解析每一个 Request 之后的 Response；所以，parse() 方法就是用来解析返回的内容，通过解析得到的 URL 同样可以创建对应的 Requests 进而继续爬取.</p><p>再来看看具体的实现。</p><ul><li>start_request(self)</li></ul><p>方法分别针对 <a href="http://quotes.toscrape.com/page/1/" target="_blank" rel="noopener">http://quotes.toscrape.com/page/1/</a> 和 <a href="http://quotes.toscrape.com/page/2/" target="_blank" rel="noopener">http://quotes.toscrape.com/page/2/</a> 创建了两个需要被爬取的 Requests 对象；并通过 yield 进行迭代返回；备注，yield 是迭代生成器，是一个 Generator；</p><ul><li>parse(self, response)</li></ul><p>对 Request 的反馈的内容 Response 进行解析，这里的解析的逻辑很简单，就是分别创建两个本地文件，然后将 response.body 的内容放入这两个文件当中。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl quotes</span><br></pre></td></tr></table></figure><p>大致会输出如下内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">2016-12-16 21:24:05 [scrapy.core.engine] INFO: Spider opened</span><br><span class="line">2016-12-16 21:24:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)</span><br><span class="line">2016-12-16 21:24:05 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023</span><br><span class="line">2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (404) &lt;GET http://quotes.toscrape.com/robots.txt&gt; (referer: None)</span><br><span class="line">2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://quotes.toscrape.com/page/1/&gt; (referer: None)</span><br><span class="line">2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://quotes.toscrape.com/page/2/&gt; (referer: None)</span><br><span class="line">2016-12-16 21:24:05 [quotes] DEBUG: Saved file quotes-1.html</span><br><span class="line">2016-12-16 21:24:05 [quotes] DEBUG: Saved file quotes-2.html</span><br><span class="line">2016-12-16 21:24:05 [scrapy.core.engine] INFO: Closing spider (finished)</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>可以看到，通过爬取，我们在本地生成了两个 html 文件 quotes-1.html 和 quotes-2.html</p><h2 id="如何提取"><a href="#如何提取" class="headerlink" title="如何提取"></a>如何提取</h2><h3 id="通过命令行的方式提取"><a href="#通过命令行的方式提取" class="headerlink" title="通过命令行的方式提取"></a>通过命令行的方式提取</h3><p>Scrapy 提供了命令行的方式可以对需要被爬取的内容进行高效的<code>调试</code>，通过使用<code>Scrapy shell</code>进入命令行，然后在命令行中可以快速的对要爬取的内容进行提取；</p><blockquote><p>一定要学会调试！！！这是不能跳过的步骤</p></blockquote><p>我们试着通过 Scrapy shell 来提取下 “<a href="http://quotes.toscrape.com/page/1/&quot;" target="_blank" rel="noopener">http://quotes.toscrape.com/page/1/&quot;</a> 中的数据，通过执行如下命令，进入 shell</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scrapy shell &quot;http:&#x2F;&#x2F;quotes.toscrape.com&#x2F;page&#x2F;1&#x2F;&quot;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 或者</span><br><span class="line"></span><br><span class="line">scrapy shll</span><br><span class="line">fetch(&#39;http:&#x2F;&#x2F;quotes.toscrape.com&#x2F;page&#x2F;1&#x2F;&#39;)</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[ ... Scrapy log here ... ]</span><br><span class="line">2016-09-19 12:09:27 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http:&#x2F;&#x2F;quotes.toscrape.com&#x2F;page&#x2F;1&#x2F;&gt; (referer: None)</span><br><span class="line">[s] Available Scrapy objects:</span><br><span class="line">[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)</span><br><span class="line">[s]   crawler    &lt;scrapy.crawler.Crawler object at 0x7fa91d888c90&gt;</span><br><span class="line">[s]   item       &#123;&#125;</span><br><span class="line">[s]   request    &lt;GET http:&#x2F;&#x2F;quotes.toscrape.com&#x2F;page&#x2F;1&#x2F;&gt;</span><br><span class="line">[s]   response   &lt;200 http:&#x2F;&#x2F;quotes.toscrape.com&#x2F;page&#x2F;1&#x2F;&gt;</span><br><span class="line">[s]   settings   &lt;scrapy.settings.Settings object at 0x7fa91d888c10&gt;</span><br><span class="line">[s]   spider     &lt;DefaultSpider &#39;default&#39; at 0x7fa91c8af990&gt;</span><br><span class="line">[s] Useful shortcuts:</span><br><span class="line">[s]   shelp()           Shell help (print this help)</span><br><span class="line">[s]   fetch(req_or_url) Fetch request (or URL) and update local objects</span><br><span class="line">[s]   view(response)    View response in a browser</span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure><p>这样，我们就进入了 Scrapy shell 的环境，上面显示了连接请求和返回的相关信息，response 返回 status code 200 表示成功返回；</p><h3 id="通过-CSS-标准进行提取"><a href="#通过-CSS-标准进行提取" class="headerlink" title="通过 CSS 标准进行提取"></a>通过 CSS 标准进行提取</h3><p>这里主要是遵循 CSS 标准 <a href="https://www.w3.org/TR/selectors/" target="_blank" rel="noopener">https://www.w3.org/TR/selectors/</a> 来对网页的元素进行提取.</p><h4 id="通过使用-css-选择我们要提取的元素"><a href="#通过使用-css-选择我们要提取的元素" class="headerlink" title="通过使用 css() 选择我们要提取的元素"></a>通过使用 css() 选择我们要提取的元素</h4><p>下面演示一下如何提取元素.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; response.css(&#39;title&#39;)</span><br><span class="line">[&lt;Selector xpath&#x3D;u&#39;descendant-or-self::title&#39; data&#x3D;u&#39;&lt;title&gt;Quotes to Scrape&lt;&#x2F;title&gt;&#39;&gt;]</span><br></pre></td></tr></table></figure><p>可以看到，它通过返回一个类似 SelectorList 的对象成功的获取到了 <a href="http://quotes.toscrape.com/page/1/" target="_blank" rel="noopener">http://quotes.toscrape.com/page/1/</a> 页面中的 <title/> 的信息，该信息是封装在Selector对象中的 data 属性中的.</p><h4 id="提取Selector元素的文本内容，一般有两种方式用来提取"><a href="#提取Selector元素的文本内容，一般有两种方式用来提取" class="headerlink" title="提取Selector元素的文本内容，一般有两种方式用来提取"></a>提取Selector元素的文本内容，一般有两种方式用来提取</h4><ul><li>通过使用 extract() 或者 extract_first() 方法来提取元素的内容；下面演示如何提取 #1 返回的元素 <title/> 中的文本内容 text；</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; response.css(&#39;title::text&#39;).extract_first()</span><br><span class="line">&#39;Quotes to Scrape&#39;</span><br></pre></td></tr></table></figure><ul><li>extract_first() 表示提取返回队列中的第一个 Selector 对象；同样也可以使用如下的方式.</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; response.css(&#39;title::text&#39;)[0].extract()</span><br><span class="line">&#39;Quotes to Scrape&#39;</span><br></pre></td></tr></table></figure><p>不过 extract_first() 方法可以在当页面没有找到的情况下，避免出现<code>IndexError</code>的错误；</p><ul><li>通过 re() 方法来使用正则表达式的方式来进行提取元素的文本内容</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; response.css(&#39;title::text&#39;).re(r&#39;Quotes.*&#39;)</span><br><span class="line">[&#39;Quotes to Scrape&#39;]</span><br><span class="line">&gt;&gt;&gt; response.css(&#39;title::text&#39;).re(r&#39;Q\w+&#39;)</span><br><span class="line">[&#39;Quotes&#39;]</span><br><span class="line">&gt;&gt;&gt; response.css(&#39;title::text&#39;).re(r&#39;(\w+) to (\w+)&#39;)</span><br><span class="line">[&#39;Quotes&#39;, &#39;Scrape&#39;]</span><br></pre></td></tr></table></figure><h3 id="使用-XPath"><a href="#使用-XPath" class="headerlink" title="使用 XPath"></a>使用 XPath</h3><p>除了使用 <code>CSS 标准</code> 来提取元素意外，我们还可以使用 <code>XPath 标准</code>来提取元素，比如:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; response.xpath(&#39;&#x2F;&#x2F;title&#39;)</span><br><span class="line">[&lt;Selector xpath&#x3D;&#39;&#x2F;&#x2F;title&#39; data&#x3D;&#39;&lt;title&gt;Quotes to Scrape&lt;&#x2F;title&gt;&#39;&gt;]</span><br><span class="line">&gt;&gt;&gt; response.xpath(&#39;&#x2F;&#x2F;title&#x2F;text()&#39;).extract_first()</span><br><span class="line">&#39;Quotes to Scrape&#39;</span><br></pre></td></tr></table></figure><p>XPath 比 CSS 的爬取方式更为强大，因为它不仅仅是根据 HTML 的结构元素去进行检索(Navigating)，并且它可以顺带的对文本(text)进行检索；所以它可以支持 CSS 标准不能做到的场景，比如，检索一个 包含文本内容”Next Page”的 link 元素；这就使得通过 XPath 去构建爬虫更为简单.</p><h2 id="数据流设计图"><a href="#数据流设计图" class="headerlink" title="数据流设计图"></a>数据流设计图</h2><p><img src="/images/%E7%88%AC%E8%99%AB/scrapy_architecture.png" alt="架构图"></p><h2 id="Items"><a href="#Items" class="headerlink" title="Items"></a>Items</h2><p>Scrapy 的核心目的就是从非结构化的网页中提取出结构化的数据；默认的，Scrapy 爬虫以 dicts 的形式返回格式化的数据；但是，这里有一个问题，就是 dicts 并不能很好的表示这种结构化数据的结构，而且经常容易出错，转换也麻烦。</p><p>因此，Item 诞生了，它提供了这样一个简单的容器来收集爬取到的数据，并提供非常简便的 API 来声明它的 fields。</p><h3 id="声明-Items"><a href="#声明-Items" class="headerlink" title="声明 Items"></a>声明 Items</h3><p>通过一个简单的 class 和多个 Field 对象来声明 Items 对象；看一个 Product Item 的例子。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line">class Product(scrapy.Item):</span><br><span class="line">    name &#x3D; scrapy.Field()</span><br><span class="line">    price &#x3D; scrapy.Field()</span><br><span class="line">    stock &#x3D; scrapy.Field()</span><br><span class="line">    last_updated &#x3D; scrapy.Field(serializer&#x3D;str)</span><br></pre></td></tr></table></figure><p>需要注意的是，Product 继承自 scrapy.Item 父类.</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;最近在和朋友写一个二手房的项目，需要用到爬虫，这里借助了scrapy来写。顺便整理了scrapy的用法.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/four-seas/source&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;fous_seas/sources&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="爬虫" scheme="http://blog.crazylaw.cn/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://blog.crazylaw.cn/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>【Golang】- pprof性能分析</title>
    <link href="http://blog.crazylaw.cn/2020/08/14/Golang/pprof%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/"/>
    <id>http://blog.crazylaw.cn/2020/08/14/Golang/pprof%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/</id>
    <published>2020-08-14T08:35:51.000Z</published>
    <updated>2021-03-20T16:25:01.799Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近在开发一个golang的大数据服务，发现只要到了业务层性能就急剧下降，为了排查这个原因，使用pprof进行性能分析</p><a id="more"></a><h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><p>在main函数中加入以下代码</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> <span class="string">"runtime/pprof"</span></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line">cpuProfile, _ := os.Create(<span class="string">"cpu_profile"</span>)</span><br><span class="line">pprof.StartCPUProfile(cpuProfile)</span><br><span class="line"><span class="keyword">defer</span> pprof.StopCPUProfile()</span><br><span class="line"><span class="comment">// ...</span></span><br></pre></td></tr></table></figure><p>这里 <code>os.Create(&quot;cpu_profile&quot;)</code> 指定生成的数据文件, 然后 pprof.StartCPUProfile 看名字就知道是开始对 CPU 的使用进行监控. 有开始就有结束, 一般直接跟着 defer pprof.StopCPUProfile() 省的后面忘了. 编译执行一次以后会在目录下生成监控数据并记录到 <code>cpu_profile</code>. 接着就可以使用 pprof 来解读分析这些监控生成的数据.</p><h2 id="CPU-Profiling"><a href="#CPU-Profiling" class="headerlink" title="CPU Profiling"></a>CPU Profiling</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">root@60b1d42d6330:&#x2F;www# go tool pprof cpu_profile</span><br><span class="line">File: main</span><br><span class="line">Build ID: 3d9d75a7fe2c5c4917c59acabfd743a6512d91fa</span><br><span class="line">Type: cpu</span><br><span class="line">Time: Aug 14, 2020 at 8:34am (UTC)</span><br><span class="line">Duration: 205.73ms, Total samples &#x3D; 30ms (14.58%)</span><br><span class="line">Entering interactive mode (type &quot;help&quot; for commands, &quot;o&quot; for options)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;最近在开发一个golang的大数据服务，发现只要到了业务层性能就急剧下降，为了排查这个原因，使用pprof进行性能分析&lt;/p&gt;
    
    </summary>
    
    
      <category term="Golang" scheme="http://blog.crazylaw.cn/categories/Golang/"/>
    
    
      <category term="Golang" scheme="http://blog.crazylaw.cn/tags/Golang/"/>
    
  </entry>
  
  <entry>
    <title>【Golang】- devle</title>
    <link href="http://blog.crazylaw.cn/2020/08/14/Golang/dlv%E5%9C%A8%E7%BA%BF%E8%B0%83%E8%AF%95/"/>
    <id>http://blog.crazylaw.cn/2020/08/14/Golang/dlv%E5%9C%A8%E7%BA%BF%E8%B0%83%E8%AF%95/</id>
    <published>2020-08-14T01:46:51.000Z</published>
    <updated>2021-03-20T16:25:01.799Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>由于我们可能断点调试程序，所以我们需要debug工具，对于golang来说，delve比gdb对go程序更友好。所以，我们今天来看看devle怎么debug程序</p><a id="more"></a><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">go get github.com&#x2F;go-delve&#x2F;delve&#x2F;cmd&#x2F;dlv</span><br></pre></td></tr></table></figure><h2 id="用法"><a href="#用法" class="headerlink" title="用法"></a>用法</h2><p>a）dlv debug</p><p>使用dlv debug可以在main函数文件所在目录直接对main函数进行调试，也可以在根目录以指定包路径的方式对main函数进行调试。</p><p>b）dlv test</p><p>使用dlv test可以对test包进行调试。</p><p>c）dlv attach</p><p>使用dlv attach可以附加到一个已在运行的进程进行调试。</p><p>d）dlv connect</p><p>使用dlv connect可以连接到调试服务器进行调试。</p><p>e）dlv trace</p><p>使用dlv trace可以追踪程序。</p><p>f）dlv exec</p><p>使用dlv exec可以对编译好的二进制进行调试</p><p>创建main.go文件，main函数先通过循初始化一个切片，然后输出切片的内容：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">"fmt"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    nums := <span class="built_in">make</span>([]<span class="keyword">int</span>, <span class="number">5</span>)</span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; <span class="built_in">len</span>(nums); i++ &#123;</span><br><span class="line">        nums[i] = i * i</span><br><span class="line">    &#125;</span><br><span class="line">    fmt.Println(nums)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>命令行进入包所在目录，然后输入<code>dlv debug</code>命令进入调试：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ dlv debug</span><br><span class="line">Type &#39;help&#39; for list of commands.</span><br><span class="line">(dlv)</span><br></pre></td></tr></table></figure><p>输入<code>help</code>命令可以查看到Delve提供的调试命令列表：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">(dlv) help</span><br><span class="line">The following commands are available:</span><br><span class="line"></span><br><span class="line">Running the program:</span><br><span class="line">    call ------------------------ Resumes process, injecting a function call (EXPERIMENTAL!!!)</span><br><span class="line">    continue (alias: c) --------- Run until breakpoint or program termination.</span><br><span class="line">    next (alias: n) ------------- Step over to next source line.</span><br><span class="line">    rebuild --------------------- Rebuild the target executable and restarts it. It does not work if the executable was not built by delve.</span><br><span class="line">    restart (alias: r) ---------- Restart process.</span><br><span class="line">    step (alias: s) ------------- Single step through program.</span><br><span class="line">    step-instruction (alias: si)  Single step a single cpu instruction.</span><br><span class="line">    stepout (alias: so) --------- Step out of the current function.</span><br><span class="line"></span><br><span class="line">Manipulating breakpoints:</span><br><span class="line">    break (alias: b) ------- Sets a breakpoint.</span><br><span class="line">    breakpoints (alias: bp)  Print out info for active breakpoints.</span><br><span class="line">    clear ------------------ Deletes breakpoint.</span><br><span class="line">    clearall --------------- Deletes multiple breakpoints.</span><br><span class="line">    condition (alias: cond)  Set breakpoint condition.</span><br><span class="line">    on --------------------- Executes a command when a breakpoint is hit.</span><br><span class="line">    trace (alias: t) ------- Set tracepoint.</span><br><span class="line"></span><br><span class="line">Viewing program variables and memory:</span><br><span class="line">    args ----------------- Print function arguments.</span><br><span class="line">    display -------------- Print value of an expression every time the program stops.</span><br><span class="line">    examinemem (alias: x)  Examine memory:</span><br><span class="line">    locals --------------- Print local variables.</span><br><span class="line">    print (alias: p) ----- Evaluate an expression.</span><br><span class="line">    regs ----------------- Print contents of CPU registers.</span><br><span class="line">    set ------------------ Changes the value of a variable.</span><br><span class="line">    vars ----------------- Print package variables.</span><br><span class="line">    whatis --------------- Prints type of an expression.</span><br><span class="line"></span><br><span class="line">Listing and switching between threads and goroutines:</span><br><span class="line">    goroutine (alias: gr) -- Shows or changes current goroutine</span><br><span class="line">    goroutines (alias: grs)  List program goroutines.</span><br><span class="line">    thread (alias: tr) ----- Switch to the specified thread.</span><br><span class="line">    threads ---------------- Print out info for every traced thread.</span><br><span class="line"></span><br><span class="line">Viewing the call stack and selecting frames:</span><br><span class="line">    deferred --------- Executes command in the context of a deferred call.</span><br><span class="line">    down ------------- Move the current frame down.</span><br><span class="line">    frame ------------ Set the current frame, or execute command on a different frame.</span><br><span class="line">    stack (alias: bt)  Print stack trace.</span><br><span class="line">    up --------------- Move the current frame up.</span><br><span class="line"></span><br><span class="line">Other commands:</span><br><span class="line">    config --------------------- Changes configuration parameters.</span><br><span class="line">    disassemble (alias: disass)  Disassembler.</span><br><span class="line">    edit (alias: ed) ----------- Open where you are in $DELVE_EDITOR or $EDITOR</span><br><span class="line">    exit (alias: quit | q) ----- Exit the debugger.</span><br><span class="line">    funcs ---------------------- Print list of functions.</span><br><span class="line">    help (alias: h) ------------ Prints the help message.</span><br><span class="line">    libraries ------------------ List loaded dynamic libraries</span><br><span class="line">    list (alias: ls | l) ------- Show source code.</span><br><span class="line">    source --------------------- Executes a file containing a list of delve commands</span><br><span class="line">    sources -------------------- Print list of source files.</span><br><span class="line">    types ---------------------- Print list of types</span><br><span class="line"></span><br><span class="line">Type help followed by a command for full documentation.</span><br></pre></td></tr></table></figure><p>每个Go程序的入口是main.main函数，我们可以用<code>break(b)</code>在此设置一个断点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(dlv) break main.main</span><br><span class="line">Breakpoint 1 set at 0x10ae9b8 for main.main() .&#x2F;main.go:7</span><br></pre></td></tr></table></figure><p>然后通过<code>breakpoints(bp)</code>查看已经设置的所有断点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(dlv) breakpoints</span><br><span class="line">Breakpoint unrecovered-panic at 0x102a380 for runtime.startpanic()</span><br><span class="line">    &#x2F;usr&#x2F;local&#x2F;go&#x2F;src&#x2F;runtime&#x2F;panic.go:588 (0)</span><br><span class="line">        print runtime.curg._panic.arg</span><br><span class="line">Breakpoint 1 at 0x10ae9b8 for main.main() .&#x2F;main.go:7 (0)</span><br></pre></td></tr></table></figure><p>我们发现除了我们自己设置的<code>main.main</code>函数断点外，Delve内部已经为panic异常函数设置了一个断点。</p><p>通过<code>vars</code>命令可以查看全部包级的变量。因为最终的目标程序可能含有大量的全局变量，我们可以通过一个正则参数选择想查看的全局变量：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(dlv) vars main</span><br><span class="line">main.initdone· &#x3D; 2</span><br><span class="line">runtime.main_init_done &#x3D; chan bool 0&#x2F;0</span><br><span class="line">runtime.mainStarted &#x3D; true</span><br><span class="line">(dlv)</span><br></pre></td></tr></table></figure><p>然后就可以通过<code>continue(c)</code>命令让程序运行到下一个断点处：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">(dlv) continue</span><br><span class="line">&gt; main.main() .&#x2F;main.go:7 (hits goroutine(1):1 total:1) (PC: 0x10ae9b8)</span><br><span class="line">     2:</span><br><span class="line">     3: import (</span><br><span class="line">     4:         &quot;fmt&quot;</span><br><span class="line">     5: )</span><br><span class="line">     6:</span><br><span class="line">&#x3D;&gt;   7: func main() &#123;</span><br><span class="line">     8:         nums :&#x3D; make([]int, 5)</span><br><span class="line">     9:         for i :&#x3D; 0; i &lt; len(nums); i++ &#123;</span><br><span class="line">    10:                 nums[i] &#x3D; i * i</span><br><span class="line">    11:         &#125;</span><br><span class="line">    12:         fmt.Println(nums)</span><br><span class="line">(dlv)</span><br></pre></td></tr></table></figure><p>输入<code>next(n)</code>命令单步执行进入main函数内部：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">(dlv) next</span><br><span class="line">&gt; main.main() .&#x2F;main.go:8 (PC: 0x10ae9cf)</span><br><span class="line">     3: import (</span><br><span class="line">     4:         &quot;fmt&quot;</span><br><span class="line">     5: )</span><br><span class="line">     6:</span><br><span class="line">     7: func main() &#123;</span><br><span class="line">&#x3D;&gt;   8:         nums :&#x3D; make([]int, 5)</span><br><span class="line">     9:         for i :&#x3D; 0; i &lt; len(nums); i++ &#123;</span><br><span class="line">    10:                 nums[i] &#x3D; i * i</span><br><span class="line">    11:         &#125;</span><br><span class="line">    12:         fmt.Println(nums)</span><br><span class="line">    13: &#125;</span><br><span class="line">(dlv)</span><br></pre></td></tr></table></figure><p>进入函数之后可以通过<code>args</code>和<code>locals</code>命令查看函数的参数和局部变量：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(dlv) args</span><br><span class="line">(no args)</span><br><span class="line">(dlv) locals</span><br><span class="line">nums &#x3D; []int len: 842350763880, cap: 17491881, nil</span><br></pre></td></tr></table></figure><p>因为main函数没有参数，因此args命令没有任何输出。而locals命令则输出了局部变量nums切片的值：此时切片还未完成初始化，切片的底层指针为nil，长度和容量都是一个随机数值。</p><p>再次输入next命令单步执行后就可以查看到nums切片初始化之后的结果了：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">(dlv) next</span><br><span class="line">&gt; main.main() .&#x2F;main.go:9 (PC: 0x10aea12)</span><br><span class="line">     4:         &quot;fmt&quot;</span><br><span class="line">     5: )</span><br><span class="line">     6:</span><br><span class="line">     7: func main() &#123;</span><br><span class="line">     8:         nums :&#x3D; make([]int, 5)</span><br><span class="line">&#x3D;&gt;   9:         for i :&#x3D; 0; i &lt; len(nums); i++ &#123;</span><br><span class="line">    10:                 nums[i] &#x3D; i * i</span><br><span class="line">    11:         &#125;</span><br><span class="line">    12:         fmt.Println(nums)</span><br><span class="line">    13: &#125;</span><br><span class="line">(dlv) locals</span><br><span class="line">nums &#x3D; []int len: 5, cap: 5, [...]</span><br><span class="line">i &#x3D; 17601536</span><br><span class="line">(dlv)</span><br></pre></td></tr></table></figure><p>此时因为调试器已经到了for语句行，因此局部变量出现了还未初始化的循环迭代变量i。</p><p>下面我们通过组合使用<code>break(b)</code>和<code>condition(cond)</code>命令，在循环内部设置一个条件断点，当循环变量i等于3时断点生效：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(dlv) break main.go:10</span><br><span class="line">Breakpoint 2 set at 0x10aea33 for main.main() .&#x2F;main.go:10</span><br><span class="line">(dlv) condition 2 i&#x3D;&#x3D;3</span><br><span class="line">(dlv)</span><br></pre></td></tr></table></figure><p>然后通过continue执行到刚设置的条件断点，并且输出局部变量：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">(dlv) continue</span><br><span class="line">&gt; main.main() .&#x2F;main.go:10 (hits goroutine(1):1 total:1) (PC: 0x10aea33)</span><br><span class="line">     5: )</span><br><span class="line">     6:</span><br><span class="line">     7: func main() &#123;</span><br><span class="line">     8:         nums :&#x3D; make([]int, 5)</span><br><span class="line">     9:         for i :&#x3D; 0; i &lt; len(nums); i++ &#123;</span><br><span class="line">&#x3D;&gt;  10:                 nums[i] &#x3D; i * i</span><br><span class="line">    11:         &#125;</span><br><span class="line">    12:         fmt.Println(nums)</span><br><span class="line">    13: &#125;</span><br><span class="line">(dlv) locals</span><br><span class="line">nums &#x3D; []int len: 5, cap: 5, [...]</span><br><span class="line">i &#x3D; 3</span><br><span class="line">(dlv) print nums</span><br><span class="line">[]int len: 5, cap: 5, [0,1,4,0,0]</span><br><span class="line">(dlv)</span><br></pre></td></tr></table></figure><p>我们发现当循环变量i等于3时，nums切片的前3个元素已经正确初始化。</p><p>我们还可以通过stack查看当前执行函数的栈帧信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">(dlv) stack</span><br><span class="line">0  0x00000000010aea33 in main.main</span><br><span class="line">   at .&#x2F;main.go:10</span><br><span class="line">1  0x000000000102bd60 in runtime.main</span><br><span class="line">   at &#x2F;usr&#x2F;local&#x2F;go&#x2F;src&#x2F;runtime&#x2F;proc.go:198</span><br><span class="line">2  0x0000000001053bd1 in runtime.goexit</span><br><span class="line">   at &#x2F;usr&#x2F;local&#x2F;go&#x2F;src&#x2F;runtime&#x2F;asm_amd64.s:2361</span><br><span class="line">(dlv)</span><br></pre></td></tr></table></figure><p>或者通过<code>goroutine</code>和<code>goroutines</code>命令查看当前Goroutine相关的信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">(dlv) goroutine</span><br><span class="line">Thread 101686 at .&#x2F;main.go:10</span><br><span class="line">Goroutine 1:</span><br><span class="line">  Runtime: .&#x2F;main.go:10 main.main (0x10aea33)</span><br><span class="line">  User: .&#x2F;main.go:10 main.main (0x10aea33)</span><br><span class="line">  Go: &#x2F;usr&#x2F;local&#x2F;go&#x2F;src&#x2F;runtime&#x2F;asm_amd64.s:258 runtime.rt0_go (0x1051643)</span><br><span class="line">  Start: &#x2F;usr&#x2F;local&#x2F;go&#x2F;src&#x2F;runtime&#x2F;proc.go:109 runtime.main (0x102bb90)</span><br><span class="line">(dlv) goroutines</span><br><span class="line">[4 goroutines]</span><br><span class="line">* Goroutine 1 - User: .&#x2F;main.go:10 main.main (0x10aea33) (thread 101686)</span><br><span class="line">  Goroutine 2 - User: &#x2F;usr&#x2F;local&#x2F;go&#x2F;src&#x2F;runtime&#x2F;proc.go:292 \</span><br><span class="line">                runtime.gopark (0x102c189)</span><br><span class="line">  Goroutine 3 - User: &#x2F;usr&#x2F;local&#x2F;go&#x2F;src&#x2F;runtime&#x2F;proc.go:292 \</span><br><span class="line">                runtime.gopark (0x102c189)</span><br><span class="line">  Goroutine 4 - User: &#x2F;usr&#x2F;local&#x2F;go&#x2F;src&#x2F;runtime&#x2F;proc.go:292 \</span><br><span class="line">                runtime.gopark (0x102c189)</span><br><span class="line">(dlv)</span><br></pre></td></tr></table></figure><p>最后完成调试工作后输入quit命令退出调试器。至此我们已经掌握了Delve调试器器的简单用法。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;由于我们可能断点调试程序，所以我们需要debug工具，对于golang来说，delve比gdb对go程序更友好。所以，我们今天来看看devle怎么debug程序&lt;/p&gt;
    
    </summary>
    
    
      <category term="Golang" scheme="http://blog.crazylaw.cn/categories/Golang/"/>
    
    
      <category term="Golang" scheme="http://blog.crazylaw.cn/tags/Golang/"/>
    
  </entry>
  
  <entry>
    <title>【DevOps】Jenkins-shared-library 实战</title>
    <link href="http://blog.crazylaw.cn/2020/07/10/DevOps/Jenkins-shared-library/"/>
    <id>http://blog.crazylaw.cn/2020/07/10/DevOps/Jenkins-shared-library/</id>
    <published>2020-07-10T09:35:30.000Z</published>
    <updated>2021-03-20T16:25:01.798Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近公司用了 <code>jenkins</code> 做为 <code>CI/CD</code> 的工具，<code>jenkins</code>是什么，我就不和大家详细说明了。</p><p>今天主要讲的是 <code>jenkins</code> 的 <code>pipeline</code>，旧版的 jenkins 我不太了解，但是听说这个 pipeline 是新出的。</p><p>今天要讲的也不单单是<code>pipeline</code>，更是有强力干货<code>share-library</code>相关的内容。</p><a id="more"></a><h2 id="Jenkinsfile"><a href="#Jenkinsfile" class="headerlink" title="Jenkinsfile"></a>Jenkinsfile</h2><p>目前我们的 jenkins 服务用过<code>普通的pipeline</code>和 <code>多分支的pipeline</code>，其主要区别就是<code>多分支pipeline</code>更加灵活一些，能够让我们少做一些逻辑处理。<br>但是这引入了一个问题，那就是我们的<code>jenkins</code>，必须存在与对应的分支中，也因此，pipeline 的流程变成了代码管理，并且由于我们存在多个项目，那么将会有<code>项目数 * 2(master/pre-develop)</code> 那么多的 jenkins 需要管理，于是就导致了我们有很多这种冗余的写法，这还不是最糟糕的，最糟糕的是如果我们需要修改一些通用的内容的时候，就需要一个个去修改，这绝对是灾难级别的需求。</p><h2 id="Shared-library"><a href="#Shared-library" class="headerlink" title="Shared-library"></a>Shared-library</h2><p>幸好，或许 jenkins 已经考虑到了这种情况的出现，给我们提供了<code>shared-library</code>，在共享库的存在下，我们可以写一些自定义的定制功能。但是这不是没有代价的，代价就是你需要了解<code>groovy</code>这门语言，这是<code>java</code>的派生语言，提供了弱类型的语法，让写 groovy 脚本的变得更加轻松简单，但是也因此，groovy 和 java 的语法常常可以混在一起写，显得有点杂乱。</p><p>但是有总比没有好，我们把共享库分成了 2 个库，分别是<code>resource-libraries</code>，<code>jenkins-shard-library</code>。</p><ul><li><code>resource-libraries</code><ul><li>这个资源共享库定义了我们需要部署不同的配置资源信息，主要是对应<code>libraryResource</code>这个 api 来加载资源</li></ul></li><li><code>jenkins-shard-library</code><ul><li>这个共享库定义了我们需要的通用方法，例如部署流程已经需要统一对外的 api</li></ul></li></ul><h3 id="resource-libraries"><a href="#resource-libraries" class="headerlink" title="resource-libraries"></a>resource-libraries</h3><p>目前，我们的资源库目录结构如下，基本不存在 src 目录，这也意味者这个库不存在独特的<code>类</code>代码，只有统一对外开放的<code>vars</code>，整个库对外开放的 api 只有一个<code>mresource_load</code>方法，核心作用就是加载<code>resources</code>目录中的个项目的配置。由于可能会存在不同组之间的相同项目名，所以我们以<code>组/项目</code>为目录切分资源配置文件，文件以<code>yaml</code>格式为统一标准。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">➜  resource-libraries git:(master) tree</span><br><span class="line">.</span><br><span class="line">├── README.MD</span><br><span class="line">├── resources</span><br><span class="line">│   ├── GamePlatform</span><br><span class="line">│   │   └── mcfx-admin.yaml</span><br><span class="line">│   ├── mc-game-admin</span><br><span class="line">│   │   └── om.yaml</span><br><span class="line">│   └── mc-webapp</span><br><span class="line">│       └── testing-tools.yaml</span><br><span class="line">├── tests</span><br><span class="line">│   └── test_mresource_load.groovy</span><br><span class="line">└── vars</span><br><span class="line">    └── mresource_load.groovy</span><br></pre></td></tr></table></figure><p>基本结构如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">➜  resource-libraries git:(master) cat resources&#x2F;mc-webapp&#x2F;testing-tools.yaml</span><br><span class="line">type: &#39;normal&#39;</span><br><span class="line"></span><br><span class="line">git_credentials_id: &#39;basedev-git-account&#39;</span><br><span class="line"></span><br><span class="line">common: &amp;common</span><br><span class="line">  directory: &#39;&#x2F;data&#x2F;webapp&#x2F;testing-tools&#39;</span><br><span class="line">  deploy_ssh_id: &#39;basedev_tp_normal&#39;</span><br><span class="line"></span><br><span class="line">debug:</span><br><span class="line">  address: &#39;192.168.8.29:61618&#39;</span><br><span class="line">  &lt;&lt;: *common</span><br><span class="line">production:</span><br><span class="line">  address: &#39;192.168.8.93:61618&#39;</span><br><span class="line">  &lt;&lt;: *common</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">➜  resource-libraries git:(master) cat resources&#x2F;mc-game-admin&#x2F;om.yaml</span><br><span class="line">type: job_name</span><br><span class="line"></span><br><span class="line">git_credentials_id: &#39;basedev_24_om&#39;</span><br><span class="line">debug_address: &amp;debug_address &#39;192.168.8.47:61618&#39;</span><br><span class="line">release_address: &amp;release_address &#39;192.168.8.47:61618&#39;</span><br><span class="line">deploy_ssh_id: &amp;deploy_ssh_id &#39;basedev_tp_normal&#39;</span><br><span class="line"></span><br><span class="line">basedev-m24-om:</span><br><span class="line">  debug:</span><br><span class="line">    address: *debug_address</span><br><span class="line">    directory: &#39;&#x2F;data&#x2F;m24&#x2F;web&#x2F;om_debug&#39;</span><br><span class="line">    deploy_ssh_id: *deploy_ssh_id</span><br><span class="line">  production:</span><br><span class="line">    address: *release_address</span><br><span class="line">    directory: &#39;&#x2F;data&#x2F;m24&#x2F;web&#x2F;om&#39;</span><br><span class="line">    deploy_ssh_id: *deploy_ssh_id</span><br></pre></td></tr></table></figure><ul><li>(*)type<ul><li>normal 普通模式</li><li>job_name job_name 模式。对于一个代码仓库，需要分项目部署的我们定义为 job_name 模式，这个的好处就是不同的项目可以以同一份配置文件进行不同的配置管理</li></ul></li><li>(*)git_credentials_id<ul><li>这是 checkout 下来的 git 唯一凭证 id</li></ul></li><li>common<ul><li>这个是推荐但是非必须存在的结构，这里存放一些我们在下面定义的一些通用配置来减少冗余</li></ul></li><li>支持的环境<ul><li>debug</li><li>reviews</li><li>production</li></ul></li><li>环境必填的属性如下<ul><li>address (ip+port)<ul><li>支持 string</li><li>支持 list</li></ul></li><li>directory 代码部署目录<ul><li>支持 string</li><li>支持 list</li></ul></li><li>deploy_ssh_id<ul><li>部署代码到服务的 ssh 的唯一凭证 id</li></ul></li></ul></li></ul><p>由以上构成我们的配置文件的格式。</p><h3 id="jenkins-shard-library"><a href="#jenkins-shard-library" class="headerlink" title="jenkins-shard-library"></a>jenkins-shard-library</h3><p>这个共享库直接影响到我们的 jenkinsfile 的写法，这是直接作用与 jenkinsfile 的共享库，先来一个目前 jenkinsfile 的写法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def link &#x3D; &#39;https:&#x2F;&#x2F;xxx.com&#x2F;mc-webapp&#x2F;testing-tools&#39;</span><br><span class="line"></span><br><span class="line">mpipeline&#123;</span><br><span class="line">    repo&#x3D;link</span><br><span class="line">    script&#x3D;this</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">import xxx.jenkins.LaravelPipeline</span><br><span class="line">def larvaelPipeline &#x3D; new LaravelPipeline(this)</span><br><span class="line"></span><br><span class="line">node &#123;</span><br><span class="line">    larvaelPipeline.preBuild().build().debug().reviews().production().execute()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这就是目前的 jenkinsfile 的写法，可以看到这个 jenkinsfile 没有定制的东西，除了一个<code>link</code>仓库链接之外，其他都是公用的内容。<br>接下来说一下<code>jenkinsfile-dsl</code></p><p>这个东西也就是我们在 jenkinsfile 中看到的<code>mpipeline</code>的结构体，这个是由于我们这个库提供的一个<code>dsl结构</code>，并非原始 jenkinsfile 提供的<br>所有的 jenkinsfile 必须存在这个<code>dsl</code>，并且需要写在流程的头部，以确保 jenkinsfile 的初始化对应的事情（checkout+环境变量+关键字加载）</p><p>目前，封装了一个基于<code>laravel</code>项目的 pipeline 操作类，用这个类来进行流程的统一创建管理。最后我们在 <code>node</code>结构中进行流程的 api 调用。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">➜  jenkins-shard-library git:(master)   tree</span><br><span class="line">.</span><br><span class="line">├── Jenkinsfile.example</span><br><span class="line">├── README.MD</span><br><span class="line">├── jars</span><br><span class="line">│   └── groovy-cps-1.1.jar</span><br><span class="line">├── resources</span><br><span class="line">├── src</span><br><span class="line">│   └── xxx</span><br><span class="line">│       └── xxx</span><br><span class="line">│           └── jenkins</span><br><span class="line">│               ├── Constant.groovy</span><br><span class="line">│               ├── Deploy.groovy</span><br><span class="line">│               ├── Git.groovy</span><br><span class="line">│               ├── LaravelPipeline.groovy</span><br><span class="line">│               ├── MPipeline.groovy</span><br><span class="line">│               ├── MetlNotify.groovy</span><br><span class="line">│               ├── Repo.groovy</span><br><span class="line">│               └── SummaryNotify.groovy</span><br><span class="line">├── tests</span><br><span class="line">│   ├── RepoTest.groovy</span><br><span class="line">│   └── classfiles</span><br><span class="line">│       └── xxx</span><br><span class="line">│           └── xxx</span><br><span class="line">│               └── jenkins</span><br><span class="line">│                   ├── Constant.class</span><br><span class="line">│                   ├── Git.class</span><br><span class="line">│                   ├── MPipeline.class</span><br><span class="line">│                   ├── MetlNotify.class</span><br><span class="line">│                   ├── Repo.class</span><br><span class="line">│                   └── SummaryNotify.class</span><br><span class="line">└── vars</span><br><span class="line">    ├── mpipeline.groovy</span><br><span class="line">    └── notify.groovy</span><br></pre></td></tr></table></figure><p>详细的就不多做讲解，只说一些重要的，LaravelPipeline 对外提供的 api 有：</p><ul><li>preBuild()<ul><li>构建前需要做的事情，例如<code>发送通知任务开始</code></li></ul></li><li>build(isParallel = true)<ul><li>构建过程，参数<code>isParallel = true</code>是否并行构建</li></ul></li><li>deployment(def sshId = null)<ul><li>部署 api，主要是把 debug，reviews，production 写在了一起</li></ul></li><li>customDeploy(def closure = {})<ul><li>自定义部署流程，参数是一个回调函数，需要参考共享库的写法</li></ul></li><li>debug(def sshId)<ul><li>如果是对应的分支，那么将会部署到 debug 环境</li></ul></li><li>reviews(def sshId)<ul><li>如果是对应的分支，那么将会部署到 reviews 环境</li></ul></li><li>production(def sshId)<ul><li>如果是对应的分支，那么将会部署到 production 环境</li></ul></li><li>show()<ul><li>这是一个用于查看构建流程的方法，并不会真正运行各个结构的 stage 里面的内容。但是可以看到节点连线图</li></ul></li><li>execute(def config = [show: false])<ul><li>这是一个启动执行的方法，如果传入了参数 show=true，那么该方法等于 show 方法</li></ul></li></ul><p>对于全局关键字的提供有：</p><ul><li>mpipeline<ul><li>用于初始化整个部署流程，其中包含了 checkout 和环境变量等的初始化</li></ul></li><li>notify<ul><li>用于发送通知</li></ul></li></ul><p>最终的效果如下：</p><p><img src="/images/CI-CD/%E6%B5%81%E7%A8%8B%E8%8A%82%E7%82%B9%E5%9B%BE.png" alt="流程节点图"></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;最近公司用了 &lt;code&gt;jenkins&lt;/code&gt; 做为 &lt;code&gt;CI/CD&lt;/code&gt; 的工具，&lt;code&gt;jenkins&lt;/code&gt;是什么，我就不和大家详细说明了。&lt;/p&gt;
&lt;p&gt;今天主要讲的是 &lt;code&gt;jenkins&lt;/code&gt; 的 &lt;code&gt;pipeline&lt;/code&gt;，旧版的 jenkins 我不太了解，但是听说这个 pipeline 是新出的。&lt;/p&gt;
&lt;p&gt;今天要讲的也不单单是&lt;code&gt;pipeline&lt;/code&gt;，更是有强力干货&lt;code&gt;share-library&lt;/code&gt;相关的内容。&lt;/p&gt;
    
    </summary>
    
    
      <category term="DevOps" scheme="http://blog.crazylaw.cn/categories/DevOps/"/>
    
    
      <category term="DevOps" scheme="http://blog.crazylaw.cn/tags/DevOps/"/>
    
  </entry>
  
  <entry>
    <title>【Docker】docker-apline的问题</title>
    <link href="http://blog.crazylaw.cn/2020/06/18/docker/docker-apline%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <id>http://blog.crazylaw.cn/2020/06/18/docker/docker-apline%E7%9A%84%E9%97%AE%E9%A2%98/</id>
    <published>2020-06-18T12:28:30.000Z</published>
    <updated>2021-03-20T16:25:01.805Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>今天领导发现了一个docker的问题，他在用apline作为基础镜像的时候，在公司的服务器上无法<code>ping baidu.com</code></p><p>但是我们却是能在nslookup中解析到域名。</p><a id="more"></a><h2 id="问题1-ping-baidu-com"><a href="#问题1-ping-baidu-com" class="headerlink" title="问题1. ping baidu.com"></a>问题1. ping baidu.com</h2><p>在用apline作为基础镜像的时候，发现无法ping的情况，在我翻阅了一些文献的。说到了docker-apline是以<code>8.8.8.8</code>Google的dns服务作为默认网关。</p><p>又由于我国具有“墙”的特性，所以我们需要指定国内的dns服务器，例如<code>114.114.114.114</code>，但是这些并不能解决我们的问题。</p><p>又有一些网友说是因为我们自己的dns网关服务不正常导致的，但是确实我们的dns服务一直很正常。</p><p>基于了这些都不能解决我们问题点。基于感觉到了没有任何希望。</p><p>但是后来还是找到了一些另类的解决思路。</p><p>例如有一批人提到了<code>/etc/resolve.conf</code>的<code>ndots</code>的参数。</p><p>于是好奇的搜索了一些<code>ndots</code>相关的内容，发现这个是用于<code>优化dns解析速度</code>的参数。</p><h2 id="ndots"><a href="#ndots" class="headerlink" title="ndots"></a>ndots</h2><p><code>options ndots:0</code> 稍微有点麻烦，我查了一些资料。如果需要查找域名是相对域名，且该域名中包含的. 的数目大于或等于 <code>option ndots:${n}</code>命令指定的数，则查询的仅是该域名。否则会依次往传入的域名后追加 search 列表（search 列表在这里没有，详见<code>search</code>）中的后缀，直到解析出ip地址，或者解析完列表中所有后缀才会停止。这里设置成 0，意思就是对于传入的相对域名，只查询该域名。</p><p>一般ndots都是5即可。最高就是15。</p><p>默认：1</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ndots:n</span><br><span class="line">Sets a threshold for the number of dots which must appear in a name given to res_query(3) (see resolver(3)) before an initial absolute query will be made.  The default  for  n  is  1, meaning  that  if  there  are  any  dots  in a name, the name will be tried first as an absolute name before any search list elements are appended to it.  The value for this option is silently capped to 15.</span><br></pre></td></tr></table></figure><p>于是抱着尝试的态度。</p><p>没有加入参数前</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@webapp_public_S1_192.168.8.135_61618_A ~]# docker run -it --rm composer:2.0 ping baidu.com</span><br><span class="line">ping: bad address 'baidu.com'</span><br></pre></td></tr></table></figure><p>加入参数后</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@webapp_public_S1_192.168.8.135_61618_A ~]# docker run -it --dns-opt&#x3D;ndots:5 --rm composer:2.0 ping baidu.com</span><br><span class="line">PING baidu.com (39.156.69.79): 56 data bytes</span><br><span class="line">64 bytes from 39.156.69.79: seq&#x3D;0 ttl&#x3D;43 time&#x3D;37.941 ms</span><br><span class="line">64 bytes from 39.156.69.79: seq&#x3D;1 ttl&#x3D;43 time&#x3D;37.891 ms</span><br></pre></td></tr></table></figure><p> 神奇的一幕出现了，dns服务正常解析了。于是可以确定ndots在解决apline域名解析上有重要的作用。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;今天领导发现了一个docker的问题，他在用apline作为基础镜像的时候，在公司的服务器上无法&lt;code&gt;ping baidu.com&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;但是我们却是能在nslookup中解析到域名。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Docker" scheme="http://blog.crazylaw.cn/categories/Docker/"/>
    
    
      <category term="Docker" scheme="http://blog.crazylaw.cn/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>【大数据】- docker构建基于Hadoop的大数据生态</title>
    <link href="http://blog.crazylaw.cn/2020/06/16/%E5%A4%A7%E6%95%B0%E6%8D%AE/docker%E6%9E%84%E5%BB%BA%E5%9F%BA%E4%BA%8EHadoop%E7%9A%84%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%94%9F%E6%80%81/"/>
    <id>http://blog.crazylaw.cn/2020/06/16/%E5%A4%A7%E6%95%B0%E6%8D%AE/docker%E6%9E%84%E5%BB%BA%E5%9F%BA%E4%BA%8EHadoop%E7%9A%84%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%94%9F%E6%80%81/</id>
    <published>2020-06-16T10:07:40.000Z</published>
    <updated>2021-03-20T16:25:01.815Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>众所周知，hadoop的大数据生态的组件版本对依赖十分的繁杂，对此我们在如果需要用apache开源库来一点点堆起积木，有点繁琐<br>我们这个项目的目的只是为了更快更方便的构建大数据生态环境。<br>所以我们这里选择采用cdh的方式来构建大数据生态，但是由于我们希望在终端就部署好服务，而不需要客户端，所以这里就不选择用cmf</p><a id="more"></a><ul><li><a href="https://github.com/base-big-data" target="_blank" rel="noopener">基于docker的cdh6的大数据生态</a></li></ul><blockquote><p>在这里，你可以找到你需要的东西</p></blockquote><ul><li>Hadoop</li><li>Impala</li><li>Hive</li><li>Kudu</li></ul><p>接下来，我会记录一下我在这个过程中所遇到的一些问题。</p><h2 id="选择底层镜像"><a href="#选择底层镜像" class="headerlink" title="选择底层镜像"></a>选择底层镜像</h2><ul><li><a href="https://github.com/base-big-data/docker-centos-openjdk" target="_blank" rel="noopener">底层操作系统镜像</a></li></ul><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> centos:<span class="number">7</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">LABEL</span><span class="bash"> maintainer=<span class="string">"Caiwenhui &lt;471113744@qq.com&gt;"</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">ENV</span> JAVA_HOME /usr/lib/jvm/java-openjdk</span><br><span class="line"></span><br><span class="line"><span class="comment"># 换阿里云源</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> yum install -y wget;\</span></span><br><span class="line"><span class="bash">  wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo;\</span></span><br><span class="line"><span class="bash">  <span class="comment"># 移除阿里已经不用的域名</span></span></span><br><span class="line">  sed -i <span class="string">'/mirrors.aliyuncs.com/d'</span> /etc/yum.repos.d/CentOS-Base.repo;\</span><br><span class="line">  sed -i <span class="string">'/mirrors.cloud.aliyuncs.com/d'</span> /etc/yum.repos.d/CentOS-Base.repo;\</span><br><span class="line">  yum clean all; \</span><br><span class="line">  yum makecache</span><br><span class="line"></span><br><span class="line"><span class="comment"># # 安装基础必备软件</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> yum update -y; \</span></span><br><span class="line"><span class="bash">  yum intall -y deltarpm;\</span></span><br><span class="line"><span class="bash">  yum install -y java-1.8.0-openjdk-devel unzip curl vim python-setuptools sudo; \</span></span><br><span class="line"><span class="bash">  yum clean all;\</span></span><br><span class="line"><span class="bash">  yum makecache</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> [<span class="string">"/bin/bash"</span>]</span></span><br></pre></td></tr></table></figure><p>在这里，我们基于的是centos7做操作系统镜像，并且在基础上，替换成了阿里云的yum的数据源，并且安装好了jdk8，方便后续部署服务</p><h2 id="选择基础镜像"><a href="#选择基础镜像" class="headerlink" title="选择基础镜像"></a>选择基础镜像</h2><ul><li><a href="https://github.com/base-big-data/docker-cdh6" target="_blank" rel="noopener">基于cdh6的基础镜像</a></li></ul><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> ccinn/centos-openjdk:latest</span><br><span class="line"></span><br><span class="line"><span class="keyword">LABEL</span><span class="bash"> maintainer=<span class="string">"Caiwenhui &lt;471113744@qq.com&gt;"</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">USER</span> root</span><br><span class="line"></span><br><span class="line"><span class="keyword">ENV</span> CDH_VERSION <span class="number">6.3</span>.<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">ADD</span><span class="bash"> cloudera-cdh6.repo /etc/yum.repos.d/</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> rpm --import https://archive.cloudera.com/cdh6/<span class="variable">$CDH_VERSION</span>/redhat7/yum/RPM-GPG-KEY-cloudera;\</span></span><br><span class="line"><span class="bash">  yum makecache</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="bash"> /</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> [<span class="string">"/bin/bash"</span>]</span></span><br></pre></td></tr></table></figure><p>在这里，我们的目录很简单，就是把<code>cloudera-cdh6.repo</code>放在yum的源中，方便我们通过yum安装服务，减少不必要的依赖问题</p><blockquote><p>注意，我们这里选择的hadoop3的版本</p></blockquote><h2 id="构建hadoop镜像"><a href="#构建hadoop镜像" class="headerlink" title="构建hadoop镜像"></a>构建hadoop镜像</h2><ul><li><a href="https://github.com/base-big-data/docker-cdh6-hadoop" target="_blank" rel="noopener">基于基础镜像的hadoop服务</a></li></ul><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> ccinn/cdh6:latest</span><br><span class="line"></span><br><span class="line"><span class="keyword">LABEL</span><span class="bash"> maintainer=<span class="string">"Caiwenhui &lt;471113744@qq.com&gt;"</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">USER</span> root</span><br><span class="line"></span><br><span class="line"><span class="keyword">ADD</span><span class="bash"> support.sh /support.sh</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> <span class="built_in">source</span> /support.sh;\</span></span><br><span class="line"><span class="bash">  loop_exec <span class="string">'yum install -y hadoop'</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="bash"> /</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> [<span class="string">"/bin/bash"</span>]</span></span><br></pre></td></tr></table></figure><h3 id="问题1"><a href="#问题1" class="headerlink" title="问题1"></a>问题1</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Install  1 Package (+52 Dependent packages)</span><br><span class="line"></span><br><span class="line">Total download size: 712 M</span><br><span class="line">Installed size: 856 M</span><br><span class="line">Downloading packages:</span><br><span class="line">https:&#x2F;&#x2F;archive.cloudera.com&#x2F;cdh6&#x2F;6.3.2&#x2F;redhat7&#x2F;yum&#x2F;RPMS&#x2F;x86_64&#x2F;hadoop-hdfs-3.0.0%2Bcdh6.3.2-1605554.el7.x86_64.rpm: [Errno 14] curl#18 - &quot;transfer closed with 23278968 bytes remaining to read&quot;</span><br><span class="line">Trying other mirror.</span><br><span class="line">https:&#x2F;&#x2F;archive.cloudera.com&#x2F;cdh6&#x2F;6.3.2&#x2F;redhat7&#x2F;yum&#x2F;RPMS&#x2F;x86_64&#x2F;hadoop-3.0.0%2Bcdh6.3.2-1605554.el7.x86_64.rpm: [Errno 14] curl#18 - &quot;transfer closed with 40971220 bytes remaining to read&quot;</span><br><span class="line">Trying other mirror.</span><br><span class="line">https:&#x2F;&#x2F;archive.cloudera.com&#x2F;cdh6&#x2F;6.3.2&#x2F;redhat7&#x2F;yum&#x2F;RPMS&#x2F;noarch&#x2F;hive-2.1.1%2Bcdh6.3.2-1605554.el7.noarch.rpm: [Errno 14] curl#18 - &quot;transfer closed with 123435828 bytes remaining to read&quot;</span><br><span class="line">Trying other mirror.</span><br></pre></td></tr></table></figure><p>我这里引入了一个<code>loop_exec</code>的函数，原因是因为国内的网络过慢，以及软件包“过大”，导致yum在安装包的时候，会导致传输中断，使得我们无法“一步”安装到位</p><p>所以这里我写了一个循环调用的函数，因为yum安装过程中是可以断点续传的，所以我利用这个函数来执行多几次这个命令即可，直到安装成功为止</p><blockquote><p>一般我是3次安装完毕</p></blockquote><h3 id="问题2"><a href="#问题2" class="headerlink" title="问题2"></a>问题2</h3><p>我发现原来<code>hadoop</code>包是基础包而已，我们还要安装<code>hadoop-namenode</code>,<code>hadoop-datanode</code>，因此。我又手动在容器中调试了起来。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/bin/hdfs --config /etc/hadoop/conf  namenode</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">[root@25b603a089cc &#x2F;]# &#x2F;usr&#x2F;bin&#x2F;hdfs --config &#x2F;etc&#x2F;hadoop&#x2F;conf  namenode</span><br><span class="line">2020-06-16 10:18:34,126 INFO namenode.NameNode: STARTUP_MSG:</span><br><span class="line">&#x2F;************************************************************</span><br><span class="line">STARTUP_MSG: Starting NameNode</span><br><span class="line">STARTUP_MSG:   host &#x3D; 25b603a089cc&#x2F;172.17.0.3</span><br><span class="line">STARTUP_MSG:   args &#x3D; []</span><br><span class="line">STARTUP_MSG:   version &#x3D; 3.0.0-cdh6.3.2</span><br><span class="line">STARTUP_MSG:   classpath &#x3D; &#x2F;etc&#x2F;hadoop&#x2F;conf:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-lang3-3.7.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-beanutils-1.9.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;log4j-1.2.17.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-logging-1.1.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;re2j-1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jackson-core-asl-1.9.13.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jettison-1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;guava-11.0.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;javax.activation-api-1.2.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;paranamer-2.8.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jetty-util-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;slf4j-log4j12.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jsr311-api-1.1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;azure-data-lake-store-sdk-2.2.9.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jersey-servlet-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jaxb-api-2.2.11.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;metrics-core-3.0.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;nimbus-jose-jwt-4.41.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;javax.servlet-api-3.1.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;protobuf-java-2.5.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-math3-3.1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jersey-core-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-compress-1.18.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;accessors-smart-1.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;woodstox-core-5.0.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jetty-io-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jackson-xc-1.9.13.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jetty-xml-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;curator-recipes-2.12.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jetty-servlet-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-lang-2.6.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;audience-annotations-0.5.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerby-config-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jetty-security-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jetty-server-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-util-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;netty-3.10.6.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-admin-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jersey-json-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jackson-databind-2.9.9.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-core-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jackson-jaxrs-1.9.13.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerby-asn1-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-configuration2-2.1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-crypto-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;httpcore-4.4.6.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;log4j-core-2.8.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;json-smart-2.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;snappy-java-1.1.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jackson-mapper-asl-1.9.13-cloudera.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-simplekdc-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;logredactor-2.0.7.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;avro.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jackson-annotations-2.9.9.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-client-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jersey-server-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;curator-client-2.12.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-cli-1.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jcip-annotations-1.0-1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-server-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerby-xdr-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-common-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;htrace-core4-4.1.0-incubating.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;aws-java-sdk-bundle-1.11.271.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerby-util-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jetty-webapp-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;stax2-api-3.1.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;slf4j-api-1.7.25.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;log4j-api-2.8.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-io-2.6.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerby-pkix-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;curator-framework-2.12.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;gson-2.2.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jsp-api-2.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;xz-1.6.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;zookeeper.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jetty-http-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;wildfly-openssl-1.0.4.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jackson-core-2.9.9.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-identity-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jaxb-impl-2.2.3-1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-net-3.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jul-to-slf4j-1.7.25.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;asm-5.0.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jsch-0.1.54.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jsr305-3.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-collections-3.2.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;httpclient-4.5.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-codec-1.11.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-format-javadoc.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-auth-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-jackson.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-azure-datalake.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-format.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-aws-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-azure-datalake-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-protobuf.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-annotations-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-auth.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-aws.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-azure.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-scala_2.11.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-nfs.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-annotations.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-pig.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-pig-bundle.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-common.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-hadoop-bundle.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-kms.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-kms-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-cascading.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-common-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-generator.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-azure-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-hadoop.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-common.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-common-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-avro.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-common-3.0.0-cdh6.3.2-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-format-sources.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-column.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-thrift.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-encoding.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-nfs-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-cascading3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-lang3-3.7.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-beanutils-1.9.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;log4j-1.2.17.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-logging-1.1.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;re2j-1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jackson-core-asl-1.9.13.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jettison-1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;guava-11.0.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;javax.activation-api-1.2.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;paranamer-2.8.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-util-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jsr311-api-1.1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jersey-servlet-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jaxb-api-2.2.11.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;nimbus-jose-jwt-4.41.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;javax.servlet-api-3.1.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;protobuf-java-2.5.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;okio-1.6.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-math3-3.1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jersey-core-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-compress-1.18.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;accessors-smart-1.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;woodstox-core-5.0.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-io-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jackson-xc-1.9.13.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-xml-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;curator-recipes-2.12.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-servlet-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-lang-2.6.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;json-simple-1.1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;avro-1.8.2-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;audience-annotations-0.5.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerby-config-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-security-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-server-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-util-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;zookeeper-3.4.5-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;netty-3.10.6.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-admin-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jersey-json-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jackson-databind-2.9.9.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-core-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jackson-jaxrs-1.9.13.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerby-asn1-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-configuration2-2.1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-crypto-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;httpcore-4.4.6.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;json-smart-2.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;snappy-java-1.1.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jackson-mapper-asl-1.9.13-cloudera.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-simplekdc-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jackson-annotations-2.9.9.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-client-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jersey-server-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;curator-client-2.12.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-cli-1.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jcip-annotations-1.0-1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-server-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerby-xdr-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-common-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;htrace-core4-4.1.0-incubating.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerby-util-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-webapp-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;stax2-api-3.1.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-io-2.6.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerby-pkix-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;curator-framework-2.12.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-daemon-1.0.13.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;gson-2.2.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;leveldbjni-all-1.8.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;xz-1.6.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-http-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;okhttp-2.7.5.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-util-ajax-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jackson-core-2.9.9.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-identity-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jaxb-impl-2.2.3-1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-net-3.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;asm-5.0.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jsch-0.1.54.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jsr305-3.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-collections-3.2.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;httpclient-4.5.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-codec-1.11.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-native-client.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-client-3.0.0-cdh6.3.2-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-nfs-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-native-client-3.0.0-cdh6.3.2-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-nfs.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-httpfs-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-client-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-3.0.0-cdh6.3.2-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-native-client-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-client-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-httpfs.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-native-client-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-client.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-examples-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-openstack.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-aliyun.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-core.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-extras-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-uploader.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-openstack-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-distcp.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-hs-plugins-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-sls.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-uploader-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-kafka.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-datajoin-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;netty-buffer-4.1.17.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;netty-common-4.1.17.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-jobclient-3.0.0-cdh6.3.2-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-gridmix-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;jdom-1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-jobclient-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-examples.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-kafka-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;netty-codec-http-4.1.17.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-rumen-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-app.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-extras.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-streaming-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;lz4-java-1.5.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;aliyun-sdk-oss-2.8.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-app-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-datajoin.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;azure-storage-5.4.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-hs.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-nativetask-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-jobclient.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-distcp-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-hs-plugins.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-shuffle-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-aliyun-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-common.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;kafka-clients-2.2.1-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;ojalgo-43.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-shuffle.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-gridmix.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-hs-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-jobclient-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;netty-resolver-4.1.17.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-resourceestimator-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-rumen.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;azure-keyvault-core-0.8.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;netty-codec-4.1.17.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-nativetask.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;netty-transport-4.1.17.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-sls-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-resourceestimator.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-archives.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-archives-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-common-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;netty-handler-4.1.17.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-core-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-archive-logs.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;zstd-jni-1.3.8-1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-streaming.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-archive-logs-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;jackson-jaxrs-json-provider-2.9.9.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;guice-servlet-4.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;jackson-module-jaxb-annotations-2.9.9.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;jersey-client-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;objenesis-1.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;guice-4.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;jackson-jaxrs-base-2.9.9.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;metrics-core-3.0.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;java-util-1.9.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;fst-2.50.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;javax.inject-1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;geronimo-jcache_1.0_spec-1.0-alpha-1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;json-io-2.5.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;HikariCP-java7-2.4.12.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;bcprov-jdk15on-1.60.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;ehcache-3.3.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;bcpkix-jdk15on-1.60.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;aopalliance-1.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;jersey-guice-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;mssql-jdbc-6.2.1.jre7.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-applicationhistoryservice-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-registry-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-tests-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-common.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-sharedcachemanager-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-client-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-applicationhistoryservice.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-applications-unmanaged-am-launcher-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-resourcemanager.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-common.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-resourcemanager-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-timeline-pluginstorage.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-common-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-applications-distributedshell.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-router-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-client.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-common-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-web-proxy-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-sharedcachemanager.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-web-proxy.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-router.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-api-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-nodemanager-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-registry.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-applications-distributedshell-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-timeline-pluginstorage-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-api.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-applications-unmanaged-am-launcher.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-nodemanager.jar</span><br><span class="line">STARTUP_MSG:   build &#x3D; http:&#x2F;&#x2F;github.com&#x2F;cloudera&#x2F;hadoop -r 9aff20de3b5ecccf3c19d57f71b214fb4d37ee89; compiled by &#39;jenkins&#39; on 2019-11-08T13:49Z</span><br><span class="line">STARTUP_MSG:   java &#x3D; 1.8.0_252</span><br><span class="line">************************************************************&#x2F;</span><br><span class="line">2020-06-16 10:18:34,143 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]</span><br><span class="line">2020-06-16 10:18:34,296 INFO namenode.NameNode: createNameNode []</span><br><span class="line">2020-06-16 10:18:34,497 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties</span><br><span class="line">2020-06-16 10:18:34,709 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).</span><br><span class="line">2020-06-16 10:18:34,709 INFO impl.MetricsSystemImpl: NameNode metrics system started</span><br><span class="line">2020-06-16 10:18:34,764 INFO namenode.NameNode: fs.defaultFS is file:&#x2F;&#x2F;&#x2F;</span><br><span class="line">2020-06-16 10:18:34,972 ERROR namenode.NameNode: Failed to start namenode.</span><br><span class="line">java.lang.IllegalArgumentException: Invalid URI for NameNode address (check fs.defaultFS): file:&#x2F;&#x2F;&#x2F; has no authority.</span><br><span class="line">at org.apache.hadoop.hdfs.DFSUtilClient.getNNAddress(DFSUtilClient.java:646)</span><br><span class="line">at org.apache.hadoop.hdfs.DFSUtilClient.getNNAddressCheckLogical(DFSUtilClient.java:675)</span><br><span class="line">at org.apache.hadoop.hdfs.DFSUtilClient.getNNAddress(DFSUtilClient.java:637)</span><br><span class="line">at org.apache.hadoop.hdfs.server.namenode.NameNode.getRpcServerAddress(NameNode.java:562)</span><br><span class="line">at org.apache.hadoop.hdfs.server.namenode.NameNode.loginAsNameNodeUser(NameNode.java:693)</span><br><span class="line">at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:713)</span><br><span class="line">at org.apache.hadoop.hdfs.server.namenode.NameNode.&lt;init&gt;(NameNode.java:950)</span><br><span class="line">at org.apache.hadoop.hdfs.server.namenode.NameNode.&lt;init&gt;(NameNode.java:929)</span><br><span class="line">at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1653)</span><br><span class="line">at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1720)</span><br><span class="line">2020-06-16 10:18:34,983 INFO util.ExitUtil: Exiting with status 1: java.lang.IllegalArgumentException: Invalid URI for NameNode address (check fs.defaultFS): file:&#x2F;&#x2F;&#x2F; has no authority.</span><br><span class="line">2020-06-16 10:18:34,988 INFO namenode.NameNode: SHUTDOWN_MSG:</span><br><span class="line">&#x2F;************************************************************</span><br><span class="line">SHUTDOWN_MSG: Shutting down NameNode at 25b603a089cc&#x2F;172.17.0.3</span><br><span class="line">************************************************************&#x2F;</span><br></pre></td></tr></table></figure><p>我以前台的方式启动服务，发现服务并没有启动成功，他告诉我检查<code>fs.defaultFS</code>参数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">[root@25b603a089cc &#x2F;]# cat &#x2F;etc&#x2F;hadoop&#x2F;conf&#x2F;core-site.xml</span><br><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot;?&gt;</span><br><span class="line">&lt;!--</span><br><span class="line">  Licensed to the Apache Software Foundation (ASF) under one or more</span><br><span class="line">  contributor license agreements.  See the NOTICE file distributed with</span><br><span class="line">  this work for additional information regarding copyright ownership.</span><br><span class="line">  The ASF licenses this file to You under the Apache License, Version 2.0</span><br><span class="line">  (the &quot;License&quot;); you may not use this file except in compliance with</span><br><span class="line">  the License.  You may obtain a copy of the License at</span><br><span class="line"></span><br><span class="line">      http:&#x2F;&#x2F;www.apache.org&#x2F;licenses&#x2F;LICENSE-2.0</span><br><span class="line"></span><br><span class="line">  Unless required by applicable law or agreed to in writing, software</span><br><span class="line">  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class="line">  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line">  See the License for the specific language governing permissions and</span><br><span class="line">  limitations under the License.</span><br><span class="line">--&gt;</span><br><span class="line">&lt;?xml-stylesheet type&#x3D;&quot;text&#x2F;xsl&quot; href&#x3D;&quot;configuration.xsl&quot;?&gt;</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure><p>于是我发现原来这个core-site文件默认是什么配置都没有的。于是我需要手动加上一些配置。</p><p>在这里顺带说明一下配置信息。</p><p>具体的参数配置请参见 &gt;&gt; <a href="https://www.stefaanlippens.net/hadoop-3-default-ports.html" target="_blank" rel="noopener">hadoop3-默认配置参数</a> &lt;&lt;&lt;</p><p>在hadoop集群中，需要配置的文件主要包括四个，分别是<code>core-site.xml</code>、<code>hdfs-site.xml</code>、<code>mapred-site.xml</code>和<code>yarn-site.xml</code>，这四个文件分别是对不同组件的配置参数，主要内容如下表所示：</p><table><thead><tr><th>配置文件名</th><th>配置对象</th><th>主要内容</th></tr></thead><tbody><tr><td>core-site.xml</td><td>集群全局参数</td><td>用于定义系统级别的参数，如HDFS  URL、Hadoop的临时目录等</td></tr><tr><td>hdfs-site.xml</td><td>HDFS参数</td><td>如名称节点和数据节点的存放位置、文件副本的个数、文件读取权限等</td></tr><tr><td>mapred-site.xml</td><td>Mapreduce参数</td><td>包括JobHistory Server和应用程序参数两部分，如reduce任务的默认个数、任务所能够使用内存的默认上下限等</td></tr><tr><td>yarn-site.xml</td><td>集群资源管理系统参数</td><td>配置 ResourceManager，NodeManager 的通信端口，web监控端口等</td></tr></tbody></table><p>搭建集群配置时重要参数：</p><p>core-site.xml</p><table><thead><tr><th>参数名</th><th>默认值</th><th>参数解释</th></tr></thead><tbody><tr><td>fs.defaultFS</td><td>file:///</td><td>文件系统主机和端口</td></tr><tr><td>io.file.buffer.size</td><td>4096</td><td>流文件的缓冲区大小</td></tr><tr><td>hadoop.tmp.dir</td><td>/tmp/hadoop-${user.name}</td><td>临时文件夹</td></tr></tbody></table><p>hdfs-site.xml</p><table><thead><tr><th>参数名</th><th>默认值</th><th>参数解释</th></tr></thead><tbody><tr><td>dfs.namenode.secondary.http-address</td><td>0.0.0.0:50090</td><td>定义HDFS对应的HTTP服务器地址和端口</td></tr><tr><td>dfs.namenode.name.dir</td><td>file://${hadoop.tmp.dir}/dfs/name</td><td>定义DFS的名称节点在本地文件系统的位置</td></tr><tr><td>dfs.datanode.data.dir</td><td>file://${hadoop.tmp.dir}/dfs/data</td><td>定义DFS数据节点存储数据块时存储在本地文件系统的位置</td></tr><tr><td>dfs.replication</td><td>3</td><td>缺省的块复制数量</td></tr><tr><td>dfs.webhdfs.enabled</td><td>true</td><td>是否通过http协议读取hdfs文件，如果选是，则集群安全性较差</td></tr></tbody></table><p>mapred-site.xml</p><table><thead><tr><th>参数名</th><th>默认值</th><th>参数解释</th></tr></thead><tbody><tr><td>mapreduce.framework.name</td><td>local</td><td>取值local、classic或yarn其中之一，如果不是yarn，则不会使用YARN集群来实现资源的分配</td></tr><tr><td>mapreduce.jobhistory.address</td><td>0.0.0.0:10020</td><td>定义历史服务器的地址和端口，通过历史服务器查看已经运行完的Mapreduce作业记录</td></tr><tr><td>mapreduce.jobhistory.webapp.address</td><td>0.0.0.0:19888</td><td>定义历史服务器web应用访问的地址和端口</td></tr></tbody></table><p>yarn-site.xml</p><table><thead><tr><th>参数名</th><th>默认值</th><th>参数解释</th></tr></thead><tbody><tr><td>yarn.resourcemanager.address</td><td>0.0.0.0:8032</td><td>ResourceManager 提供给客户端访问的地址。客户端通过该地址向RM提交应用程序，杀死应用程序等</td></tr><tr><td>yarn.resourcemanager.scheduler.address</td><td>0.0.0.0:8030</td><td>ResourceManager提供给ApplicationMaster的访问地址。ApplicationMaster通过该地址向RM申请资源、释放资源等</td></tr><tr><td>yarn.resourcemanager.resource-tracker.address</td><td>0.0.0.0:8031</td><td>ResourceManager 提供给NodeManager的地址。NodeManager通过该地址向RM汇报心跳，领取任务等</td></tr><tr><td>yarn.resourcemanager.webapp.address</td><td>0.0.0.0:8088</td><td>ResourceManager对web 服务提供地址。用户可通过该地址在浏览器中查看集群各类信息</td></tr><tr><td>yarn.nodemanager.aux-services</td><td></td><td>通过该配置项，用户可以自定义一些服务，例如Map-Reduce的shuffle功能就是采用这种方式实现的，这样就可以在NodeManager上扩展自己的服务。</td></tr></tbody></table><p>基于以上信息。</p><p>整理一份配置如下：</p><p>core-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://0.0.0.0:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.hue.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.hue.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>io.compression.codecs<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.io.compress.SnappyCodec<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>hdfs-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///var/lib/hadoop-hdfs/cache/hdfs/dfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- CLOUDERA-BUILD: CDH-64745. --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>cloudera.erasure_coding.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>接着，我们尝试启动服务。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br></pre></td><td class="code"><pre><span class="line">[root@25b603a089cc &#x2F;]# sudo &#x2F;usr&#x2F;bin&#x2F;hdfs --config &#x2F;etc&#x2F;hadoop&#x2F;conf namenode</span><br><span class="line">2020-06-16 15:55:52,251 INFO namenode.NameNode: STARTUP_MSG:</span><br><span class="line">&#x2F;************************************************************</span><br><span class="line">STARTUP_MSG: Starting NameNode</span><br><span class="line">STARTUP_MSG:   host &#x3D; 25b603a089cc&#x2F;172.17.0.3</span><br><span class="line">STARTUP_MSG:   args &#x3D; []</span><br><span class="line">STARTUP_MSG:   version &#x3D; 3.0.0-cdh6.3.2</span><br><span class="line">STARTUP_MSG:   classpath &#x3D; &#x2F;etc&#x2F;hadoop&#x2F;conf:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-lang3-3.7.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-beanutils-1.9.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;log4j-1.2.17.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-logging-1.1.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;re2j-1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jackson-core-asl-1.9.13.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jettison-1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;guava-11.0.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;javax.activation-api-1.2.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;paranamer-2.8.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jetty-util-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;slf4j-log4j12.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jsr311-api-1.1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;azure-data-lake-store-sdk-2.2.9.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jersey-servlet-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jaxb-api-2.2.11.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;metrics-core-3.0.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;nimbus-jose-jwt-4.41.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;javax.servlet-api-3.1.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;protobuf-java-2.5.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-math3-3.1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jersey-core-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-compress-1.18.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;accessors-smart-1.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;woodstox-core-5.0.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jetty-io-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jackson-xc-1.9.13.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jetty-xml-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;curator-recipes-2.12.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jetty-servlet-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-lang-2.6.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;audience-annotations-0.5.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerby-config-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jetty-security-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jetty-server-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-util-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;netty-3.10.6.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-admin-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jersey-json-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jackson-databind-2.9.9.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-core-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jackson-jaxrs-1.9.13.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerby-asn1-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-configuration2-2.1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-crypto-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;httpcore-4.4.6.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;log4j-core-2.8.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;json-smart-2.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;snappy-java-1.1.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jackson-mapper-asl-1.9.13-cloudera.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-simplekdc-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;logredactor-2.0.7.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;avro.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jackson-annotations-2.9.9.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-client-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jersey-server-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;curator-client-2.12.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-cli-1.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jcip-annotations-1.0-1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-server-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerby-xdr-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-common-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;htrace-core4-4.1.0-incubating.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;aws-java-sdk-bundle-1.11.271.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerby-util-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jetty-webapp-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;stax2-api-3.1.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;slf4j-api-1.7.25.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;log4j-api-2.8.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-io-2.6.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerby-pkix-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;curator-framework-2.12.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;gson-2.2.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jsp-api-2.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;xz-1.6.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;zookeeper.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jetty-http-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;wildfly-openssl-1.0.4.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jackson-core-2.9.9.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-identity-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jaxb-impl-2.2.3-1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-net-3.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jul-to-slf4j-1.7.25.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;asm-5.0.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jsch-0.1.54.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jsr305-3.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-collections-3.2.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;httpclient-4.5.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-codec-1.11.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-format-javadoc.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-auth-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-jackson.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-azure-datalake.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-format.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-aws-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-azure-datalake-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-protobuf.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-annotations-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-auth.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-aws.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-azure.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-scala_2.11.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-nfs.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-annotations.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-pig.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-pig-bundle.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-common.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-hadoop-bundle.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-kms.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-kms-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-cascading.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-common-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-generator.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-azure-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-hadoop.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-common.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-common-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-avro.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-common-3.0.0-cdh6.3.2-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-format-sources.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-column.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-thrift.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-encoding.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-nfs-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-cascading3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-lang3-3.7.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-beanutils-1.9.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;log4j-1.2.17.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-logging-1.1.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;re2j-1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jackson-core-asl-1.9.13.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jettison-1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;guava-11.0.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;javax.activation-api-1.2.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;paranamer-2.8.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-util-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jsr311-api-1.1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jersey-servlet-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jaxb-api-2.2.11.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;nimbus-jose-jwt-4.41.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;javax.servlet-api-3.1.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;protobuf-java-2.5.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;okio-1.6.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-math3-3.1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jersey-core-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-compress-1.18.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;accessors-smart-1.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;woodstox-core-5.0.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-io-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jackson-xc-1.9.13.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-xml-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;curator-recipes-2.12.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-servlet-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-lang-2.6.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;json-simple-1.1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;avro-1.8.2-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;audience-annotations-0.5.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerby-config-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-security-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-server-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-util-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;zookeeper-3.4.5-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;netty-3.10.6.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-admin-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jersey-json-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jackson-databind-2.9.9.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-core-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jackson-jaxrs-1.9.13.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerby-asn1-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-configuration2-2.1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-crypto-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;httpcore-4.4.6.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;json-smart-2.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;snappy-java-1.1.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jackson-mapper-asl-1.9.13-cloudera.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-simplekdc-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jackson-annotations-2.9.9.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-client-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jersey-server-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;curator-client-2.12.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-cli-1.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jcip-annotations-1.0-1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-server-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerby-xdr-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-common-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;htrace-core4-4.1.0-incubating.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerby-util-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-webapp-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;stax2-api-3.1.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-io-2.6.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerby-pkix-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;curator-framework-2.12.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-daemon-1.0.13.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;gson-2.2.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;leveldbjni-all-1.8.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;xz-1.6.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-http-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;okhttp-2.7.5.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-util-ajax-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jackson-core-2.9.9.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-identity-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jaxb-impl-2.2.3-1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-net-3.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;asm-5.0.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jsch-0.1.54.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jsr305-3.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-collections-3.2.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;httpclient-4.5.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-codec-1.11.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-native-client.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-client-3.0.0-cdh6.3.2-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-nfs-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-native-client-3.0.0-cdh6.3.2-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-nfs.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-httpfs-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-client-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-3.0.0-cdh6.3.2-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-native-client-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-client-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-httpfs.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-native-client-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-client.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-examples-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-openstack.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-aliyun.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-core.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-extras-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-uploader.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-openstack-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-distcp.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-hs-plugins-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-sls.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-uploader-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-kafka.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-datajoin-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;netty-buffer-4.1.17.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;netty-common-4.1.17.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-jobclient-3.0.0-cdh6.3.2-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-gridmix-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;jdom-1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-jobclient-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-examples.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-kafka-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;netty-codec-http-4.1.17.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-rumen-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-app.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-extras.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-streaming-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;lz4-java-1.5.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;aliyun-sdk-oss-2.8.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-app-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-datajoin.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;azure-storage-5.4.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-hs.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-nativetask-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-jobclient.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-distcp-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-hs-plugins.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-shuffle-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-aliyun-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-common.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;kafka-clients-2.2.1-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;ojalgo-43.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-shuffle.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-gridmix.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-hs-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-jobclient-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;netty-resolver-4.1.17.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-resourceestimator-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-rumen.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;azure-keyvault-core-0.8.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;netty-codec-4.1.17.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-nativetask.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;netty-transport-4.1.17.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-sls-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-resourceestimator.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-archives.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-archives-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-common-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;netty-handler-4.1.17.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-core-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-archive-logs.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;zstd-jni-1.3.8-1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-streaming.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-archive-logs-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;jackson-jaxrs-json-provider-2.9.9.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;guice-servlet-4.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;jackson-module-jaxb-annotations-2.9.9.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;jersey-client-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;objenesis-1.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;guice-4.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;jackson-jaxrs-base-2.9.9.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;metrics-core-3.0.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;java-util-1.9.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;fst-2.50.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;javax.inject-1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;geronimo-jcache_1.0_spec-1.0-alpha-1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;json-io-2.5.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;HikariCP-java7-2.4.12.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;bcprov-jdk15on-1.60.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;ehcache-3.3.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;bcpkix-jdk15on-1.60.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;aopalliance-1.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;jersey-guice-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;mssql-jdbc-6.2.1.jre7.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-applicationhistoryservice-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-registry-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-tests-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-common.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-sharedcachemanager-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-client-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-applicationhistoryservice.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-applications-unmanaged-am-launcher-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-resourcemanager.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-common.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-resourcemanager-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-timeline-pluginstorage.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-common-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-applications-distributedshell.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-router-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-client.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-common-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-web-proxy-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-sharedcachemanager.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-web-proxy.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-router.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-api-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-nodemanager-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-registry.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-applications-distributedshell-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-timeline-pluginstorage-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-api.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-applications-unmanaged-am-launcher.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-nodemanager.jar</span><br><span class="line">STARTUP_MSG:   build &#x3D; http:&#x2F;&#x2F;github.com&#x2F;cloudera&#x2F;hadoop -r 9aff20de3b5ecccf3c19d57f71b214fb4d37ee89; compiled by &#39;jenkins&#39; on 2019-11-08T13:49Z</span><br><span class="line">STARTUP_MSG:   java &#x3D; 1.8.0_252</span><br><span class="line">************************************************************&#x2F;</span><br><span class="line">2020-06-16 15:55:52,271 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]</span><br><span class="line">2020-06-16 15:55:52,438 INFO namenode.NameNode: createNameNode []</span><br><span class="line">2020-06-16 15:55:52,671 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties</span><br><span class="line">2020-06-16 15:55:52,908 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).</span><br><span class="line">2020-06-16 15:55:52,909 INFO impl.MetricsSystemImpl: NameNode metrics system started</span><br><span class="line">2020-06-16 15:55:52,970 INFO namenode.NameNode: fs.defaultFS is hdfs:&#x2F;&#x2F;0.0.0.0:8020</span><br><span class="line">2020-06-16 15:55:52,977 INFO namenode.NameNode: Clients are to use 0.0.0.0:8020 to access this namenode&#x2F;service.</span><br><span class="line">2020-06-16 15:55:53,268 INFO util.JvmPauseMonitor: Starting JVM pause monitor</span><br><span class="line">2020-06-16 15:55:53,317 INFO hdfs.DFSUtil: Starting Web-server for hdfs at: http:&#x2F;&#x2F;0.0.0.0:9870</span><br><span class="line">2020-06-16 15:55:53,363 INFO util.log: Logging initialized @2085ms</span><br><span class="line">2020-06-16 15:55:53,573 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.</span><br><span class="line">2020-06-16 15:55:53,606 INFO http.HttpRequestLog: Http request log for http.requests.namenode is not defined</span><br><span class="line">2020-06-16 15:55:53,629 INFO http.HttpServer2: Added global filter &#39;safety&#39; (class&#x3D;org.apache.hadoop.http.HttpServer2$QuotingInputFilter)</span><br><span class="line">2020-06-16 15:55:53,635 INFO http.HttpServer2: Added filter static_user_filter (class&#x3D;org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs</span><br><span class="line">2020-06-16 15:55:53,635 INFO http.HttpServer2: Added filter static_user_filter (class&#x3D;org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static</span><br><span class="line">2020-06-16 15:55:53,635 INFO http.HttpServer2: Added filter static_user_filter (class&#x3D;org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs</span><br><span class="line">2020-06-16 15:55:53,697 INFO http.HttpServer2: Added filter &#39;org.apache.hadoop.hdfs.web.AuthFilter&#39; (class&#x3D;org.apache.hadoop.hdfs.web.AuthFilter)</span><br><span class="line">2020-06-16 15:55:53,700 INFO http.HttpServer2: addJerseyResourcePackage: packageName&#x3D;org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec&#x3D;&#x2F;webhdfs&#x2F;v1&#x2F;*</span><br><span class="line">2020-06-16 15:55:53,738 INFO http.HttpServer2: Jetty bound to port 9870</span><br><span class="line">2020-06-16 15:55:53,747 INFO server.Server: jetty-9.3.25.v20180904, build timestamp: 2018-09-04T21:11:46Z, git hash: 3ce520221d0240229c862b122d2b06c12a625732</span><br><span class="line">2020-06-16 15:55:53,858 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7bba5817&#123;&#x2F;logs,file:&#x2F;&#x2F;&#x2F;var&#x2F;log&#x2F;hadoop-hdfs&#x2F;,AVAILABLE&#125;</span><br><span class="line">2020-06-16 15:55:53,859 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@75437611&#123;&#x2F;static,file:&#x2F;&#x2F;&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;webapps&#x2F;static&#x2F;,AVAILABLE&#125;</span><br><span class="line">2020-06-16 15:55:54,026 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@20bd8be5&#123;&#x2F;,file:&#x2F;&#x2F;&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;webapps&#x2F;hdfs&#x2F;,AVAILABLE&#125;&#123;&#x2F;hdfs&#125;</span><br><span class="line">2020-06-16 15:55:54,041 INFO server.AbstractConnector: Started ServerConnector@24105dc5&#123;HTTP&#x2F;1.1,[http&#x2F;1.1]&#125;&#123;0.0.0.0:9870&#125;</span><br><span class="line">2020-06-16 15:55:54,041 INFO server.Server: Started @2764ms</span><br><span class="line">2020-06-16 15:55:54,394 WARN namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!</span><br><span class="line">2020-06-16 15:55:54,395 WARN namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!</span><br><span class="line">2020-06-16 15:55:54,533 INFO namenode.FSEditLog: Edit logging is async:true</span><br><span class="line">2020-06-16 15:55:54,560 INFO namenode.FSNamesystem: KeyProvider: null</span><br><span class="line">2020-06-16 15:55:54,563 INFO namenode.FSNamesystem: fsLock is fair: true</span><br><span class="line">2020-06-16 15:55:54,564 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false</span><br><span class="line">2020-06-16 15:55:54,583 INFO namenode.FSNamesystem: fsOwner             &#x3D; root (auth:SIMPLE)</span><br><span class="line">2020-06-16 15:55:54,583 INFO namenode.FSNamesystem: supergroup          &#x3D; supergroup</span><br><span class="line">2020-06-16 15:55:54,583 INFO namenode.FSNamesystem: isPermissionEnabled &#x3D; true</span><br><span class="line">2020-06-16 15:55:54,584 INFO namenode.FSNamesystem: HA Enabled: false</span><br><span class="line">2020-06-16 15:55:54,683 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling</span><br><span class="line">2020-06-16 15:55:54,712 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured&#x3D;1000, counted&#x3D;60, effected&#x3D;1000</span><br><span class="line">2020-06-16 15:55:54,713 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check&#x3D;true</span><br><span class="line">2020-06-16 15:55:54,722 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000</span><br><span class="line">2020-06-16 15:55:54,723 INFO blockmanagement.BlockManager: The block deletion will start around 2020 Jun 16 15:55:54</span><br><span class="line">2020-06-16 15:55:54,727 INFO util.GSet: Computing capacity for map BlocksMap</span><br><span class="line">2020-06-16 15:55:54,727 INFO util.GSet: VM type       &#x3D; 64-bit</span><br><span class="line">2020-06-16 15:55:54,732 INFO util.GSet: 2.0% max memory 876.5 MB &#x3D; 17.5 MB</span><br><span class="line">2020-06-16 15:55:54,732 INFO util.GSet: capacity      &#x3D; 2^21 &#x3D; 2097152 entries</span><br><span class="line">2020-06-16 15:55:54,760 INFO blockmanagement.BlockManager: dfs.block.access.token.enable &#x3D; false</span><br><span class="line">2020-06-16 15:55:54,771 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS</span><br><span class="line">2020-06-16 15:55:54,772 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct &#x3D; 0.9990000128746033</span><br><span class="line">2020-06-16 15:55:54,772 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes &#x3D; 0</span><br><span class="line">2020-06-16 15:55:54,772 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension &#x3D; 30000</span><br><span class="line">2020-06-16 15:55:54,772 INFO blockmanagement.BlockManager: defaultReplication         &#x3D; 3</span><br><span class="line">2020-06-16 15:55:54,772 INFO blockmanagement.BlockManager: maxReplication             &#x3D; 512</span><br><span class="line">2020-06-16 15:55:54,773 INFO blockmanagement.BlockManager: minReplication             &#x3D; 1</span><br><span class="line">2020-06-16 15:55:54,773 INFO blockmanagement.BlockManager: maxReplicationStreams      &#x3D; 2</span><br><span class="line">2020-06-16 15:55:54,773 INFO blockmanagement.BlockManager: redundancyRecheckInterval  &#x3D; 3000ms</span><br><span class="line">2020-06-16 15:55:54,773 INFO blockmanagement.BlockManager: encryptDataTransfer        &#x3D; false</span><br><span class="line">2020-06-16 15:55:54,773 INFO blockmanagement.BlockManager: maxNumBlocksToLog          &#x3D; 1000</span><br><span class="line">2020-06-16 15:55:54,841 INFO namenode.FSDirectory: GLOBAL serial map: bits&#x3D;24 maxEntries&#x3D;16777215</span><br><span class="line">2020-06-16 15:55:54,878 INFO util.GSet: Computing capacity for map INodeMap</span><br><span class="line">2020-06-16 15:55:54,878 INFO util.GSet: VM type       &#x3D; 64-bit</span><br><span class="line">2020-06-16 15:55:54,879 INFO util.GSet: 1.0% max memory 876.5 MB &#x3D; 8.8 MB</span><br><span class="line">2020-06-16 15:55:54,879 INFO util.GSet: capacity      &#x3D; 2^20 &#x3D; 1048576 entries</span><br><span class="line">2020-06-16 15:55:54,880 INFO namenode.FSDirectory: ACLs enabled? false</span><br><span class="line">2020-06-16 15:55:54,880 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true</span><br><span class="line">2020-06-16 15:55:54,880 INFO namenode.FSDirectory: XAttrs enabled? true</span><br><span class="line">2020-06-16 15:55:54,881 INFO namenode.NameNode: Caching file names occurring more than 10 times</span><br><span class="line">2020-06-16 15:55:54,891 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: true, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true</span><br><span class="line">2020-06-16 15:55:54,903 INFO util.GSet: Computing capacity for map cachedBlocks</span><br><span class="line">2020-06-16 15:55:54,903 INFO util.GSet: VM type       &#x3D; 64-bit</span><br><span class="line">2020-06-16 15:55:54,905 INFO util.GSet: 0.25% max memory 876.5 MB &#x3D; 2.2 MB</span><br><span class="line">2020-06-16 15:55:54,905 INFO util.GSet: capacity      &#x3D; 2^18 &#x3D; 262144 entries</span><br><span class="line">2020-06-16 15:55:54,925 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets &#x3D; 10</span><br><span class="line">2020-06-16 15:55:54,925 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users &#x3D; 10</span><br><span class="line">2020-06-16 15:55:54,925 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes &#x3D; 1,5,25</span><br><span class="line">2020-06-16 15:55:54,938 INFO namenode.FSNamesystem: Retry cache on namenode is enabled</span><br><span class="line">2020-06-16 15:55:54,938 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis</span><br><span class="line">2020-06-16 15:55:54,943 INFO util.GSet: Computing capacity for map NameNodeRetryCache</span><br><span class="line">2020-06-16 15:55:54,944 INFO util.GSet: VM type       &#x3D; 64-bit</span><br><span class="line">2020-06-16 15:55:54,944 INFO util.GSet: 0.029999999329447746% max memory 876.5 MB &#x3D; 269.3 KB</span><br><span class="line">2020-06-16 15:55:54,944 INFO util.GSet: capacity      &#x3D; 2^15 &#x3D; 32768 entries</span><br><span class="line">2020-06-16 15:55:54,978 INFO common.Storage: Lock on &#x2F;var&#x2F;lib&#x2F;hadoop-hdfs&#x2F;cache&#x2F;hdfs&#x2F;dfs&#x2F;name&#x2F;in_use.lock acquired by nodename 2029@25b603a089cc</span><br><span class="line">2020-06-16 15:55:55,028 INFO namenode.FileJournalManager: Recovering unfinalized segments in &#x2F;var&#x2F;lib&#x2F;hadoop-hdfs&#x2F;cache&#x2F;hdfs&#x2F;dfs&#x2F;name&#x2F;current</span><br><span class="line">2020-06-16 15:55:55,084 INFO namenode.FileJournalManager: Finalizing edits file &#x2F;var&#x2F;lib&#x2F;hadoop-hdfs&#x2F;cache&#x2F;hdfs&#x2F;dfs&#x2F;name&#x2F;current&#x2F;edits_inprogress_0000000000000000001 -&gt; &#x2F;var&#x2F;lib&#x2F;hadoop-hdfs&#x2F;cache&#x2F;hdfs&#x2F;dfs&#x2F;name&#x2F;current&#x2F;edits_0000000000000000001-0000000000000000001</span><br><span class="line">2020-06-16 15:55:55,121 INFO namenode.FSImage: Planning to load image: FSImageFile(file&#x3D;&#x2F;var&#x2F;lib&#x2F;hadoop-hdfs&#x2F;cache&#x2F;hdfs&#x2F;dfs&#x2F;name&#x2F;current&#x2F;fsimage_0000000000000000000, cpktTxId&#x3D;0000000000000000000)</span><br><span class="line">2020-06-16 15:55:55,282 INFO namenode.FSImageFormatPBINode: Loading 1 INodes.</span><br><span class="line">2020-06-16 15:55:55,349 INFO namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.</span><br><span class="line">2020-06-16 15:55:55,350 INFO namenode.FSImage: Loaded image for txid 0 from &#x2F;var&#x2F;lib&#x2F;hadoop-hdfs&#x2F;cache&#x2F;hdfs&#x2F;dfs&#x2F;name&#x2F;current&#x2F;fsimage_0000000000000000000</span><br><span class="line">2020-06-16 15:55:55,350 INFO namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@5674e1f2 expecting start txid #1</span><br><span class="line">2020-06-16 15:55:55,351 INFO namenode.FSImage: Start loading edits file &#x2F;var&#x2F;lib&#x2F;hadoop-hdfs&#x2F;cache&#x2F;hdfs&#x2F;dfs&#x2F;name&#x2F;current&#x2F;edits_0000000000000000001-0000000000000000001 maxTxnsToRead &#x3D; 9223372036854775807</span><br><span class="line">2020-06-16 15:55:55,355 INFO namenode.RedundantEditLogInputStream: Fast-forwarding stream &#39;&#x2F;var&#x2F;lib&#x2F;hadoop-hdfs&#x2F;cache&#x2F;hdfs&#x2F;dfs&#x2F;name&#x2F;current&#x2F;edits_0000000000000000001-0000000000000000001&#39; to transaction ID 1</span><br><span class="line">2020-06-16 15:55:55,390 INFO namenode.FSImage: Edits file &#x2F;var&#x2F;lib&#x2F;hadoop-hdfs&#x2F;cache&#x2F;hdfs&#x2F;dfs&#x2F;name&#x2F;current&#x2F;edits_0000000000000000001-0000000000000000001 of size 1048576 edits # 1 loaded in 0 seconds</span><br><span class="line">2020-06-16 15:55:55,390 INFO namenode.FSNamesystem: Need to save fs image? false (staleImage&#x3D;false, haEnabled&#x3D;false, isRollingUpgrade&#x3D;false)</span><br><span class="line">2020-06-16 15:55:55,392 INFO namenode.FSEditLog: Starting log segment at 2</span><br><span class="line">2020-06-16 15:55:55,519 INFO namenode.NameCache: initialized with 0 entries 0 lookups</span><br><span class="line">2020-06-16 15:55:55,519 INFO namenode.FSNamesystem: Finished loading FSImage in 569 msecs</span><br><span class="line">2020-06-16 15:55:55,917 INFO namenode.NameNode: RPC server is binding to 0.0.0.0:8020</span><br><span class="line">2020-06-16 15:55:55,939 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler</span><br><span class="line">2020-06-16 15:55:55,965 INFO ipc.Server: Starting Socket Reader #1 for port 8020</span><br><span class="line">2020-06-16 15:55:56,404 INFO namenode.FSNamesystem: Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.</span><br><span class="line">2020-06-16 15:55:56,423 INFO namenode.LeaseManager: Number of blocks under construction: 0</span><br><span class="line">2020-06-16 15:55:56,448 INFO blockmanagement.BlockManager: initializing replication queues</span><br><span class="line">2020-06-16 15:55:56,449 INFO hdfs.StateChange: STATE* Leaving safe mode after 0 secs</span><br><span class="line">2020-06-16 15:55:56,449 INFO hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes</span><br><span class="line">2020-06-16 15:55:56,449 INFO hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks</span><br><span class="line">2020-06-16 15:55:56,469 INFO blockmanagement.BlockManager: Total number of blocks            &#x3D; 0</span><br><span class="line">2020-06-16 15:55:56,469 INFO blockmanagement.BlockManager: Number of invalid blocks          &#x3D; 0</span><br><span class="line">2020-06-16 15:55:56,469 INFO blockmanagement.BlockManager: Number of under-replicated blocks &#x3D; 0</span><br><span class="line">2020-06-16 15:55:56,469 INFO blockmanagement.BlockManager: Number of  over-replicated blocks &#x3D; 0</span><br><span class="line">2020-06-16 15:55:56,470 INFO blockmanagement.BlockManager: Number of blocks being written    &#x3D; 0</span><br><span class="line">2020-06-16 15:55:56,471 INFO hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 22 msec</span><br><span class="line">2020-06-16 15:55:56,536 INFO ipc.Server: IPC Server Responder: starting</span><br><span class="line">2020-06-16 15:55:56,543 INFO ipc.Server: IPC Server listener on 8020: starting</span><br><span class="line">2020-06-16 15:55:56,554 INFO namenode.NameNode: NameNode RPC up at: 0.0.0.0&#x2F;0.0.0.0:8020</span><br><span class="line">2020-06-16 15:55:56,561 INFO namenode.FSNamesystem: Starting services required for active state</span><br><span class="line">2020-06-16 15:55:56,561 INFO namenode.FSDirectory: Initializing quota with 4 thread(s)</span><br><span class="line">2020-06-16 15:55:56,581 INFO namenode.FSDirectory: Quota initialization completed in 19 milliseconds</span><br><span class="line">name space&#x3D;1</span><br><span class="line">storage space&#x3D;0</span><br><span class="line">storage types&#x3D;RAM_DISK&#x3D;0, SSD&#x3D;0, DISK&#x3D;0, ARCHIVE&#x3D;0</span><br><span class="line">2020-06-16 15:55:56,595 INFO blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds</span><br><span class="line">^C2020-06-16 15:56:01,491 ERROR namenode.NameNode: RECEIVED SIGNAL 2: SIGINT</span><br><span class="line">2020-06-16 15:56:01,496 INFO namenode.NameNode: SHUTDOWN_MSG:</span><br><span class="line">&#x2F;************************************************************</span><br><span class="line">SHUTDOWN_MSG: Shutting down NameNode at 25b603a089cc&#x2F;172.17.0.3</span><br><span class="line">************************************************************&#x2F;</span><br></pre></td></tr></table></figure><p>发现namenode启动成功。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@25b603a089cc &#x2F;]# netstat -anp</span><br><span class="line">Active Internet connections (servers and established)</span><br><span class="line">Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID&#x2F;Program name</span><br><span class="line">tcp        0      0 0.0.0.0:9870            0.0.0.0:*               LISTEN      1872&#x2F;java</span><br><span class="line">tcp        0      0 0.0.0.0:8020            0.0.0.0:*               LISTEN      1872&#x2F;java</span><br><span class="line">tcp        0      0 172.17.0.3:36568        151.101.108.167:443     TIME_WAIT   -</span><br><span class="line">tcp        0      0 172.17.0.3:43778        202.104.186.227:80      TIME_WAIT   -</span><br><span class="line">tcp        0      0 172.17.0.3:43774        202.104.186.227:80      TIME_WAIT   -</span><br></pre></td></tr></table></figure><p>并且看到了8020他的rpc端口已经启动，9870就是web的端口。</p><p>接着，我们就可以在浏览器看到服务启动完毕了。</p><p>浏览器输入：<code>http://localhost:9870/</code></p><p><img src="/images/%E5%A4%A7%E6%95%B0%E6%8D%AE/hadoop-namenode.png" alt="hadoop-namenode-webui"></p><p>至此，大数据服务namenode容器安装完毕.</p><p>接着，我们继续部署我们的datanode服务。为了调试方便，我在同一个容器中部署datanode</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y hadoop-hdfs-datanode</span><br></pre></td></tr></table></figure><p>安装完毕之后，再启动服务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@9ba32201bb25 &#x2F;]# netstat -anp</span><br><span class="line">Active Internet connections (servers and established)</span><br><span class="line">Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID&#x2F;Program name</span><br><span class="line">tcp        0      0 0.0.0.0:8020            0.0.0.0:*               LISTEN      917&#x2F;java</span><br><span class="line">tcp        0      0 127.0.0.11:39643        0.0.0.0:*               LISTEN      -</span><br><span class="line">tcp        0      0 0.0.0.0:9864            0.0.0.0:*               LISTEN      990&#x2F;java</span><br><span class="line">tcp        0      0 0.0.0.0:9866            0.0.0.0:*               LISTEN      990&#x2F;java</span><br><span class="line">tcp        0      0 0.0.0.0:9867            0.0.0.0:*               LISTEN      990&#x2F;java</span><br><span class="line">tcp        0      0 127.0.0.1:38411         0.0.0.0:*               LISTEN      990&#x2F;java</span><br><span class="line">tcp        0      0 0.0.0.0:9870            0.0.0.0:*               LISTEN      917&#x2F;java</span><br><span class="line">tcp        0      0 172.24.0.2:42042        113.96.181.216:80       TIME_WAIT   -</span><br><span class="line">tcp        0      0 172.24.0.2:8020         172.24.0.2:58574        ESTABLISHED 917&#x2F;java</span><br><span class="line">tcp        0      0 172.24.0.2:58574        172.24.0.2:8020         ESTABLISHED 990&#x2F;java</span><br><span class="line">udp        0      0 127.0.0.11:54498        0.0.0.0:*</span><br></pre></td></tr></table></figure><p>这里我们可以看到除了8020/9870这2个是namenode的端口之外，其他端口都是datanode节点的端口。</p><p>我们在<code>hdfs-site.xml</code>中添加了如下配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line"> &lt;name&gt;dfs.datanode.data.dir&lt;&#x2F;name&gt;</span><br><span class="line"> &lt;value&gt;file:&#x2F;&#x2F;&#x2F;var&#x2F;lib&#x2F;hadoop-hdfs&#x2F;cache&#x2F;hdfs&#x2F;dfs&#x2F;data&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure><p>再调整dockerfile。最终如下。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">FROM ccinn&#x2F;cdh6:latest</span><br><span class="line"></span><br><span class="line">LABEL maintainer&#x3D;&quot;Caiwenhui &lt;471113744@qq.com&gt;&quot;</span><br><span class="line"></span><br><span class="line">USER root</span><br><span class="line"></span><br><span class="line">ADD support.sh &#x2F;support.sh</span><br><span class="line">ADD conf&#x2F;core-site.xml &#x2F;etc&#x2F;hadoop&#x2F;conf&#x2F;</span><br><span class="line">ADD conf&#x2F;hdfs-site.xml &#x2F;etc&#x2F;hadoop&#x2F;conf&#x2F;</span><br><span class="line">ADD conf&#x2F;mapred-site.xml &#x2F;etc&#x2F;hadoop&#x2F;conf&#x2F;</span><br><span class="line">ADD bin&#x2F;run.sh &#x2F;bin&#x2F;</span><br><span class="line"></span><br><span class="line">RUN source &#x2F;support.sh;\</span><br><span class="line">  loop_exec &#39;yum install -y hadoop-hdfs-namenode hadoop-hdfs-datanode&#39;</span><br><span class="line"></span><br><span class="line">RUN mkdir -p var&#x2F;lib&#x2F;hadoop-hdfs&#x2F;cache&#x2F;hdfs&#x2F;dfs&#x2F;name;\</span><br><span class="line">  mkdir -p var&#x2F;lib&#x2F;hadoop-hdfs&#x2F;cache&#x2F;hdfs&#x2F;dfs&#x2F;data;</span><br><span class="line"></span><br><span class="line">WORKDIR &#x2F;</span><br><span class="line"></span><br><span class="line"># 9870 namenode&#39;s http</span><br><span class="line"># 9864 datanode&#39;s http</span><br><span class="line"></span><br><span class="line">EXPOSE 9870 9864</span><br><span class="line"></span><br><span class="line">CMD [&quot;&#x2F;bin&#x2F;run.sh&quot;]</span><br></pre></td></tr></table></figure><p>浏览器输入：<code>http://localhost:9864/</code></p><p><img src="/images/%E5%A4%A7%E6%95%B0%E6%8D%AE/hadoop-datanode.png" alt="hadoop-datanode-webui"></p><h2 id="构建hive镜像"><a href="#构建hive镜像" class="headerlink" title="构建hive镜像"></a>构建hive镜像</h2><ul><li><a href="https://github.com/base-big-data/docker-cdh6-hive" target="_blank" rel="noopener">基于基础镜像的hive服务</a></li></ul><p>前面，我们说到hadoop的生态组件是个繁杂的依赖关系，版本搞不对，经常会出现服务起不来的问题。</p><p>当我们不想用cdh给我们选择好的组件的时候，记得需要自己梳理好版本关系。</p><p><a href="http://hive.apache.org/downloads.html" target="_blank" rel="noopener">hive下载前的版本依赖说明-查看hadoop和hive版本的关系</a></p><p><a href="https://archive.cloudera.com/cdh6/6.3.2/redhat7/yum/RPMS/noarch/" target="_blank" rel="noopener">由于我们用的cdh6的源，所以我直接yum安装，这里hive的版本为2.1.1</a></p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> ccinn/cdh6:latest</span><br><span class="line"></span><br><span class="line"><span class="keyword">LABEL</span><span class="bash"> maintainer=<span class="string">"Caiwenhui &lt;471113744@qq.com&gt;"</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">USER</span> root</span><br><span class="line"></span><br><span class="line"><span class="keyword">ADD</span><span class="bash"> bin/support.sh /bin/</span></span><br><span class="line"><span class="keyword">ADD</span><span class="bash"> bin/run.sh /bin/</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装元数据存储服务 postgres</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> <span class="built_in">source</span> /bin/support.sh;\</span></span><br><span class="line"><span class="bash">  loop_exec <span class="string">'yum install -y hive hive-metastore postgresql-jdbc'</span> ;\</span></span><br><span class="line"><span class="bash">  ln -s /usr/share/java/postgresql-jdbc.jar /usr/lib/hive/lib/postgresql-jdbc.jar</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">ADD</span><span class="bash"> conf/hive-site.xml /etc/hive/conf/</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="bash"> /</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> [<span class="string">"/bin/run.sh"</span>]</span></span><br></pre></td></tr></table></figure><p>这里我们使用<code>postgresql</code>作为我们的元数据存储服务，所以我们安装了一个<code>postgresql-jdbc</code>，并且需要把jar包放在hive服务加载的环境变量下。</p><h2 id="构建用于hive镜像的postgres镜像"><a href="#构建用于hive镜像的postgres镜像" class="headerlink" title="构建用于hive镜像的postgres镜像"></a>构建用于hive镜像的postgres镜像</h2><ul><li><a href="https://github.com/base-big-data/docker-cdh6-hive-postgresql" target="_blank" rel="noopener">构建用于hive镜像的postgres镜像</a></li></ul><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> postgres:<span class="number">9</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">LABEL</span><span class="bash"> maintainer=<span class="string">"Caiwenhui &lt;471113744@qq.com&gt;"</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> cdh6-hive-postgres /cdh6-hive-postgres</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> init-hive-db.sh /docker-entrypoint-initdb.d/init-user-db.sh</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 因为schema内部用了相对地址加载关联的sql，所以这里的工作目录需要指定为脚本当前目录</span></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="bash"> /cdh6-hive-postgres</span></span><br></pre></td></tr></table></figure><p>整个dockerfile很简单，我们的用户和密码都是使用了了<code>hive</code>。</p><h2 id="构建impala镜像"><a href="#构建impala镜像" class="headerlink" title="构建impala镜像"></a>构建impala镜像</h2><ul><li><a href="https://github.com/base-big-data/docker-cdh6-impala" target="_blank" rel="noopener">构建impala镜像</a></li></ul><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> ccinn/cdh6:latest</span><br><span class="line"></span><br><span class="line"><span class="keyword">LABEL</span><span class="bash"> maintainer=<span class="string">"Caiwenhui &lt;471113744@qq.com&gt;"</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">USER</span> root</span><br><span class="line"></span><br><span class="line"><span class="keyword">ADD</span><span class="bash"> bin/support.sh /bin/</span></span><br><span class="line"><span class="keyword">ADD</span><span class="bash"> bin/run.sh /bin/</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> <span class="built_in">source</span> /bin/support.sh;\</span></span><br><span class="line"><span class="bash">  loop_exec <span class="string">'yum install -y impala impala-server impala-shell impala-catalog impala-state-store'</span> ;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="bash"> /</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> [<span class="string">"/bin/bash"</span>]</span></span><br></pre></td></tr></table></figure><p>在这里，需要注意的是，我们的impala服务启动是有关联顺序问题的。</p><ul><li>1⃣️ impala-state-store</li><li>2⃣️ impala-catalog</li><li>3⃣️ impala-server(impalad)</li></ul><p><img src="/images/%E5%A4%A7%E6%95%B0%E6%8D%AE/impala.png" alt="impala-shell"></p><p>到此，我们整套大数据<code>OLAP</code>的体系设施，算是基本完成了。其实到hive已经算ok了。但是大家其实可以看到，我这里到计算引擎并没有使用<code>hiveserver2</code>。这里到<code>hive只是用了metastore</code>。</p><p>也因此，这个hive到metastore，目前来说仅仅只是<code>为了服务impala</code>用到。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;众所周知，hadoop的大数据生态的组件版本对依赖十分的繁杂，对此我们在如果需要用apache开源库来一点点堆起积木，有点繁琐&lt;br&gt;我们这个项目的目的只是为了更快更方便的构建大数据生态环境。&lt;br&gt;所以我们这里选择采用cdh的方式来构建大数据生态，但是由于我们希望在终端就部署好服务，而不需要客户端，所以这里就不选择用cmf&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://blog.crazylaw.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="大数据，Hadoop" scheme="http://blog.crazylaw.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%EF%BC%8CHadoop/"/>
    
  </entry>
  
  <entry>
    <title>【大数据】- Hadoop 基本操作</title>
    <link href="http://blog.crazylaw.cn/2020/06/12/%E5%A4%A7%E6%95%B0%E6%8D%AE/hadoop%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/"/>
    <id>http://blog.crazylaw.cn/2020/06/12/%E5%A4%A7%E6%95%B0%E6%8D%AE/hadoop%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/</id>
    <published>2020-06-12T01:56:40.000Z</published>
    <updated>2021-03-20T16:25:01.815Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>由于之前一直没有整理过hadoop的内容，或者说整理得比较少，所以觉得有必要</p><a id="more"></a><h2 id="Hadoop-存储-HDFS"><a href="#Hadoop-存储-HDFS" class="headerlink" title="Hadoop 存储 - HDFS"></a>Hadoop 存储 - HDFS</h2><p>Hadoop 的存储系统是 HDFS(Hadoop Distributed File System)分布式文件系统，对外部客户端而言，HDFS 就像一个传统的分级文件系统，可以进行创建、删除、移动或重命名文件或文件夹等操作，与 Linux 文件系统类似。</p><p>但是，Hadoop HDFS 的架构是基于一组特定的节点构建的，名称节点（NameNode，仅一个)，它在 HDFS 内部提供<code>元数据服务</code>；第二名称节点(Secondary NameNode)，名称节点的帮助节点，主要是为了<code>整合元数据操作(注意不是名称节点的备份)</code>；数据节点(DataNode)，它为 HDFS 提供<code>存储块</code>。由于仅有一个 NameNode，因此这是 HDFS 的一个缺点(单点失败，在 Hadoop2.x 后有较大改善)。</p><h3 id="NameNode"><a href="#NameNode" class="headerlink" title="NameNode"></a>NameNode</h3><p>它是一个通常在 HDFS 架构中单独机器上运行的组件，负责管理文件系统名称空间和控制外部客户机的访问。NameNode 决定是否将文件映射到 DataNode 上的复制块上。对于最常见的 3 个复制块，第一个复制块存储在同一机架的不同节点上，最后一个复制块存储在不同机架的某个节点上。</p><h3 id="Secondary-NameNode"><a href="#Secondary-NameNode" class="headerlink" title="Secondary NameNode"></a>Secondary NameNode</h3><p>第二名称节点的作用在于为 HDFS 中的名称节点提供一个 Checkpoint，它只是名称节点的一个<code>助手节点</code>，这也是它在社区内被认为是 Checkpoint Node 的原因。</p><h3 id="DatabNode"><a href="#DatabNode" class="headerlink" title="DatabNode"></a>DatabNode</h3><p>数据节点也是一个通常在 HDFS 架构中的单独机器上运行的组件。Hadoop 集群包含一个 NameNode 和大量 DataNode。数据节点通常以机架的形式组织，机架通过一个交换机将所有系统连接起来。</p><p>数据节点响应来自 HDFS 客户机的<code>读写请求</code>。它们还响应来自 NameNode 的<code>创建</code>、<code>删除</code>和<code>复制块</code>的命令。名称节点依赖来自每个数据节点的定期<code>心跳（heartbeat）消息</code>。每条消息都包含一个<code>块报告</code>，名称节点可以根据这个报告验证块<code>映射和其他文件系统元数据</code>。如果数据节点不能发送心跳消息，名称节点将采取修复措施，<code>重新复制在该节点上丢失的块</code>。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;由于之前一直没有整理过hadoop的内容，或者说整理得比较少，所以觉得有必要&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://blog.crazylaw.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="大数据，Hadoop" scheme="http://blog.crazylaw.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%EF%BC%8CHadoop/"/>
    
  </entry>
  
  <entry>
    <title>【Docker】零碎知识整理</title>
    <link href="http://blog.crazylaw.cn/2020/06/10/docker/docker-%E9%9B%B6%E7%A2%8E%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/"/>
    <id>http://blog.crazylaw.cn/2020/06/10/docker/docker-%E9%9B%B6%E7%A2%8E%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/</id>
    <published>2020-06-10T02:28:30.000Z</published>
    <updated>2021-03-20T16:25:01.805Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>虽然自己对docker有了一定了解了，但是有一些比较零碎对知识点，一直没整理，所以现在用一篇文章来记录下一些零碎点知识点，方便自己查阅</p><a id="more"></a><h2 id="DOCKER-给运行中的容器添加映射端口"><a href="#DOCKER-给运行中的容器添加映射端口" class="headerlink" title="DOCKER 给运行中的容器添加映射端口"></a>DOCKER 给运行中的容器添加映射端口</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ docker ps</span><br><span class="line">CONTAINER ID        IMAGE                             COMMAND                  CREATED             STATUS                PORTS                    NAMES</span><br><span class="line">8eed84e6b307        golang:1.14.2                     "bash"                   3 weeks ago         Up 4 days                                      msource</span><br><span class="line">241a499c7061        garethflowers/svn-server:latest   "/usr/bin/svnserve -…"   5 weeks ago         Up 6 days (healthy)   0.0.0.0:3690-&gt;3690/tcp   svn</span><br></pre></td></tr></table></figure><h2 id="方法1"><a href="#方法1" class="headerlink" title="方法1"></a>方法1</h2><ol><li>把现有的容器commit成一个新的images</li><li>再基于这个images run一个新的容器，这个时候带上端口映射规则即可</li></ol><blockquote><p>优点就是不需要涉及太多底层的东西，遵循docker的默认提供的API操作即可</p></blockquote><h3 id="方法2"><a href="#方法2" class="headerlink" title="方法2"></a>方法2</h3><p>修改底层配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker inspect `svn` | grep IPAddress</span><br></pre></td></tr></table></figure><p>实例：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">    &#123;</span><br><span class="line">        "Id": "241a499c7061a70ce20216d6e2310d851e8dcc41b4072727a16c752c4463a8a5",</span><br><span class="line">        "Created": "2020-05-05T01:40:40.8904321Z",</span><br><span class="line">        "Path": "/usr/bin/svnserve",</span><br><span class="line">        "Args": [</span><br><span class="line">            "--daemon",</span><br><span class="line">            "--foreground",</span><br><span class="line">            "--root",</span><br><span class="line">            "/var/opt/svn"</span><br><span class="line">        ],</span><br><span class="line">        "State": &#123;</span><br><span class="line">            "Status": "running",</span><br><span class="line">            "Running": true,</span><br><span class="line">            "Paused": false,</span><br><span class="line">            "Restarting": false,</span><br><span class="line">            "OOMKilled": false,</span><br><span class="line">            "Dead": false,</span><br><span class="line">            "Pid": 1893,</span><br><span class="line">            "ExitCode": 0,</span><br><span class="line">            "Error": "",</span><br><span class="line">            "StartedAt": "2020-06-04T01:01:15.928217933Z",</span><br><span class="line">            "FinishedAt": "2020-06-04T01:01:13.970497593Z",</span><br><span class="line">            "Health": &#123;</span><br><span class="line">                "Status": "healthy",</span><br><span class="line">                "FailingStreak": 0,</span><br><span class="line">                "Log": [</span><br><span class="line">                    &#123;</span><br><span class="line">                        "Start": "2020-06-10T03:19:51.4613637Z",</span><br><span class="line">                        "End": "2020-06-10T03:19:51.5609883Z",</span><br><span class="line">                        "ExitCode": 0,</span><br><span class="line">                        "Output": "tcp        0      0 0.0.0.0:3690            0.0.0.0:*               LISTEN      \n"</span><br><span class="line">                    &#125;,</span><br><span class="line">                    &#123;</span><br><span class="line">                        "Start": "2020-06-10T03:20:21.5325319Z",</span><br><span class="line">                        "End": "2020-06-10T03:20:21.6352731Z",</span><br><span class="line">                        "ExitCode": 0,</span><br><span class="line">                        "Output": "tcp        0      0 0.0.0.0:3690            0.0.0.0:*               LISTEN      \n"</span><br><span class="line">                    &#125;,</span><br><span class="line">                    &#123;</span><br><span class="line">                        "Start": "2020-06-10T03:20:51.6086611Z",</span><br><span class="line">                        "End": "2020-06-10T03:20:51.7427852Z",</span><br><span class="line">                        "ExitCode": 0,</span><br><span class="line">                        "Output": "tcp        0      0 0.0.0.0:3690            0.0.0.0:*               LISTEN      \n"</span><br><span class="line">                    &#125;,</span><br><span class="line">                    &#123;</span><br><span class="line">                        "Start": "2020-06-10T03:21:21.7182145Z",</span><br><span class="line">                        "End": "2020-06-10T03:21:21.8185606Z",</span><br><span class="line">                        "ExitCode": 0,</span><br><span class="line">                        "Output": "tcp        0      0 0.0.0.0:3690            0.0.0.0:*               LISTEN      \n"</span><br><span class="line">                    &#125;,</span><br><span class="line">                    &#123;</span><br><span class="line">                        "Start": "2020-06-10T03:21:51.7906908Z",</span><br><span class="line">                        "End": "2020-06-10T03:21:51.8788176Z",</span><br><span class="line">                        "ExitCode": 0,</span><br><span class="line">                        "Output": "tcp        0      0 0.0.0.0:3690            0.0.0.0:*               LISTEN      \n"</span><br><span class="line">                    &#125;</span><br><span class="line">                ]</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        "Image": "sha256:cc28899d5b9077f49f22494074e1197ef4af59d0a1ddca636f57c3b1dc3289d3",</span><br><span class="line">        "ResolvConfPath": "/var/lib/docker/containers/241a499c7061a70ce20216d6e2310d851e8dcc41b4072727a16c752c4463a8a5/resolv.conf",</span><br><span class="line">        "HostnamePath": "/var/lib/docker/containers/241a499c7061a70ce20216d6e2310d851e8dcc41b4072727a16c752c4463a8a5/hostname",</span><br><span class="line">        "HostsPath": "/var/lib/docker/containers/241a499c7061a70ce20216d6e2310d851e8dcc41b4072727a16c752c4463a8a5/hosts",</span><br><span class="line">        "LogPath": "/var/lib/docker/containers/241a499c7061a70ce20216d6e2310d851e8dcc41b4072727a16c752c4463a8a5/241a499c7061a70ce20216d6e2310d851e8dcc41b4072727a16c752c4463a8a5-json.log",</span><br><span class="line">        "Name": "/svn",</span><br><span class="line">        "RestartCount": 0,</span><br><span class="line">        "Driver": "overlay2",</span><br><span class="line">        "Platform": "linux",</span><br><span class="line">        "MountLabel": "",</span><br><span class="line">        "ProcessLabel": "",</span><br><span class="line">        "AppArmorProfile": "",</span><br><span class="line">        "ExecIDs": null,</span><br><span class="line">        "HostConfig": &#123;</span><br><span class="line">            "Binds": null,</span><br><span class="line">            "ContainerIDFile": "",</span><br><span class="line">            "LogConfig": &#123;</span><br><span class="line">                "Type": "json-file",</span><br><span class="line">                "Config": &#123;&#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            "NetworkMode": "default",</span><br><span class="line">            "PortBindings": &#123;</span><br><span class="line">                "3690/tcp": [</span><br><span class="line">                    &#123;</span><br><span class="line">                        "HostIp": "",</span><br><span class="line">                        "HostPort": "3690"</span><br><span class="line">                    &#125;</span><br><span class="line">                ]</span><br><span class="line">            &#125;,</span><br><span class="line">            "RestartPolicy": &#123;</span><br><span class="line">                "Name": "always",</span><br><span class="line">                "MaximumRetryCount": 0</span><br><span class="line">            &#125;,</span><br><span class="line">            "AutoRemove": false,</span><br><span class="line">            "VolumeDriver": "",</span><br><span class="line">            "VolumesFrom": null,</span><br><span class="line">            "CapAdd": null,</span><br><span class="line">            "CapDrop": null,</span><br><span class="line">            "Capabilities": null,</span><br><span class="line">            "Dns": [],</span><br><span class="line">            "DnsOptions": [],</span><br><span class="line">            "DnsSearch": [],</span><br><span class="line">            "ExtraHosts": null,</span><br><span class="line">            "GroupAdd": null,</span><br><span class="line">            "IpcMode": "private",</span><br><span class="line">            "Cgroup": "",</span><br><span class="line">            "Links": null,</span><br><span class="line">            "OomScoreAdj": 0,</span><br><span class="line">            "PidMode": "",</span><br><span class="line">            "Privileged": false,</span><br><span class="line">            "PublishAllPorts": false,</span><br><span class="line">            "ReadonlyRootfs": false,</span><br><span class="line">            "SecurityOpt": null,</span><br><span class="line">            "UTSMode": "",</span><br><span class="line">            "UsernsMode": "",</span><br><span class="line">            "ShmSize": 67108864,</span><br><span class="line">            "Runtime": "runc",</span><br><span class="line">            "ConsoleSize": [</span><br><span class="line">                0,</span><br><span class="line">                0</span><br><span class="line">            ],</span><br><span class="line">            "Isolation": "",</span><br><span class="line">            "CpuShares": 0,</span><br><span class="line">            "Memory": 0,</span><br><span class="line">            "NanoCpus": 0,</span><br><span class="line">            "CgroupParent": "",</span><br><span class="line">            "BlkioWeight": 0,</span><br><span class="line">            "BlkioWeightDevice": [],</span><br><span class="line">            "BlkioDeviceReadBps": null,</span><br><span class="line">            "BlkioDeviceWriteBps": null,</span><br><span class="line">            "BlkioDeviceReadIOps": null,</span><br><span class="line">            "BlkioDeviceWriteIOps": null,</span><br><span class="line">            "CpuPeriod": 0,</span><br><span class="line">            "CpuQuota": 0,</span><br><span class="line">            "CpuRealtimePeriod": 0,</span><br><span class="line">            "CpuRealtimeRuntime": 0,</span><br><span class="line">            "CpusetCpus": "",</span><br><span class="line">            "CpusetMems": "",</span><br><span class="line">            "Devices": [],</span><br><span class="line">            "DeviceCgroupRules": null,</span><br><span class="line">            "DeviceRequests": null,</span><br><span class="line">            "KernelMemory": 0,</span><br><span class="line">            "KernelMemoryTCP": 0,</span><br><span class="line">            "MemoryReservation": 0,</span><br><span class="line">            "MemorySwap": 0,</span><br><span class="line">            "MemorySwappiness": null,</span><br><span class="line">            "OomKillDisable": false,</span><br><span class="line">            "PidsLimit": null,</span><br><span class="line">            "Ulimits": null,</span><br><span class="line">            "CpuCount": 0,</span><br><span class="line">            "CpuPercent": 0,</span><br><span class="line">            "IOMaximumIOps": 0,</span><br><span class="line">            "IOMaximumBandwidth": 0,</span><br><span class="line">            "MaskedPaths": [</span><br><span class="line">                "/proc/asound",</span><br><span class="line">                "/proc/acpi",</span><br><span class="line">                "/proc/kcore",</span><br><span class="line">                "/proc/keys",</span><br><span class="line">                "/proc/latency_stats",</span><br><span class="line">                "/proc/timer_list",</span><br><span class="line">                "/proc/timer_stats",</span><br><span class="line">                "/proc/sched_debug",</span><br><span class="line">                "/proc/scsi",</span><br><span class="line">                "/sys/firmware"</span><br><span class="line">            ],</span><br><span class="line">            "ReadonlyPaths": [</span><br><span class="line">                "/proc/bus",</span><br><span class="line">                "/proc/fs",</span><br><span class="line">                "/proc/irq",</span><br><span class="line">                "/proc/sys",</span><br><span class="line">                "/proc/sysrq-trigger"</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        "GraphDriver": &#123;</span><br><span class="line">            "Data": &#123;</span><br><span class="line">                "LowerDir": "/var/lib/docker/overlay2/1e550f38444220f2853beac87489c7d717e24ac60cd9c64fe67e5571d01d7aee-init/diff:/var/lib/docker/overlay2/c9d13c35fbce9cce4b87c4c9f8e3a1fa335c495806c9aa28e9c85b01a7425f6f/diff:/var/lib/docker/overlay2/f0d6ab7fcc03bb7f98dd26452fd413e0db8b5b4fdc491fdef13aa8064f6d2b44/diff:/var/lib/docker/overlay2/9b3dba47a78414d42c011724a52ea1b25df7939c1db9746d8582c9c48ce80643/diff",</span><br><span class="line">                "MergedDir": "/var/lib/docker/overlay2/1e550f38444220f2853beac87489c7d717e24ac60cd9c64fe67e5571d01d7aee/merged",</span><br><span class="line">                "UpperDir": "/var/lib/docker/overlay2/1e550f38444220f2853beac87489c7d717e24ac60cd9c64fe67e5571d01d7aee/diff",</span><br><span class="line">                "WorkDir": "/var/lib/docker/overlay2/1e550f38444220f2853beac87489c7d717e24ac60cd9c64fe67e5571d01d7aee/work"</span><br><span class="line">            &#125;,</span><br><span class="line">            "Name": "overlay2"</span><br><span class="line">        &#125;,</span><br><span class="line">        "Mounts": [</span><br><span class="line">            &#123;</span><br><span class="line">                "Type": "volume",</span><br><span class="line">                "Name": "c3a499500757570063073552298c2c6a2973351692eb91d9e3437944aeb79e6a",</span><br><span class="line">                "Source": "/var/lib/docker/volumes/c3a499500757570063073552298c2c6a2973351692eb91d9e3437944aeb79e6a/_data",</span><br><span class="line">                "Destination": "/var/opt/svn",</span><br><span class="line">                "Driver": "local",</span><br><span class="line">                "Mode": "",</span><br><span class="line">                "RW": true,</span><br><span class="line">                "Propagation": ""</span><br><span class="line">            &#125;</span><br><span class="line">        ],</span><br><span class="line">        "Config": &#123;</span><br><span class="line">            "Hostname": "241a499c7061",</span><br><span class="line">            "Domainname": "",</span><br><span class="line">            "User": "",</span><br><span class="line">            "AttachStdin": false,</span><br><span class="line">            "AttachStdout": false,</span><br><span class="line">            "AttachStderr": false,</span><br><span class="line">            "ExposedPorts": &#123;</span><br><span class="line">                "3690/tcp": &#123;&#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            "Tty": false,</span><br><span class="line">            "OpenStdin": false,</span><br><span class="line">            "StdinOnce": false,</span><br><span class="line">            "Env": [</span><br><span class="line">                "PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"</span><br><span class="line">            ],</span><br><span class="line">            "Cmd": [</span><br><span class="line">                "/usr/bin/svnserve",</span><br><span class="line">                "--daemon",</span><br><span class="line">                "--foreground",</span><br><span class="line">                "--root",</span><br><span class="line">                "/var/opt/svn"</span><br><span class="line">            ],</span><br><span class="line">            "Healthcheck": &#123;</span><br><span class="line">                "Test": [</span><br><span class="line">                    "CMD-SHELL",</span><br><span class="line">                    "netstat -ln | grep 3690 || exit 1"</span><br><span class="line">                ]</span><br><span class="line">            &#125;,</span><br><span class="line">            "Image": "garethflowers/svn-server:latest",</span><br><span class="line">            "Volumes": &#123;</span><br><span class="line">                "/var/opt/svn": &#123;&#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            "WorkingDir": "/var/opt/svn",</span><br><span class="line">            "Entrypoint": null,</span><br><span class="line">            "OnBuild": null,</span><br><span class="line">            "Labels": &#123;</span><br><span class="line">                "org.label-schema.build-date": "2020-02-13T21:46:23Z",</span><br><span class="line">                "org.label-schema.description": "SVN Server",</span><br><span class="line">                "org.label-schema.docker.cmd": "docker run --detach --publish 3690:3690 --volume :/var/opt/svn garethflowers/svn-server",</span><br><span class="line">                "org.label-schema.name": "svn-server",</span><br><span class="line">                "org.label-schema.schema-version": "1.0",</span><br><span class="line">                "org.label-schema.url": "https://subversion.apache.org",</span><br><span class="line">                "org.label-schema.vcs-ref": "d622ac5",</span><br><span class="line">                "org.label-schema.vcs-url": "https://github.com/garethflowers/docker-svn-server",</span><br><span class="line">                "org.label-schema.vendor": "garethflowers",</span><br><span class="line">                "org.label-schema.version": "1.3.3"</span><br><span class="line">            &#125;</span><br><span class="line">            ...</span><br></pre></td></tr></table></figure><p>我们关注一下，这个容器的id。</p><p>如果是linux系统下的，我们可以这样子操作。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">cd /var/lib/docker/241a499c7061a70ce20216d6e2310d851e8dcc41b4072727a16c752c4463a8a5 #这里是CONTAINER ID</span><br><span class="line">vi hostconfig.json</span><br><span class="line">如果之前没有端口映射, 应该有这样的一段:</span><br></pre></td></tr></table></figure><p>“PortBindings”:{“3690/tcp”:[{“HostIp”:””,”HostPort”:”3690”}]}</p><h2 id="格式化后"><a href="#格式化后" class="headerlink" title="格式化后"></a>格式化后</h2><p>“PortBindings”: {<br>    “3690/tcp”: [<br>        {<br>            “HostIp”: “”,<br>            “HostPort”: “3690”<br>        }<br>    ]<br>},</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">增加一个映射, 这样写:</span><br></pre></td></tr></table></figure><p>“PortBindings”:{“3690/tcp”:[{“HostIp”:””,”HostPort”:”3690”}],”3306/tcp”:[{“HostIp”:””,”HostPort”:”3307”}]}</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">前一个数字是容器端口, 后一个是宿主机端口.</span><br><span class="line">而修改现有端口映射更简单, 把端口号改掉就行.</span><br><span class="line"></span><br><span class="line">记得要重启docker服务，否则配置会被覆盖回去</span><br></pre></td></tr></table></figure><p>如果是再mac系统下操作的话，由于<code>docker for mac</code>的本质是通过一个小型虚拟机操作的，所以需要先进入到虚拟机中</p><p>docker for mac 的内容默认在 <code>~/Library/Containers/com.docker.docker/</code></p><p>我们找到 <code>~/Library/Containers/com.docker.docker/Data/vms/0/tty</code></p><p>通过 <code>screen tty</code> 进入到虚拟机，这个时候你就可以看到<code>/var/lib/docker/241a499c7061a70ce20216d6e2310d851e8dcc41b4072727a16c752c4463a8a5</code> 这里路径了</p><blockquote><p>退出到时候记得用<code>control+a k</code>到组合方式退出</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker-desktop:~# cat &#x2F;var&#x2F;lib&#x2F;docker&#x2F;containers&#x2F;241a499c7061a70ce20216d6e2310d8</span><br><span class="line">51e8dcc41b4072727a16c752c4463a8a5&#x2F;hostconfig.json</span><br><span class="line">&#123;&quot;Binds&quot;:null,&quot;ContainerIDFile&quot;:&quot;&quot;,&quot;LogConfig&quot;:&#123;&quot;Type&quot;:&quot;json-file&quot;,&quot;Config&quot;:&#123;&#125;&#125;,&quot;NetworkMode&quot;:&quot;default&quot;,&quot;PortBindings&quot;:&#123;&quot;3690&#x2F;tcp&quot;:[&#123;&quot;HostIp&quot;:&quot;&quot;,&quot;HostPort&quot;:&quot;3690&quot;&#125;]&#125;,&quot;RestartPolicy&quot;:&#123;&quot;Name&quot;:&quot;always&quot;,&quot;MaximumRetryCount&quot;:0&#125;,&quot;AutoRemove&quot;:false,&quot;VolumeDriver&quot;:&quot;&quot;,&quot;VolumesFrom&quot;:null,&quot;CapAdd&quot;:null,&quot;CapDrop&quot;:null,&quot;Capabilities&quot;:null,&quot;Dns&quot;:[],&quot;DnsOptions&quot;:[],&quot;DnsSearch&quot;:[],&quot;ExtraHosts&quot;:null,&quot;GroupAdd&quot;:null,&quot;IpcMode&quot;:&quot;private&quot;,&quot;Cgroup&quot;:&quot;&quot;,&quot;Links&quot;:null,&quot;OomScoreAdj&quot;:0,&quot;PidMode&quot;:&quot;&quot;,&quot;Privileged&quot;:false,&quot;PublishAllPorts&quot;:false,&quot;ReadonlyRootfs&quot;:false,&quot;SecurityOpt&quot;:null,&quot;UTSMode&quot;:&quot;&quot;,&quot;UsernsMode&quot;:&quot;&quot;,&quot;ShmSize&quot;:67108864,&quot;Runtime&quot;:&quot;runc&quot;,&quot;ConsoleSize&quot;:[0,0],&quot;Isolation&quot;:&quot;&quot;,&quot;CpuShares&quot;:0,&quot;Memory&quot;:0,&quot;NanoCpus&quot;:0,&quot;CgroupParent&quot;:&quot;&quot;,&quot;BlkioWeight&quot;:0,&quot;BlkioWeightDevice&quot;:[],&quot;BlkioDeviceReadBps&quot;:null,&quot;BlkioDeviceWriteBps&quot;:null,&quot;BlkioDeviceReadIOps&quot;:null,&quot;BlkioDeviceWriteIOps&quot;:null,&quot;CpuPeriod&quot;:0,&quot;CpuQuota&quot;:0,&quot;CpuRealtimePeriod&quot;:0,&quot;CpuRealtimeRuntime&quot;:0,&quot;CpusetCpus&quot;:&quot;&quot;,&quot;CpusetMems&quot;:&quot;&quot;,&quot;Devices&quot;:[],&quot;DeviceCgroupRules&quot;:null,&quot;DeviceRequests&quot;:null,&quot;KernelMemory&quot;:0,&quot;KernelMemoryTCP&quot;:0,&quot;MemoryReservation&quot;:0,&quot;MemorySwap&quot;:0,&quot;MemorySwappiness&quot;:null,&quot;OomKillDisable&quot;:false,&quot;PidsLimit&quot;:null,&quot;Ulimits&quot;:null,&quot;CpuCount&quot;:0,&quot;CpuPercent&quot;:0,&quot;IOMaximumIOps&quot;:0,&quot;IOMaximumBandwidth&quot;:0,&quot;MaskedPaths&quot;:[&quot;&#x2F;proc&#x2F;asound&quot;,&quot;&#x2F;proc&#x2F;acpi&quot;,&quot;&#x2F;proc&#x2F;kcore&quot;,&quot;&#x2F;proc&#x2F;keys&quot;,&quot;&#x2F;proc&#x2F;latency_stats&quot;,&quot;&#x2F;proc&#x2F;timer_list&quot;,&quot;&#x2F;proc&#x2F;timer_stats&quot;,&quot;&#x2F;proc&#x2F;sched_debug&quot;,&quot;&#x2F;proc&#x2F;scsi&quot;,&quot;&#x2F;sys&#x2F;firmware&quot;],&quot;ReadonlyPaths&quot;:[&quot;&#x2F;proc&#x2F;bus&quot;,&quot;&#x2F;proc&#x2F;fs&quot;,&quot;&#x2F;proc&#x2F;irq&quot;,&quot;&#x2F;proc&#x2F;sys&quot;,&quot;&#x2F;proc&#x2F;sysrq-trigger&quot;]&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;虽然自己对docker有了一定了解了，但是有一些比较零碎对知识点，一直没整理，所以现在用一篇文章来记录下一些零碎点知识点，方便自己查阅&lt;/p&gt;
    
    </summary>
    
    
      <category term="Docker" scheme="http://blog.crazylaw.cn/categories/Docker/"/>
    
    
      <category term="Docker" scheme="http://blog.crazylaw.cn/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>【Golang】- sync.map</title>
    <link href="http://blog.crazylaw.cn/2020/06/02/Golang/sync-map/"/>
    <id>http://blog.crazylaw.cn/2020/06/02/Golang/sync-map/</id>
    <published>2020-06-02T05:59:51.000Z</published>
    <updated>2021-03-20T16:25:01.799Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>由于要造一些轮子，例如 像 laravel 一样的 <code>Event组件</code>，我参考了网上的几个库</p><ul><li><a href="https://github.com/sadlil/go-trigger" target="_blank" rel="noopener">go-trigger</a></li><li><a href="https://github.com/kataras/go-events" target="_blank" rel="noopener">go-events</a></li></ul><p>这 2 个库，大同小异，只有一小部分差别，新打组件将会在这 2 个库的基础上再封装</p><p>这 2 个库都是比较早期的库，所以在实现上用到了 map，但是由于考虑到 map 的线程安全性问题，所以他们都使用了 go1.9 之前实现的方式，就是在结构体嵌入一个读写锁来避免线程安全问题。</p><p>在 Go 1.6 之前， 内置的 map 类型是部分 goroutine 安全的，并发的读没有问题，并发的写可能有问题。自 go 1.6 之后， 并发地读写 map 会报错，这在一些知名的开源库中都存在这个问题，所以 go 1.9 之前的解决方案是额外绑定一个锁，封装成一个新的 struct 或者单独使用锁都可以。</p><p>本文带你深入到 sync.Map 的具体实现中，看看为了增加一个功能，代码是如何变的复杂的,以及作者在实现 sync.Map 的一些思想。</p><a id="more"></a><h2 id="有并发问题的-map"><a href="#有并发问题的-map" class="headerlink" title="有并发问题的 map"></a>有并发问题的 map</h2><p>官方的 <a href="https://golang.org/doc/faq#atomic_maps" target="_blank" rel="noopener">faq</a> 已经提到内建的 map 不是线程(goroutine)安全的。</p><p>首先，让我们看一段并发读写的代码,下列程序中一个 goroutine 一直读，一个 goroutine 一只写同一个键值，即即使读写的键不相同，而且 map 也没有”扩容”等操作，代码还是会报错。</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">m := <span class="built_in">make</span>(<span class="keyword">map</span>[<span class="keyword">int</span>]<span class="keyword">int</span>)</span><br><span class="line"><span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="keyword">for</span> &#123;</span><br><span class="line">_ = m[<span class="number">1</span>]</span><br><span class="line">&#125;</span><br><span class="line">&#125;()</span><br><span class="line"><span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="keyword">for</span> &#123;</span><br><span class="line">m[<span class="number">2</span>] = <span class="number">2</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;()</span><br><span class="line"><span class="keyword">select</span> &#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>报错信息为：fatal error: concurrent map read and map write</p></blockquote><blockquote><p>在开发 <code>msource</code> 组件的时候，同样遇到了这个问题</p></blockquote><p>如果你查看 Go 的源代码: <a href="https://github.com/golang/go/blob/master/src/runtime/hashmap.go#L542" target="_blank" rel="noopener">hashmap_fast.go#L118</a>,会看到读的时候会检查 hashWriting 标志， 如果有这个标志，就会报并发错误。</p><p>写的时候会设置这个标志: <a href="https://github.com/golang/go/blob/master/src/runtime/hashmap.go#L542" target="_blank" rel="noopener">hashmap.go#L542</a></p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">h.flags |= hashWriting</span><br></pre></td></tr></table></figure><p><a href="https://github.com/golang/go/blob/master/src/runtime/hashmap.go#L628" target="_blank" rel="noopener">hashmap.go#L628</a> 设置完之后会取消这个标记。</p><p>当然，代码中还有好几处并发读写的检查， 比如写的时候也会检查是不是有并发的写，删除键的时候类似写，遍历的时候并发读写问题等。</p><p>有时候，map 的并发问题不是那么容易被发现, 你可以利用 <code>-race</code> 参数来检查。</p><h2 id="Go-1-9-之前的解决方案"><a href="#Go-1-9-之前的解决方案" class="headerlink" title="Go 1.9 之前的解决方案"></a>Go 1.9 之前的解决方案</h2><p>但是，很多时候，我们会并发地使用 map 对象，尤其是在一定规模的项目中，map 总会保存 goroutine 共享的数据。在 Go 官方 blog 的 <a href="https://blog.golang.org/go-maps-in-action" target="_blank" rel="noopener">Go maps in action</a> 一文中，提供了一种简便的解决方案。</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> counter = <span class="keyword">struct</span>&#123;</span><br><span class="line">    sync.RWMutex</span><br><span class="line">    m <span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">int</span></span><br><span class="line">&#125;&#123;m: <span class="built_in">make</span>(<span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">int</span>)&#125;</span><br></pre></td></tr></table></figure><p>它使用嵌入 struct 为 map 增加一个读写锁。</p><p>读数据的时候很方便的加锁：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">counter.RLock()</span><br><span class="line">n := counter.m[<span class="string">"some_key"</span>]</span><br><span class="line">counter.RUnlock()</span><br><span class="line">fmt.Println(<span class="string">"some_key:"</span>, n)</span><br></pre></td></tr></table></figure><p>写数据的时候:</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">counter.Lock()</span><br><span class="line">counter.m[<span class="string">"some_key"</span>]++</span><br><span class="line">counter.Unlock()</span><br></pre></td></tr></table></figure><h2 id="sync-Map"><a href="#sync-Map" class="headerlink" title="sync.Map"></a>sync.Map</h2><p>可以说，上面的解决方案相当简洁，并且利用读写锁而不是 Mutex 可以进一步减少读写的时候因为锁带来的性能。</p><p>但是，它在一些场景下也有问题，如果熟悉 Java 的同学，可以对比一下 java 的 <code>ConcurrentHashMap</code> 的实现，在 map 的数据非常大的情况下，一把锁会导致大并发的客户端共争一把锁，Java 的解决方案是 shard, 内部使用多个锁，每个区间共享一把锁，这样减少了数据共享一把锁带来的性能影响，<a href="https://github.com/orcaman" target="_blank" rel="noopener">orcaman</a> 提供了这个思路的一个实现： <a href="https://github.com/orcaman/concurrent-map" target="_blank" rel="noopener">concurrent-map</a>，他也询问了 Go 相关的开发人员是否在 Go 中也实现这种<a href="https://github.com/golang/go/issues/20360" target="_blank" rel="noopener">方案</a>，由于实现的复杂性，答案是 <code>Yes, we considered it</code>.,但是除非有特别的性能提升和应用场景，否则没有进一步的开发消息。</p><p>那么，在 Go 1.9 中 <code>sync.Map</code> 是怎么实现的呢？它是如何解决并发提升性能的呢？</p><p><code>sync.Map</code> 的实现有几个优化点，这里先列出来，我们后面慢慢分析。</p><p>空间换时间。 通过冗余的两个数据结构(read、dirty),实现加锁对性能的影响。<br>使用只读数据(read)，避免读写冲突。<br>动态调整，miss 次数多了之后，将 dirty 数据提升为 read。<br>double-checking。<br>延迟删除。 删除一个键值只是打标记，只有在提升 dirty 的时候才清理删除的数据。<br>优先从 read 读取、更新、删除，因为对 read 的读取不需要锁。<br>下面我们介绍 <code>sync.Map</code> 的重点代码，以便理解它的实现思想。</p><p>首先，我们看一下 <code>sync.Map</code> 的数据结构：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> Map <span class="keyword">struct</span> &#123;</span><br><span class="line"><span class="comment">// 当涉及到dirty数据的操作的时候，需要使用这个锁</span></span><br><span class="line">mu Mutex</span><br><span class="line"><span class="comment">// 一个只读的数据结构，因为只读，所以不会有读写冲突。</span></span><br><span class="line"><span class="comment">// 所以从这个数据中读取总是安全的。</span></span><br><span class="line"><span class="comment">// 实际上，实际也会更新这个数据的entries,如果entry是未删除的(unexpunged), 并不需要加锁。如果entry已经被删除了，需要加锁，以便更新dirty数据。</span></span><br><span class="line">read atomic.Value <span class="comment">// readOnly</span></span><br><span class="line"><span class="comment">// dirty数据包含当前的map包含的entries,它包含最新的entries(包括read中未删除的数据,虽有冗余，但是提升dirty字段为read的时候非常快，不用一个一个的复制，而是直接将这个数据结构作为read字段的一部分),有些数据还可能没有移动到read字段中。</span></span><br><span class="line"><span class="comment">// 对于dirty的操作需要加锁，因为对它的操作可能会有读写竞争。</span></span><br><span class="line"><span class="comment">// 当dirty为空的时候， 比如初始化或者刚提升完，下一次的写操作会复制read字段中未删除的数据到这个数据中。</span></span><br><span class="line">dirty <span class="keyword">map</span>[<span class="keyword">interface</span>&#123;&#125;]*entry</span><br><span class="line"><span class="comment">// 当从Map中读取entry的时候，如果read中不包含这个entry,会尝试从dirty中读取，这个时候会将misses加一，</span></span><br><span class="line"><span class="comment">// 当misses累积到 dirty的长度的时候， 就会将dirty提升为read,避免从dirty中miss太多次。因为操作dirty需要加锁。</span></span><br><span class="line">misses <span class="keyword">int</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>它的数据结构很简单，值包含四个字段：<code>read</code>、<code>mu</code>、<code>dirty</code>、<code>misses</code>。</p><p>它使用了冗余的数据结构 <code>read</code>、<code>dirty</code>。<code>dirty</code> 中会包含 <code>read 中为删除的 entries</code>，新增加的 <code>entries 会加入到 dirty</code> 中。</p><p><code>read</code> 的数据结构是：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> readOnly <span class="keyword">struct</span> &#123;</span><br><span class="line">m       <span class="keyword">map</span>[<span class="keyword">interface</span>&#123;&#125;]*entry</span><br><span class="line">amended <span class="keyword">bool</span> <span class="comment">// 如果Map.dirty有些数据不在中的时候，这个值为true</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>amended 指明 <code>Map.dirty</code> 中有 <code>readOnly.m</code> 未包含的数据，所以如果从 Map.read 找不到数据的话，还要进一步到 Map.dirty 中查找。</p><p>对 Map.read 的修改是通过原子操作进行的。</p><p>虽然 <code>read</code> 和 <code>dirty</code> 有冗余数据，但这些数据是通过指针指向同一个数据，所以尽管 Map 的 value 会很大，但是冗余的空间占用还是有限的。</p><p><code>readOnly.m</code> 和 <code>Map.dirty</code> 存储的值类型是<code>\*entry</code>,它包含一个指针 p, 指向用户存储的 value 值。</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> entry <span class="keyword">struct</span> &#123;</span><br><span class="line">p unsafe.Pointer <span class="comment">// *interface&#123;&#125;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>p 有三种值：</p><ul><li>nil: entry 已被删除了，并且 m.dirty 为 nil</li><li>expunged: entry 已被删除了，并且 m.dirty 不为 nil，而且这个 entry 不存在于 m.dirty 中</li><li>其它： entry 是一个正常的值</li></ul><p>以上是 sync.Map 的数据结构，下面我们重点看看 <code>Load</code>、<code>Store</code>、<code>Delete</code>、<code>Range</code> 这四个方法，其它辅助方法可以参考这四个方法来理解。</p><h3 id="Load"><a href="#Load" class="headerlink" title="Load"></a>Load</h3><p>加载方法，也就是提供一个键 <code>key</code>,查找对应的值 <code>value</code>,如果不存在，通过 ok 反映：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m *Map)</span> <span class="title">Load</span><span class="params">(key <span class="keyword">interface</span>&#123;&#125;)</span> <span class="params">(value <span class="keyword">interface</span>&#123;&#125;, ok <span class="keyword">bool</span>)</span></span> &#123;</span><br><span class="line"><span class="comment">// 1.首先从m.read中得到只读readOnly,从它的map中查找，不需要加锁</span></span><br><span class="line">read, _ := m.read.Load().(readOnly)</span><br><span class="line">e, ok := read.m[key]</span><br><span class="line"><span class="comment">// 2. 如果没找到，并且m.dirty中有新数据，需要从m.dirty查找，这个时候需要加锁</span></span><br><span class="line"><span class="keyword">if</span> !ok &amp;&amp; read.amended &#123;</span><br><span class="line">m.mu.Lock()</span><br><span class="line"><span class="comment">// 双检查，避免加锁的时候m.dirty提升为m.read,这个时候m.read可能被替换了。</span></span><br><span class="line">read, _ = m.read.Load().(readOnly)</span><br><span class="line">e, ok = read.m[key]</span><br><span class="line"><span class="comment">// 如果m.read中还是不存在，并且m.dirty中有新数据</span></span><br><span class="line"><span class="keyword">if</span> !ok &amp;&amp; read.amended &#123;</span><br><span class="line"><span class="comment">// 从m.dirty查找</span></span><br><span class="line">e, ok = m.dirty[key]</span><br><span class="line"><span class="comment">// 不管m.dirty中存不存在，都将misses计数加一</span></span><br><span class="line"><span class="comment">// missLocked()中满足条件后就会提升m.dirty</span></span><br><span class="line">m.missLocked()</span><br><span class="line">&#125;</span><br><span class="line">m.mu.Unlock()</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> !ok &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, <span class="literal">false</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> e.load()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里有两个值的关注的地方。一个是首先从 <code>m.read</code> 中加载，不存在的情况下，并且 <code>m.dirty</code> 中有新数据，加锁，然后从 <code>m.dirty</code> 中加载。</p><p>二是这里使用了双检查的处理，因为在下面的两个语句中，这两行语句并不是一个原子操作。</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> !ok &amp;&amp; read.amended &#123;</span><br><span class="line">m.mu.Lock()</span><br></pre></td></tr></table></figure><p>虽然第一句执行的时候条件满足，但是在加锁之前，<code>m.dirty</code> 可能被提升为 <code>m.read</code>,所以加锁后还得再检查 <code>m.read</code>，后续的方法中都使用了这个方法。</p><p>双检查的技术 Java 程序员非常熟悉了，单例模式的实现之一就是利用双检查的技术。</p><p>可以看到，如果我们查询的键值正好存在于 <code>m.read</code> 中，无须加锁，直接返回，理论上性能优异。即使不存在于 <code>m.read</code> 中，经过 <code>miss</code> 几次之后，<code>m.dirty</code> 会被提升为 <code>m.read</code>，又会从 <code>m.read</code> 中查找。所以对于更新／增加较少，加载存在的 key 很多的 case,性能基本和无锁的 map 类似。</p><p>下面看看 <code>m.dirty</code> 是如何被提升的。<code>missLocked</code>方法中可能会将 <code>m.dirty</code> 提升。</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m *Map)</span> <span class="title">missLocked</span><span class="params">()</span></span> &#123;</span><br><span class="line">m.misses++</span><br><span class="line"><span class="keyword">if</span> m.misses &lt; <span class="built_in">len</span>(m.dirty) &#123;</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line">m.read.Store(readOnly&#123;m: m.dirty&#125;)</span><br><span class="line">m.dirty = <span class="literal">nil</span></span><br><span class="line">m.misses = <span class="number">0</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Store"><a href="#Store" class="headerlink" title="Store"></a>Store</h3><p>这个方法是更新或者新增一个 entry。</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m *Map)</span> <span class="title">Store</span><span class="params">(key, value <span class="keyword">interface</span>&#123;&#125;)</span></span> &#123;</span><br><span class="line"><span class="comment">// 如果m.read存在这个键，并且这个entry没有被标记删除，尝试直接存储。</span></span><br><span class="line"><span class="comment">// 因为m.dirty也指向这个entry,所以m.dirty也保持最新的entry。</span></span><br><span class="line">read, _ := m.read.Load().(readOnly)</span><br><span class="line"><span class="keyword">if</span> e, ok := read.m[key]; ok &amp;&amp; e.tryStore(&amp;value) &#123;</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 如果`m.read`不存在或者已经被标记删除</span></span><br><span class="line">m.mu.Lock()</span><br><span class="line">read, _ = m.read.Load().(readOnly)</span><br><span class="line"><span class="keyword">if</span> e, ok := read.m[key]; ok &#123;</span><br><span class="line"><span class="keyword">if</span> e.unexpungeLocked() &#123; <span class="comment">//标记成未被删除</span></span><br><span class="line">m.dirty[key] = e <span class="comment">//m.dirty中不存在这个键，所以加入m.dirty</span></span><br><span class="line">&#125;</span><br><span class="line">e.storeLocked(&amp;value) <span class="comment">//更新</span></span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> e, ok := m.dirty[key]; ok &#123; <span class="comment">// m.dirty存在这个键，更新</span></span><br><span class="line">e.storeLocked(&amp;value)</span><br><span class="line">&#125; <span class="keyword">else</span> &#123; <span class="comment">//新键值</span></span><br><span class="line"><span class="keyword">if</span> !read.amended &#123; <span class="comment">//m.dirty中没有新的数据，往m.dirty中增加第一个新键</span></span><br><span class="line">m.dirtyLocked() <span class="comment">//从m.read中复制未删除的数据</span></span><br><span class="line">m.read.Store(readOnly&#123;m: read.m, amended: <span class="literal">true</span>&#125;)</span><br><span class="line">&#125;</span><br><span class="line">m.dirty[key] = newEntry(value) <span class="comment">//将这个entry加入到m.dirty中</span></span><br><span class="line">&#125;</span><br><span class="line">m.mu.Unlock()</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m *Map)</span> <span class="title">dirtyLocked</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="keyword">if</span> m.dirty != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line">read, _ := m.read.Load().(readOnly)</span><br><span class="line">m.dirty = <span class="built_in">make</span>(<span class="keyword">map</span>[<span class="keyword">interface</span>&#123;&#125;]*entry, <span class="built_in">len</span>(read.m))</span><br><span class="line"><span class="keyword">for</span> k, e := <span class="keyword">range</span> read.m &#123;</span><br><span class="line"><span class="keyword">if</span> !e.tryExpungeLocked() &#123;</span><br><span class="line">m.dirty[k] = e</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(e *entry)</span> <span class="title">tryExpungeLocked</span><span class="params">()</span> <span class="params">(isExpunged <span class="keyword">bool</span>)</span></span> &#123;</span><br><span class="line">p := atomic.LoadPointer(&amp;e.p)</span><br><span class="line"><span class="keyword">for</span> p == <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="comment">// 将已经删除标记为nil的数据标记为expunged</span></span><br><span class="line"><span class="keyword">if</span> atomic.CompareAndSwapPointer(&amp;e.p, <span class="literal">nil</span>, expunged) &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">true</span></span><br><span class="line">&#125;</span><br><span class="line">p = atomic.LoadPointer(&amp;e.p)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> p == expunged</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>你可以看到，以上操作都是先从操作 <code>m.read</code> 开始的，不满足条件再加锁，然后操作 <code>m.dirty</code>。</p><p><code>Store</code> 可能会在某种情况下(初始化或者 <code>m.dirty</code> 刚被提升后)从 <code>m.read</code> 中复制数据，如果这个时候 <code>m.read</code> 中数据量非常大，可能会影响性能。</p><h3 id="Delete"><a href="#Delete" class="headerlink" title="Delete"></a>Delete</h3><p>删除一个键值。</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m *Map)</span> <span class="title">Delete</span><span class="params">(key <span class="keyword">interface</span>&#123;&#125;)</span></span> &#123;</span><br><span class="line">read, _ := m.read.Load().(readOnly)</span><br><span class="line">e, ok := read.m[key]</span><br><span class="line"><span class="keyword">if</span> !ok &amp;&amp; read.amended &#123;</span><br><span class="line">m.mu.Lock()</span><br><span class="line">read, _ = m.read.Load().(readOnly)</span><br><span class="line">e, ok = read.m[key]</span><br><span class="line"><span class="keyword">if</span> !ok &amp;&amp; read.amended &#123;</span><br><span class="line"><span class="built_in">delete</span>(m.dirty, key)</span><br><span class="line">&#125;</span><br><span class="line">m.mu.Unlock()</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> ok &#123;</span><br><span class="line">e.<span class="built_in">delete</span>()</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>同样，删除操作还是从 <code>m.read</code> 中开始， 如果这个 entry 不存在于 <code>m.read</code> 中，并且 <code>m.dirty</code> 中有新数据，则加锁尝试从 <code>m.dirty</code> 中删除。</p><p>注意，还是要双检查的。 从 <code>m.dirty</code> 中直接删除即可，就当它没存在过，但是如果是从 <code>m.read</code> 中删除，并不会直接删除，而是打标记：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(e *entry)</span> <span class="title">delete</span><span class="params">()</span> <span class="params">(hadValue <span class="keyword">bool</span>)</span></span> &#123;</span><br><span class="line"><span class="keyword">for</span> &#123;</span><br><span class="line">p := atomic.LoadPointer(&amp;e.p)</span><br><span class="line"><span class="comment">// 已标记为删除</span></span><br><span class="line"><span class="keyword">if</span> p == <span class="literal">nil</span> || p == expunged &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">false</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 原子操作，e.p标记为nil</span></span><br><span class="line"><span class="keyword">if</span> atomic.CompareAndSwapPointer(&amp;e.p, p, <span class="literal">nil</span>) &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">true</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Range"><a href="#Range" class="headerlink" title="Range"></a>Range</h3><p>因为<code>for ... range map</code>是内建的语言特性，所以没有办法使用<code>for range</code>遍历 sync.Map, 但是可以使用它的<code>Range</code>方法，通过回调的方式遍历。</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m *Map)</span> <span class="title">Range</span><span class="params">(f <span class="keyword">func</span>(key, value <span class="keyword">interface</span>&#123;&#125;)</span> <span class="title">bool</span>)</span> &#123;</span><br><span class="line">read, _ := m.read.Load().(readOnly)</span><br><span class="line"><span class="comment">// 如果m.dirty中有新数据，则提升m.dirty,然后在遍历</span></span><br><span class="line"><span class="keyword">if</span> read.amended &#123;</span><br><span class="line"><span class="comment">//提升m.dirty</span></span><br><span class="line">m.mu.Lock()</span><br><span class="line">read, _ = m.read.Load().(readOnly) <span class="comment">//双检查</span></span><br><span class="line"><span class="keyword">if</span> read.amended &#123;</span><br><span class="line">read = readOnly&#123;m: m.dirty&#125;</span><br><span class="line">m.read.Store(read)</span><br><span class="line">m.dirty = <span class="literal">nil</span></span><br><span class="line">m.misses = <span class="number">0</span></span><br><span class="line">&#125;</span><br><span class="line">m.mu.Unlock()</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 遍历, for range是安全的</span></span><br><span class="line"><span class="keyword">for</span> k, e := <span class="keyword">range</span> read.m &#123;</span><br><span class="line">v, ok := e.load()</span><br><span class="line"><span class="keyword">if</span> !ok &#123;</span><br><span class="line"><span class="keyword">continue</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> !f(k, v) &#123;</span><br><span class="line"><span class="keyword">break</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Range 方法调用前可能会做一个 <code>m.dirty</code> 的提升，不过提升 <code>m.dirty</code> 不是一个耗时的操作。</p><h4 id="sync-Map-的性能"><a href="#sync-Map-的性能" class="headerlink" title="sync.Map 的性能"></a>sync.Map 的性能</h4><p>Go 1.9 源代码中提供了性能的测试： <a href="https://github.com/golang/go/blob/master/src/sync/map_bench_test.go" target="_blank" rel="noopener">map_bench_test.go</a>、<a href="https://github.com/golang/go/blob/master/src/sync/map_reference_test.go" target="_blank" rel="noopener">map_reference_test.go</a></p><p>我也基于这些代码修改了一下，得到下面的测试数据，相比较以前的解决方案，性能多少回有些提升，如果你特别关注性能，可以考虑 sync.Map。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">BenchmarkHitAll&#x2F;*sync.RWMutexMap-4   20000000        83.8 ns&#x2F;op</span><br><span class="line">BenchmarkHitAll&#x2F;*sync.Map-4          30000000        59.9 ns&#x2F;op</span><br><span class="line">BenchmarkHitAll_WithoutPrompting&#x2F;*sync.RWMutexMap-4         20000000        96.9 ns&#x2F;op</span><br><span class="line">BenchmarkHitAll_WithoutPrompting&#x2F;*sync.Map-4                20000000        64.1 ns&#x2F;op</span><br><span class="line">BenchmarkHitNone&#x2F;*sync.RWMutexMap-4                         20000000        79.1 ns&#x2F;op</span><br><span class="line">BenchmarkHitNone&#x2F;*sync.Map-4                                30000000        43.3 ns&#x2F;op</span><br><span class="line">BenchmarkHit_WithoutPrompting&#x2F;*sync.RWMutexMap-4            20000000        81.5 ns&#x2F;op</span><br><span class="line">BenchmarkHit_WithoutPrompting&#x2F;*sync.Map-4                   30000000        44.0 ns&#x2F;op</span><br><span class="line">BenchmarkUpdate&#x2F;*sync.RWMutexMap-4                           5000000       328 ns&#x2F;op</span><br><span class="line">BenchmarkUpdate&#x2F;*sync.Map-4                                 10000000       146 ns&#x2F;op</span><br><span class="line">BenchmarkUpdate_WithoutPrompting&#x2F;*sync.RWMutexMap-4          5000000       336 ns&#x2F;op</span><br><span class="line">BenchmarkUpdate_WithoutPrompting&#x2F;*sync.Map-4                 5000000       324 ns&#x2F;op</span><br><span class="line">BenchmarkDelete&#x2F;*sync.RWMutexMap-4                          10000000       155 ns&#x2F;op</span><br><span class="line">BenchmarkDelete&#x2F;*sync.Map-4                                 30000000        55.0 ns&#x2F;op</span><br><span class="line">BenchmarkDelete_WithoutPrompting&#x2F;*sync.RWMutexMap-4         10000000       173 ns&#x2F;op</span><br><span class="line">BenchmarkDelete_WithoutPrompting&#x2F;*sync.Map-4                10000000</span><br></pre></td></tr></table></figure><h4 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h4><p><code>sync.Map</code> 没有 <code>Len</code> 方法，并且目前没有迹象要加上 (<a href="https://github.com/golang/go/issues/20680" target="_blank" rel="noopener">issue#20680</a>),所以如果想得到当前 Map 中有效的 entries 的数量，需要使用 <code>Range</code> 方法遍历一次， 比较 X 疼。</p><p><code>LoadOrStore</code> 方法如果提供的 key 存在，则返回已存在的值(Load)，否则保存提供的键值(Store)。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;由于要造一些轮子，例如 像 laravel 一样的 &lt;code&gt;Event组件&lt;/code&gt;，我参考了网上的几个库&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/sadlil/go-trigger&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;go-trigger&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/kataras/go-events&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;go-events&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这 2 个库，大同小异，只有一小部分差别，新打组件将会在这 2 个库的基础上再封装&lt;/p&gt;
&lt;p&gt;这 2 个库都是比较早期的库，所以在实现上用到了 map，但是由于考虑到 map 的线程安全性问题，所以他们都使用了 go1.9 之前实现的方式，就是在结构体嵌入一个读写锁来避免线程安全问题。&lt;/p&gt;
&lt;p&gt;在 Go 1.6 之前， 内置的 map 类型是部分 goroutine 安全的，并发的读没有问题，并发的写可能有问题。自 go 1.6 之后， 并发地读写 map 会报错，这在一些知名的开源库中都存在这个问题，所以 go 1.9 之前的解决方案是额外绑定一个锁，封装成一个新的 struct 或者单独使用锁都可以。&lt;/p&gt;
&lt;p&gt;本文带你深入到 sync.Map 的具体实现中，看看为了增加一个功能，代码是如何变的复杂的,以及作者在实现 sync.Map 的一些思想。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Golang" scheme="http://blog.crazylaw.cn/categories/Golang/"/>
    
    
      <category term="Golang" scheme="http://blog.crazylaw.cn/tags/Golang/"/>
    
  </entry>
  
  <entry>
    <title>【Redis】- 事务和Lua脚本</title>
    <link href="http://blog.crazylaw.cn/2020/06/02/Redis/redis%E4%BA%8B%E5%8A%A1%E5%92%8Clua/"/>
    <id>http://blog.crazylaw.cn/2020/06/02/Redis/redis%E4%BA%8B%E5%8A%A1%E5%92%8Clua/</id>
    <published>2020-06-02T02:46:51.000Z</published>
    <updated>2021-03-20T16:25:01.800Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Redis 我们用的蛮多的了，但是一直也没有整理什么资料，刚好今天要处理一下，顺便一下相关的事务和 lua 脚本的内容，由于管道(pipeline)不在这次原子性问题当中，所以我们就不加进来比较说明了。</p><a id="more"></a><h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h2><p>Redis 的原子性问题，在一直高并发的场景下，是一个需要重视的问题，否则原子性不满足将会导致业务逻辑出现问题。</p><p>在这里，我们常用解决原子性问题的一般都是用 redis 自带的命令，例如 brpoplpush 之类的命令</p><p>但是我们可能需要更多的组合成一个原子性操作</p><p>因此，这里会有 2 种选择，分别如下：</p><ul><li>事务</li><li>lua 脚本</li></ul><h3 id="事务"><a href="#事务" class="headerlink" title="事务"></a>事务</h3><p>redis 事务提供了一种“将多个命令打包， 然后一次性、按顺序地执行”的机制， 并且事务在执行的期间不会主动中断</p><p>我们可以通过 MULTI 命令开启一个事务，类似于 mysql 的 BEGIN TRANSACTION 语句</p><p>在该语句之后执行的命令都将被视为事务之内的操作</p><p>最后我们可以通过执行 <code>EXEC/DISCARD</code> 命令来提交/回滚该事务内的所有操作</p><p>这两个 Redis 命令可被视为等同于关系型数据库中的 COMMIT/ROLLBACK 语句</p><p>服务器在执行完事务中的所有命令之后， 才会继续处理其他客户端的其他命令</p><p>被执行的命令要么全部都被执行，要么一个也不执行，并且事务执行过程中不会被其他工作打断</p><p>一个 redis 事务从开始到执行会经历以下三个阶段：</p><p><img src="/images/Redis/redis%E4%BA%8B%E5%8A%A1.png" alt="redis事务流程"></p><h4 id="开始事务-–-MULTI"><a href="#开始事务-–-MULTI" class="headerlink" title="开始事务 – MULTI"></a>开始事务 – MULTI</h4><p>multi 命令让客户端从非事务状态切换到事务状态</p><h4 id="命令入队"><a href="#命令入队" class="headerlink" title="命令入队"></a>命令入队</h4><p>如果客户端处于非事务状态下，那么所有发送给服务端的命令都会立即被服务器执行，而如果客户端处于事务状态下，那么所有命令都还不会立即执行，而是被发送到一个事务队列中，返回 QUEUED，表示入队成功</p><p>事务队列是一个数组， 每个数组项是都包含三个属性</p><ul><li>cmd – 要执行的命令</li><li>argv – 命令的参数</li><li>argc – 参数个数</li></ul><p>例如，我们执行以下两个命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1:6379&gt; SET msg &quot;hello caiwenhui&quot;</span><br><span class="line">QUEUED</span><br><span class="line">127.0.0.1:6379&gt; GET msg</span><br><span class="line">QUEUED</span><br></pre></td></tr></table></figure><p>redis server 将创建以下事务队列：</p><table><thead><tr><th>index</th><th>cmd</th><th>argv</th><th>argc</th></tr></thead><tbody><tr><td>0</td><td>SET</td><td>[“msg”, “hello caiwenhui”]</td><td>2</td></tr><tr><td>1</td><td>GET</td><td>[“msg”]</td><td>1</td></tr></tbody></table><h4 id="执行事务"><a href="#执行事务" class="headerlink" title="执行事务"></a>执行事务</h4><p>如果客户端正处于事务状态， 那么当 EXEC 命令执行时， 服务器根据客户端所保存的事务队列， 以先进先出（FIFO）的方式执行事务队列中的命令</p><p>然后将执行命令所得的结果以 FIFO 的顺序保存到一个回复队列中</p><p>例如，当我们执行上述两个命令后执行 EXEC 命令，将会创建如下回复队列：</p><table><thead><tr><th>index</th><th>类型</th><th>内容</th></tr></thead><tbody><tr><td>0</td><td>status code reply</td><td>OK</td></tr><tr><td>1</td><td>bulk reply</td><td>“hello moto”</td></tr></tbody></table><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1:6379&gt; multi</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379&gt; SET msg &quot;hello caiwenhi&quot;</span><br><span class="line">QUEUED</span><br><span class="line">127.0.0.1:6379&gt; GET msg</span><br><span class="line">QUEUED</span><br><span class="line">127.0.0.1:6379&gt; EXEC</span><br><span class="line">1) OK</span><br><span class="line">1) hello caiwenhui</span><br></pre></td></tr></table></figure><h3 id="WATCH"><a href="#WATCH" class="headerlink" title="WATCH"></a>WATCH</h3><p>Watch 命令用于监视一个(或多个) key ，如果在事务执行之前这个(或这些) key 被其他命令所改动，那么事务将被打断</p><p>WATCH 命令只能在客户端进入事务状态之前执行， 在事务状态下发送 WATCH 命令会引发错误</p><p>redis 中保存了一个 watched_keys 字典，字典的键是这个数据库被监视的键，而字典的值则是一个链表，链表中保存了所有监视这个键的客户端</p><p>每当一个客户端执行 WATCH 命令，对应的 key 指向的链表中就会增加该客户端的节点</p><p><img src="/images/Redis/redis-watch-key.png" alt="redis-watch-key"></p><p>图意味着，key1 正在被 client1 和 client2 两个客户端监视着，key2 被 client3 监视着，key3 被 client4 监视着</p><p>一旦对数据库键空间进行的修改成功执行，multi.c 的 touchWatchedKey 函数都会被调用，他的工作就是遍历上述字典中该 key 所对应的整个链表的所有节点，打开每一个 WATCH 该 key 的 client 的 REDIS_DIRTY_CAS 选项</p><p>当客户端发送 EXEC 命令触发事务执行时，服务器会对客户端状态进行检查，如果客户端的 REDIS_DIRTY_CAS 选项已经被打开，那么说明被客户端监视的键至少有一个已经被修改，事务安全性已经被破坏，则服务端直接向客户端返回空回复，表示事务执行失败</p><p>当一个客户端结束它的事务时，无论事务是成功执行，还是失败， watched_keys 字典中和这个客户端相关的资料都会被清除</p><h3 id="redis-事务的特性"><a href="#redis-事务的特性" class="headerlink" title="redis 事务的特性"></a>redis 事务的特性</h3><ul><li>如果在执行 exec 之前事务中断了，那么所有的命令都不会执行</li><li>如果某个命令语法错误，不仅会导致该命令入队失败，整个事务都将无法执行</li><li>如果执行了 exec 命令之后，那么所有的命令都会按序执行</li><li>当 redis 在执行命令时，如果出现了错误，那么 redis 不会终止其它命令的执行，这是与关系型数据库事务最大的区别，redis 事务不会因为某个命令执行失败而回滚</li></ul><h3 id="redis-事务的缺陷"><a href="#redis-事务的缺陷" class="headerlink" title="redis 事务的缺陷"></a>redis 事务的缺陷</h3><h4 id="不满足原子性"><a href="#不满足原子性" class="headerlink" title="不满足原子性"></a>不满足原子性</h4><p>与关系型数据库的事务不同，redis 事务是不满足原子性的，一个事务执行过程中，其他事务或 client 是可以对相应的 key 进行修改的</p><p>想要避免这样的并发性问题就需要使用 WATCH 命令，但是通常来说，必须经过仔细考虑才能决定究竟需要对哪些 key 进行 WATCH 加锁</p><p>额外的 WATCH 会增加事务失败的可能，而缺少必要的 WATCH 又会让我们的程序产生竞争条件</p><h4 id="后执行的命令无法依赖先执行命令的结果"><a href="#后执行的命令无法依赖先执行命令的结果" class="headerlink" title="后执行的命令无法依赖先执行命令的结果"></a>后执行的命令无法依赖先执行命令的结果</h4><p>由于事务中的所有命令都是互相独立的，在遇到 exec 命令之前并没有真正的执行，所以我们无法在事务中的命令中使用前面命令的查询结果</p><p>我们唯一可以做的就是通过 watch 保证在我们进行修改时，如果其它事务刚好进行了修改，则我们的修改停止，然后应用层做相应的处理</p><h4 id="事务中的每条命令都会与-redis-服务器进行网络交互"><a href="#事务中的每条命令都会与-redis-服务器进行网络交互" class="headerlink" title="事务中的每条命令都会与 redis 服务器进行网络交互"></a>事务中的每条命令都会与 redis 服务器进行网络交互</h4><p>redis 事务开启之后，每执行一个操作返回的都是 queued，这里就涉及到客户端与服务器端的多次交互</p><p>明明是需要一次批量执行的 n 条命令，还需要通过多次网络交互，显然非常浪费</p><blockquote><p>这个就是为什么会有 pipeline 的原因，减少 RTT 的时间</p></blockquote><h2 id="redis-事务缺陷的解决-–-Lua"><a href="#redis-事务缺陷的解决-–-Lua" class="headerlink" title="redis 事务缺陷的解决 – Lua"></a>redis 事务缺陷的解决 – Lua</h2><p>Lua 是一个小巧的脚本语言，有标准 C 编写，几乎在所有操作系统和平台上都可以编译运行</p><p>一个完整的 Lua 解释器不过 200k，在目前所有脚本引擎中，Lua 的速度是最快的，这一切都决定了 Lua 是作为嵌入式脚本的最佳选择</p><p>redis 2.6 版本之后也内嵌了一个 Lua 解释器，可以用于一些简单的事务与逻辑运算</p><h3 id="Redis-内嵌-Lua-的优势"><a href="#Redis-内嵌-Lua-的优势" class="headerlink" title="Redis 内嵌 Lua 的优势"></a>Redis 内嵌 Lua 的优势</h3><h4 id="在服务端实现业务逻辑"><a href="#在服务端实现业务逻辑" class="headerlink" title="在服务端实现业务逻辑"></a>在服务端实现业务逻辑</h4><p>按照我们上面介绍的，redis 事务执行中，每一条指令之间是相互独立的，我们无法让后面的操作依赖前面命名的结果，这就让整个事务仅仅成为了一个命令集合，在命令之间我们完全无法做任何事</p><p>但是，Lua 作为一个脚本语言，可以拥有分支、循环等语法结构，可以进行业务逻辑的编写</p><h4 id="原子性"><a href="#原子性" class="headerlink" title="原子性"></a>原子性</h4><p>由于 Lua 脚本是提交到 Redis server 进行一次性执行的，整个执行过程中不会被其他任何工作打断，其它任何脚本或者命令都无法执行,也就不会引起竞争条件，从而本身就实现了事务的原子性</p><p>但是，这同样会引起一个问题，正如官方文档所说的，正是由于 script 执行的原子性，<code>所以我们不要在 script 中执行过长开销的程序，否则会验证影响其它请求的执行</code></p><h4 id="可复用"><a href="#可复用" class="headerlink" title="可复用"></a>可复用</h4><p>所有 Lua 脚本都是可重用的，这样就减少了网络开销</p><ul><li>EVAL script numkeys key[key …] arg [arg…]</li><li>EVALSHA sha1</li><li>SCRIPT LOAD script</li><li>SCRIPT EXISTS sha1</li></ul><h5 id="EVAL"><a href="#EVAL" class="headerlink" title="EVAL"></a>EVAL</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">EVAL script numkeys key [key ...] arg [arg ...]</span><br></pre></td></tr></table></figure><table><thead><tr><th>参数</th><th>描述</th></tr></thead><tbody><tr><td>script</td><td>一段 Lua 脚本或 Lua 脚本文件所在路径及文件名</td></tr><tr><td>numkeys</td><td>Lua 脚本对应参数数量</td></tr><tr><td>key [key …]</td><td>Lua 中通过全局变量 KEYS 数组存储的传入参数</td></tr><tr><td>arg [arg …]</td><td>Lua 中通过全局变量 ARGV 数组存储的传入附加参数</td></tr></tbody></table><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">EVAL &quot;return &#123;KEYS[1],KEYS[2],ARGV[1],ARGV[2]&#125;&quot; 2 key1 key2 first second</span><br><span class="line">1) &quot;key1&quot;</span><br><span class="line">2) &quot;key2&quot;</span><br><span class="line">3) &quot;first&quot;</span><br><span class="line">4) &quot;second&quot;</span><br></pre></td></tr></table></figure><p><img src="/images/Redis/redis-lua.png" alt="redis-lua"></p><h5 id="SCRIPT-LOAD-与-EVALSHA-命令"><a href="#SCRIPT-LOAD-与-EVALSHA-命令" class="headerlink" title="SCRIPT LOAD 与 EVALSHA 命令"></a>SCRIPT LOAD 与 EVALSHA 命令</h5><p>对于不立即执行的 Lua 脚本，或需要重用的 Lua 脚本，可以通过 SCRIPT LOAD 提前载入 Lua 脚本，这个命令会立即返回对应的 SHA1 校验码</p><p>当需要执行函数时，通过 EVALSHA 调用 SCRIPT LOAD 返回的 SHA1 即可</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">SCRIPT LOAD &quot;return &#123;KEYS[1],KEYS[2],ARGV[1],ARGV[2]&#125;&quot;</span><br><span class="line">&quot;232fd51614574cf0867b83d384a5e898cfd24e5a&quot;</span><br><span class="line"></span><br><span class="line">EVALSHA &quot;232fd51614574cf0867b83d384a5e898cfd24e5a&quot; 2 key1 key2 first second</span><br><span class="line">1) &quot;key1&quot;</span><br><span class="line">2) &quot;key2&quot;</span><br><span class="line">3) &quot;first&quot;</span><br><span class="line">4) &quot;second&quot;</span><br></pre></td></tr></table></figure><h4 id="通过-Lua-脚本执行-redis-命令"><a href="#通过-Lua-脚本执行-redis-命令" class="headerlink" title="通过 Lua 脚本执行 redis 命令"></a>通过 Lua 脚本执行 redis 命令</h4><p>在 Lua 脚本中，只要使用 redis.call 传入 redis 命令就可以直接执行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">eval &quot;return redis.call(&#39;set&#39;,KEYS[1],&#39;bar&#39;)&quot; 1 foo     --等同于在服务端执行 set foo bar</span><br></pre></td></tr></table></figure><h4 id="使用-Lua-脚本实现访问频率限制"><a href="#使用-Lua-脚本实现访问频率限制" class="headerlink" title="使用 Lua 脚本实现访问频率限制"></a>使用 Lua 脚本实现访问频率限制</h4><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--</span></span><br><span class="line"><span class="comment">-- KEYS[1] 要限制的ip</span></span><br><span class="line"><span class="comment">-- ARGV[1] 限制的访问次数</span></span><br><span class="line"><span class="comment">-- ARGV[2] 限制的时间</span></span><br><span class="line"><span class="comment">--</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">local</span> key = <span class="string">"rate.limit:"</span> .. KEYS[<span class="number">1</span>]</span><br><span class="line"><span class="keyword">local</span> limit = <span class="built_in">tonumber</span>(ARGV[<span class="number">1</span>])</span><br><span class="line"><span class="keyword">local</span> expire_time = ARGV[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">local</span> is_exists = redis.call(<span class="string">"EXISTS"</span>, key)</span><br><span class="line"><span class="keyword">if</span> is_exists == <span class="number">1</span> <span class="keyword">then</span></span><br><span class="line">    <span class="keyword">if</span> redis.call(<span class="string">"INCR"</span>, key) &gt; limit <span class="keyword">then</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    redis.call(<span class="string">"SET"</span>, key, <span class="number">1</span>)</span><br><span class="line">    redis.call(<span class="string">"EXPIRE"</span>, key, expire_time)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;Redis 我们用的蛮多的了，但是一直也没有整理什么资料，刚好今天要处理一下，顺便一下相关的事务和 lua 脚本的内容，由于管道(pipeline)不在这次原子性问题当中，所以我们就不加进来比较说明了。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Redis" scheme="http://blog.crazylaw.cn/categories/Redis/"/>
    
    
      <category term="Redis" scheme="http://blog.crazylaw.cn/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>【Golang】- confluent-kafka-go</title>
    <link href="http://blog.crazylaw.cn/2020/06/01/Golang/confluent-kafka-go/"/>
    <id>http://blog.crazylaw.cn/2020/06/01/Golang/confluent-kafka-go/</id>
    <published>2020-06-01T01:46:51.000Z</published>
    <updated>2021-03-20T16:25:01.799Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>由于我们部门的一些大数据服务是用到 kafka 的，这个时期正值我们对 golang 语言对一个转型阶段，对比了一下开源对 kafka 客户端，决定使用 <code>confluent-kafka-go</code>, 所以在这里记录一下<code>confluent-kafka-go</code> 的一些内容</p><a id="more"></a><h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h2><p><code>confluent-kafka-go</code> 是一个 confluent 官方的 golang 语言库，其依赖于 <code>librdkafka</code> 实现，大多数机器，librakafka 都已经预编译进去 golang 扩展了，不需要额外安装 librdkafka，如果不支持预编译的话，则需要额外安装。</p><p>提供一个 dockerfile 的 demo</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># debian10 : buster</span></span><br><span class="line"><span class="comment"># debian9 : buster</span></span><br><span class="line"><span class="comment"># debian8 : jessie</span></span><br><span class="line"><span class="comment"># debian7 : wheezy</span></span><br><span class="line"><span class="keyword">FROM</span> golang:<span class="number">1.14</span>.<span class="number">3</span>-buster</span><br><span class="line"></span><br><span class="line"><span class="keyword">ENV</span> GO111MODULE=on</span><br><span class="line"><span class="keyword">ENV</span> GOPROXY=https://goproxy.io,direct</span><br><span class="line"><span class="keyword">ENV</span> GOPRIVATE=git.mingchao.com</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 换源</span></span><br><span class="line"><span class="comment"># 2. 加入confluent的源，安装librakafka</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> <span class="built_in">echo</span> \</span></span><br><span class="line"><span class="bash">        deb http://mirrors.aliyun.com/debian/ buster main non-free contrib \</span></span><br><span class="line"><span class="bash">        deb-src http://mirrors.aliyun.com/debian/ buster main non-free contrib \</span></span><br><span class="line"><span class="bash">        deb http://mirrors.aliyun.com/debian-security buster/updates main \</span></span><br><span class="line"><span class="bash">        deb-src http://mirrors.aliyun.com/debian-security buster/updates main \</span></span><br><span class="line"><span class="bash">        deb http://mirrors.aliyun.com/debian/ buster-updates main non-free contrib \</span></span><br><span class="line"><span class="bash">        deb-src http://mirrors.aliyun.com/debian/ buster-updates main non-free contrib \</span></span><br><span class="line"><span class="bash">        deb http://mirrors.aliyun.com/debian/ buster-backports main non-free contrib \</span></span><br><span class="line"><span class="bash">        deb-src http://mirrors.aliyun.com/debian/ buster-backports main non-free contrib \</span></span><br><span class="line"><span class="bash">    &gt; /etc/apt/sources.list &amp;&amp; \</span></span><br><span class="line"><span class="bash">    apt-get update -y &amp;&amp; \</span></span><br><span class="line"><span class="bash">    wget -qO - https://packages.confluent.io/deb/5.5/archive.key |  apt-key add - &gt; /dev/null &amp;&amp; \</span></span><br><span class="line"><span class="bash">    sed -i <span class="string">'$a deb [arch=amd64] https://packages.confluent.io/deb/5.5 stable main'</span> /etc/apt/sources.list &amp;&amp; \</span></span><br><span class="line"><span class="bash">    apt-get install -y librdkafka-dev</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">ARG</span> GIT_USERNAME=git</span><br><span class="line"><span class="keyword">ARG</span> GIT_PASSWORD=git-password</span><br><span class="line"><span class="keyword">ARG</span> GIT_CREDEN_FILE=/.git-credentials</span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> touch <span class="variable">$&#123;GIT_CREDEN_FILE&#125;</span> &amp;&amp; \</span></span><br><span class="line"><span class="bash">    chown 600 <span class="variable">$&#123;GIT_CREDEN_FILE&#125;</span> &amp;&amp; \</span></span><br><span class="line"><span class="bash">    git config --global credential.helper <span class="string">'store --file '</span><span class="variable">$&#123;GIT_CREDEN_FILE&#125;</span> &amp;&amp; \</span></span><br><span class="line"><span class="bash">    <span class="built_in">echo</span> https://<span class="variable">$&#123;GIT_USERNAME&#125;</span>:<span class="variable">$&#123;GIT_PASSWORD&#125;</span>@git.mingchao.com | tee <span class="variable">$&#123;GIT_CREDEN_FILE&#125;</span></span></span><br></pre></td></tr></table></figure><h2 id="源码-Api-说明"><a href="#源码-Api-说明" class="headerlink" title="源码 Api 说明"></a>源码 Api 说明</h2><h3 id="consumer-go"><a href="#consumer-go" class="headerlink" title="consumer.go"></a>consumer.go</h3><ul><li>这是一个 consumer 相关的文件。</li></ul><h4 id="Subscribe-topic-string-rebalanceCb-RebalanceCb-error"><a href="#Subscribe-topic-string-rebalanceCb-RebalanceCb-error" class="headerlink" title="Subscribe(topic string, rebalanceCb RebalanceCb) error"></a>Subscribe(topic string, rebalanceCb RebalanceCb) error</h4><p>订阅一个 topic，这个 api 会覆盖之前设置过了的 topic 订阅</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Consumer)</span> <span class="title">Subscribe</span><span class="params">(topic <span class="keyword">string</span>, rebalanceCb RebalanceCb)</span> <span class="title">error</span></span> &#123;</span><br><span class="line"><span class="keyword">return</span> c.SubscribeTopics([]<span class="keyword">string</span>&#123;topic&#125;, rebalanceCb)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="SubscribeTopics-topics-string-rebalanceCb-RebalanceCb-err-error"><a href="#SubscribeTopics-topics-string-rebalanceCb-RebalanceCb-err-error" class="headerlink" title="SubscribeTopics(topics []string, rebalanceCb RebalanceCb) (err error)"></a>SubscribeTopics(topics []string, rebalanceCb RebalanceCb) (err error)</h4><ul><li>订阅多个 topic</li><li>这个 api 会覆盖之前设置过了的 topic 订阅</li></ul><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Consumer)</span> <span class="title">SubscribeTopics</span><span class="params">(topics []<span class="keyword">string</span>, rebalanceCb RebalanceCb)</span> <span class="params">(err error)</span></span> &#123;</span><br><span class="line">ctopics := C.rd_kafka_topic_partition_list_new(C.<span class="keyword">int</span>(<span class="built_in">len</span>(topics)))</span><br><span class="line"><span class="keyword">defer</span> C.rd_kafka_topic_partition_list_destroy(ctopics)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _, topic := <span class="keyword">range</span> topics &#123;</span><br><span class="line">ctopic := C.CString(topic)</span><br><span class="line"><span class="keyword">defer</span> C.free(unsafe.Pointer(ctopic))</span><br><span class="line">C.rd_kafka_topic_partition_list_add(ctopics, ctopic, C.RD_KAFKA_PARTITION_UA)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">e := C.rd_kafka_subscribe(c.handle.rk, ctopics)</span><br><span class="line"><span class="keyword">if</span> e != C.RD_KAFKA_RESP_ERR_NO_ERROR &#123;</span><br><span class="line"><span class="keyword">return</span> newError(e)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">c.rebalanceCb = rebalanceCb</span><br><span class="line">c.handle.currAppRebalanceEnable = c.rebalanceCb != <span class="literal">nil</span> || c.appRebalanceEnable</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="Unsubscribe-err-error"><a href="#Unsubscribe-err-error" class="headerlink" title="Unsubscribe() (err error)"></a>Unsubscribe() (err error)</h4><ul><li>取消当前对 topic 的订阅</li></ul><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Consumer)</span> <span class="title">Unsubscribe</span><span class="params">()</span> <span class="params">(err error)</span></span> &#123;</span><br><span class="line">C.rd_kafka_unsubscribe(c.handle.rk)</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="Assign-partitions-TopicPartition-err-error"><a href="#Assign-partitions-TopicPartition-err-error" class="headerlink" title="Assign(partitions []TopicPartition) (err error)"></a>Assign(partitions []TopicPartition) (err error)</h4><ul><li>分配一组要使用的 partition</li><li>这个 api 会覆盖之前分配过的</li></ul><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Consumer)</span> <span class="title">Assign</span><span class="params">(partitions []TopicPartition)</span> <span class="params">(err error)</span></span> &#123;</span><br><span class="line">c.appReassigned = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">cparts := newCPartsFromTopicPartitions(partitions)</span><br><span class="line"><span class="keyword">defer</span> C.rd_kafka_topic_partition_list_destroy(cparts)</span><br><span class="line"></span><br><span class="line">e := C.rd_kafka_assign(c.handle.rk, cparts)</span><br><span class="line"><span class="keyword">if</span> e != C.RD_KAFKA_RESP_ERR_NO_ERROR &#123;</span><br><span class="line"><span class="keyword">return</span> newError(e)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="Unassign-err-error"><a href="#Unassign-err-error" class="headerlink" title="Unassign() (err error)"></a>Unassign() (err error)</h4><ul><li>取消当前分配的 partition</li></ul><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Consumer)</span> <span class="title">Unassign</span><span class="params">()</span> <span class="params">(err error)</span></span> &#123;</span><br><span class="line">c.appReassigned = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">e := C.rd_kafka_assign(c.handle.rk, <span class="literal">nil</span>)</span><br><span class="line"><span class="keyword">if</span> e != C.RD_KAFKA_RESP_ERR_NO_ERROR &#123;</span><br><span class="line"><span class="keyword">return</span> newError(e)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="Commit-TopicPartition-error"><a href="#Commit-TopicPartition-error" class="headerlink" title="Commit() ([]TopicPartition, error)"></a>Commit() ([]TopicPartition, error)</h4><ul><li>提交当前已经分配的 partition 的 offset 值</li><li>基于 <code>StoreOffsets(offsets []TopicPartition) (storedOffsets []TopicPartition, err error)</code></li><li>这是一个阻塞请求，如果需要异步操作，需要调用者自行用协程</li><li>返回成功提交 offset 的 topicPartition</li></ul><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Consumer)</span> <span class="title">Commit</span><span class="params">()</span> <span class="params">([]TopicPartition, error)</span></span> &#123;</span><br><span class="line"><span class="keyword">return</span> c.commit(<span class="literal">nil</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="CommitMessage-m-Message-TopicPartition-error"><a href="#CommitMessage-m-Message-TopicPartition-error" class="headerlink" title="CommitMessage(m *Message) ([]TopicPartition, error)"></a>CommitMessage(m *Message) ([]TopicPartition, error)</h4><ul><li>这个 API 基于 message 结构体</li><li>这是一个阻塞请求，如果需要异步操作，需要调用者自行用协程</li><li>返回成功提交 offset 的 topicPartition</li></ul><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Consumer)</span> <span class="title">CommitMessage</span><span class="params">(m *Message)</span> <span class="params">([]TopicPartition, error)</span></span> &#123;</span><br><span class="line"><span class="keyword">if</span> m.TopicPartition.Error != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, newErrorFromString(ErrInvalidArg, <span class="string">"Can't commit errored message"</span>)</span><br><span class="line">&#125;</span><br><span class="line">offsets := []TopicPartition&#123;m.TopicPartition&#125;</span><br><span class="line">offsets[<span class="number">0</span>].Offset++</span><br><span class="line"><span class="keyword">return</span> c.commit(offsets)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="CommitOffsets-offsets-TopicPartition-TopicPartition-error"><a href="#CommitOffsets-offsets-TopicPartition-TopicPartition-error" class="headerlink" title="CommitOffsets(offsets []TopicPartition) ([]TopicPartition, error)"></a>CommitOffsets(offsets []TopicPartition) ([]TopicPartition, error)</h4><ul><li>根据 []TopicPartition 来提交 offset</li><li>这是一个阻塞请求，如果需要异步操作，需要调用者自行用协程</li><li>返回成功提交 offset 的 topicPartition</li></ul><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Consumer)</span> <span class="title">CommitOffsets</span><span class="params">(offsets []TopicPartition)</span> <span class="params">([]TopicPartition, error)</span></span> &#123;</span><br><span class="line"><span class="keyword">return</span> c.commit(offsets)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="StoreOffsets-offsets-TopicPartition-storedOffsets-TopicPartition-err-error"><a href="#StoreOffsets-offsets-TopicPartition-storedOffsets-TopicPartition-err-error" class="headerlink" title="StoreOffsets(offsets []TopicPartition) (storedOffsets []TopicPartition, err error)"></a>StoreOffsets(offsets []TopicPartition) (storedOffsets []TopicPartition, err error)</h4><ul><li>根据 []TopicPartition 来记录将会被提交的 offset（如果允许自动提交的话，那么会受<code>auto.commit.interval.ms</code>的影响，一定周期性提交，如果是手动提交的话则依赖 <code>Commit()</code>Api）</li><li>返回成功存储的 offsets，如果至少有一个偏移量无法存储，则返回一个错误和偏移量列表。每个偏移量都可以通过它的来检查特定的错误</li></ul><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Consumer)</span> <span class="title">StoreOffsets</span><span class="params">(offsets []TopicPartition)</span> <span class="params">(storedOffsets []TopicPartition, err error)</span></span> &#123;</span><br><span class="line">coffsets := newCPartsFromTopicPartitions(offsets)</span><br><span class="line"><span class="keyword">defer</span> C.rd_kafka_topic_partition_list_destroy(coffsets)</span><br><span class="line"></span><br><span class="line">cErr := C.rd_kafka_offsets_store(c.handle.rk, coffsets)</span><br><span class="line"></span><br><span class="line"><span class="comment">// coffsets might be annotated with an error</span></span><br><span class="line">storedOffsets = newTopicPartitionsFromCparts(coffsets)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> cErr != C.RD_KAFKA_RESP_ERR_NO_ERROR &#123;</span><br><span class="line"><span class="keyword">return</span> storedOffsets, newError(cErr)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> storedOffsets, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="Seek-partition-TopicPartition-timeoutMs-int-error"><a href="#Seek-partition-TopicPartition-timeoutMs-int-error" class="headerlink" title="Seek(partition TopicPartition, timeoutMs int) error"></a>Seek(partition TopicPartition, timeoutMs int) error</h4><ul><li>获取指定 partition 的 offset</li><li>如果<code>timeoutMs</code>不是 0，则调用将等待这么长时间以执行查找。如果超时到达，内部状态将未知，并且此函数返回 ErrTimedOut。</li><li>如果<code>timeoutMs</code> 为 0，它将发起查找，但立即返回，不报告任何错误(例如，异步)。</li><li>Seek()只能用于已经使用的分区(通过 Assign()或隐式使用通过自平衡订阅())。</li><li>要设置起始偏移量，最好使用 Assign()并为每个分区提供一个起始偏移量。</li></ul><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Consumer)</span> <span class="title">Seek</span><span class="params">(partition TopicPartition, timeoutMs <span class="keyword">int</span>)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">rkt := c.handle.getRkt(*partition.Topic)</span><br><span class="line">cErr := C.rd_kafka_seek(rkt,</span><br><span class="line">C.int32_t(partition.Partition),</span><br><span class="line">C.int64_t(partition.Offset),</span><br><span class="line">C.<span class="keyword">int</span>(timeoutMs))</span><br><span class="line"><span class="keyword">if</span> cErr != C.RD_KAFKA_RESP_ERR_NO_ERROR &#123;</span><br><span class="line"><span class="keyword">return</span> newError(cErr)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="Poll-timeoutMs-int-event-Event"><a href="#Poll-timeoutMs-int-event-Event" class="headerlink" title="Poll(timeoutMs int) (event Event)"></a>Poll(timeoutMs int) (event Event)</h4><ul><li>轮询消息或事件。</li><li>将阻塞最多 <code>timeoutMs</code> 的超时时间</li><li>以下回调可能会被触发<ul><li>Subscribe()’s rebalanceCb</li></ul></li><li>如果超时则返回 nil，否则返回一个事件</li></ul><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Consumer)</span> <span class="title">Poll</span><span class="params">(timeoutMs <span class="keyword">int</span>)</span> <span class="params">(event Event)</span></span> &#123;</span><br><span class="line">ev, _ := c.handle.eventPoll(<span class="literal">nil</span>, timeoutMs, <span class="number">1</span>, <span class="literal">nil</span>)</span><br><span class="line"><span class="keyword">return</span> ev</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="ReadMessage-timeout-time-Duration-Message-error"><a href="#ReadMessage-timeout-time-Duration-Message-error" class="headerlink" title="ReadMessage(timeout time.Duration) (*Message, error)"></a>ReadMessage(timeout time.Duration) (*Message, error)</h4><ul><li>返回一条消息</li><li>这是一个方便的 API，它封装了 Poll()，只返回消息或错误。所有其他事件类型都被丢弃。</li><li>该调用最多会阻塞 <code>timeout</code> 等待新消息或错误。<code>timeout</code>可以设置为-1，表示无限期等待。</li><li>超时将会返回<code>(nil, err)</code> 当 err 是 <code>kafka.(Error).Code == Kafka.ErrTimedOut</code></li><li>消息将会返回 <code>(msg, nil)</code>, 当有错误当时候将会返回 <code>(nil, err)</code>, 当指定 partition 错误的时候（topic，partition，offset），将会返回 <code>(msg,err)</code></li><li>全部其他的事件类型，像<code>PartitionEOF</code>,<code>AssingedPartitions</code>等等将会被默认丢弃</li></ul><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Consumer)</span> <span class="title">ReadMessage</span><span class="params">(timeout time.Duration)</span> <span class="params">(*Message, error)</span></span> &#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> absTimeout time.Time</span><br><span class="line"><span class="keyword">var</span> timeoutMs <span class="keyword">int</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> timeout &gt; <span class="number">0</span> &#123;</span><br><span class="line">absTimeout = time.Now().Add(timeout)</span><br><span class="line">timeoutMs = (<span class="keyword">int</span>)(timeout.Seconds() * <span class="number">1000.0</span>)</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">timeoutMs = (<span class="keyword">int</span>)(timeout)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> &#123;</span><br><span class="line">ev := c.Poll(timeoutMs)</span><br><span class="line"></span><br><span class="line"><span class="keyword">switch</span> e := ev.(<span class="keyword">type</span>) &#123;</span><br><span class="line"><span class="keyword">case</span> *Message:</span><br><span class="line"><span class="keyword">if</span> e.TopicPartition.Error != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> e, e.TopicPartition.Error</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> e, <span class="literal">nil</span></span><br><span class="line"><span class="keyword">case</span> Error:</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, e</span><br><span class="line"><span class="keyword">default</span>:</span><br><span class="line"><span class="comment">// Ignore other event types</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> timeout &gt; <span class="number">0</span> &#123;</span><br><span class="line"><span class="comment">// Calculate remaining time</span></span><br><span class="line">timeoutMs = <span class="keyword">int</span>(math.Max(<span class="number">0.0</span>, absTimeout.Sub(time.Now()).Seconds()*<span class="number">1000.0</span>))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> timeoutMs == <span class="number">0</span> &amp;&amp; ev == <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, newError(C.RD_KAFKA_RESP_ERR__TIMED_OUT)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="Close-err-error"><a href="#Close-err-error" class="headerlink" title="Close() (err error)"></a>Close() (err error)</h4><ul><li>关闭一个 Consumer 对象</li><li>调用后，对象不再可用。</li></ul><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Consumer)</span> <span class="title">Close</span><span class="params">()</span> <span class="params">(err error)</span></span> &#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Wait for consumerReader() or pollLogEvents to terminate (by closing readerTermChan)</span></span><br><span class="line"><span class="built_in">close</span>(c.readerTermChan)</span><br><span class="line">c.handle.waitGroup.Wait()</span><br><span class="line"><span class="keyword">if</span> c.eventsChanEnable &#123;</span><br><span class="line"><span class="built_in">close</span>(c.events)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">C.rd_kafka_queue_destroy(c.handle.rkq)</span><br><span class="line">c.handle.rkq = <span class="literal">nil</span></span><br><span class="line"></span><br><span class="line">e := C.rd_kafka_consumer_close(c.handle.rk)</span><br><span class="line"><span class="keyword">if</span> e != C.RD_KAFKA_RESP_ERR_NO_ERROR &#123;</span><br><span class="line"><span class="keyword">return</span> newError(e)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">c.handle.cleanup()</span><br><span class="line"></span><br><span class="line">C.rd_kafka_destroy(c.handle.rk)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="GetMetadata-topic-string-allTopics-bool-timeoutMs-int-Metadata-error"><a href="#GetMetadata-topic-string-allTopics-bool-timeoutMs-int-Metadata-error" class="headerlink" title="GetMetadata(topic string, allTopics bool, timeoutMs int) (Metadata, error)"></a>GetMetadata(topic <em>string, allTopics bool, timeoutMs int) (</em>Metadata, error)</h4><ul><li>用于查询集群中 broker 和 topic 的元数据</li><li>如果 <code>topic</code> 参数不为 nil，则返回和 topoic 相关数据，否则（如果<code>allTopics</code>参数为 false，那么将会返回当前使用 topic 的元数据，如果 <code>allTopics</code>参数为 true, 那么将返回 broker 中所有 topic 的元数据）</li><li>GetMetadata 相当于 Java API 中的 listTopics、describeTopics 和 describeCluster。</li></ul><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Consumer)</span> <span class="title">GetMetadata</span><span class="params">(topic *<span class="keyword">string</span>, allTopics <span class="keyword">bool</span>, timeoutMs <span class="keyword">int</span>)</span> <span class="params">(*Metadata, error)</span></span> &#123;</span><br><span class="line"><span class="keyword">return</span> getMetadata(c, topic, allTopics, timeoutMs)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="QueryWatermarkOffsets-topic-string-partition-int32-timeoutMs-int-low-high-int64-err-error"><a href="#QueryWatermarkOffsets-topic-string-partition-int32-timeoutMs-int-low-high-int64-err-error" class="headerlink" title="QueryWatermarkOffsets(topic string, partition int32, timeoutMs int) (low, high int64, err error)"></a>QueryWatermarkOffsets(topic string, partition int32, timeoutMs int) (low, high int64, err error)</h4><ul><li>根据 topic 和 partition，查询当前 broker 中他们的低水位和高水位的 offset</li></ul><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Consumer)</span> <span class="title">QueryWatermarkOffsets</span><span class="params">(topic <span class="keyword">string</span>, partition <span class="keyword">int32</span>, timeoutMs <span class="keyword">int</span>)</span> <span class="params">(low, high <span class="keyword">int64</span>, err error)</span></span> &#123;</span><br><span class="line"><span class="keyword">return</span> queryWatermarkOffsets(c, topic, partition, timeoutMs)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="GetWatermarkOffsets-topic-string-partition-int32-low-high-int64-err-error"><a href="#GetWatermarkOffsets-topic-string-partition-int32-low-high-int64-err-error" class="headerlink" title="GetWatermarkOffsets(topic string, partition int32) (low, high int64, err error)"></a>GetWatermarkOffsets(topic string, partition int32) (low, high int64, err error)</h4><ul><li>根据 topic 和 partition 返回当前服务存储的低水位和高水位的 offset</li><li>每个 fetch 响应或通过调用 <code>QueryWatermarkOffsets</code> 填充高水位的 offset</li><li>如果设置了 <code>statistics.interval.ms</code>, 低水位将会有一个 <code>statistics.interval.ms</code> 的周期来更新</li></ul><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Consumer)</span> <span class="title">GetWatermarkOffsets</span><span class="params">(topic <span class="keyword">string</span>, partition <span class="keyword">int32</span>)</span> <span class="params">(low, high <span class="keyword">int64</span>, err error)</span></span> &#123;</span><br><span class="line"><span class="keyword">return</span> getWatermarkOffsets(c, topic, partition)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="OffsetsForTimes-times-TopicPartition-timeoutMs-int-offsets-TopicPartition-err-error"><a href="#OffsetsForTimes-times-TopicPartition-timeoutMs-int-offsets-TopicPartition-err-error" class="headerlink" title="OffsetsForTimes(times []TopicPartition, timeoutMs int) (offsets []TopicPartition, err error)"></a>OffsetsForTimes(times []TopicPartition, timeoutMs int) (offsets []TopicPartition, err error)</h4><ul><li>每个分区返回的偏移量是最早的偏移量，其时间戳大于或等于相应分区中的给定时间戳。如果提供的时间戳超过分区中最后一条消息的时间戳，则返回-1 值。</li></ul><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Consumer)</span> <span class="title">OffsetsForTimes</span><span class="params">(times []TopicPartition, timeoutMs <span class="keyword">int</span>)</span> <span class="params">(offsets []TopicPartition, err error)</span></span> &#123;</span><br><span class="line"><span class="keyword">return</span> offsetsForTimes(c, times, timeoutMs)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="Subscription-topics-string-err-error"><a href="#Subscription-topics-string-err-error" class="headerlink" title="Subscription() (topics []string, err error)"></a>Subscription() (topics []string, err error)</h4><ul><li>返回当前被订阅的 topic</li></ul><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Consumer)</span> <span class="title">Subscription</span><span class="params">()</span> <span class="params">(topics []<span class="keyword">string</span>, err error)</span></span> &#123;</span><br><span class="line"><span class="keyword">var</span> cTopics *C.rd_kafka_topic_partition_list_t</span><br><span class="line"></span><br><span class="line">cErr := C.rd_kafka_subscription(c.handle.rk, &amp;cTopics)</span><br><span class="line"><span class="keyword">if</span> cErr != C.RD_KAFKA_RESP_ERR_NO_ERROR &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, newError(cErr)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">defer</span> C.rd_kafka_topic_partition_list_destroy(cTopics)</span><br><span class="line"></span><br><span class="line">topicCnt := <span class="keyword">int</span>(cTopics.cnt)</span><br><span class="line">topics = <span class="built_in">make</span>([]<span class="keyword">string</span>, topicCnt)</span><br><span class="line"><span class="keyword">for</span> i := <span class="number">0</span>; i &lt; topicCnt; i++ &#123;</span><br><span class="line">crktpar := C._c_rdkafka_topic_partition_list_entry(cTopics,</span><br><span class="line">C.<span class="keyword">int</span>(i))</span><br><span class="line">topics[i] = C.GoString(crktpar.topic)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> topics, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="Assignment-partitions-TopicPartition-err-error"><a href="#Assignment-partitions-TopicPartition-err-error" class="headerlink" title="Assignment() (partitions []TopicPartition, err error)"></a>Assignment() (partitions []TopicPartition, err error)</h4><ul><li>返回当前指派的 partition</li></ul><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Consumer)</span> <span class="title">Assignment</span><span class="params">()</span> <span class="params">(partitions []TopicPartition, err error)</span></span> &#123;</span><br><span class="line"><span class="keyword">var</span> cParts *C.rd_kafka_topic_partition_list_t</span><br><span class="line"></span><br><span class="line">cErr := C.rd_kafka_assignment(c.handle.rk, &amp;cParts)</span><br><span class="line"><span class="keyword">if</span> cErr != C.RD_KAFKA_RESP_ERR_NO_ERROR &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, newError(cErr)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">defer</span> C.rd_kafka_topic_partition_list_destroy(cParts)</span><br><span class="line"></span><br><span class="line">partitions = newTopicPartitionsFromCparts(cParts)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> partitions, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="Committed-partitions-TopicPartition-timeoutMs-int-offsets-TopicPartition-err-error"><a href="#Committed-partitions-TopicPartition-timeoutMs-int-offsets-TopicPartition-err-error" class="headerlink" title="Committed(partitions []TopicPartition, timeoutMs int) (offsets []TopicPartition, err error)"></a>Committed(partitions []TopicPartition, timeoutMs int) (offsets []TopicPartition, err error)</h4><ul><li>查询已经提交 commit 的 offset</li></ul><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Consumer)</span> <span class="title">Committed</span><span class="params">(partitions []TopicPartition, timeoutMs <span class="keyword">int</span>)</span> <span class="params">(offsets []TopicPartition, err error)</span></span> &#123;</span><br><span class="line">cparts := newCPartsFromTopicPartitions(partitions)</span><br><span class="line"><span class="keyword">defer</span> C.rd_kafka_topic_partition_list_destroy(cparts)</span><br><span class="line">cerr := C.rd_kafka_committed(c.handle.rk, cparts, C.<span class="keyword">int</span>(timeoutMs))</span><br><span class="line"><span class="keyword">if</span> cerr != C.RD_KAFKA_RESP_ERR_NO_ERROR &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, newError(cerr)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> newTopicPartitionsFromCparts(cparts), <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="Position-partitions-TopicPartition-offsets-TopicPartition-err-error"><a href="#Position-partitions-TopicPartition-offsets-TopicPartition-err-error" class="headerlink" title="Position(partitions []TopicPartition) (offsets []TopicPartition, err error)"></a>Position(partitions []TopicPartition) (offsets []TopicPartition, err error)</h4><ul><li>根据 partition 返回其 offset</li><li>典型的用法是调用 assign()来获取分区列表，然后将其传递给 Position()来获取每个分区的当前 offset</li><li>消费的位置是分区读取的下一个消息，例如（最后一条信息的+1）</li></ul><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Consumer)</span> <span class="title">Position</span><span class="params">(partitions []TopicPartition)</span> <span class="params">(offsets []TopicPartition, err error)</span></span> &#123;</span><br><span class="line">cparts := newCPartsFromTopicPartitions(partitions)</span><br><span class="line"><span class="keyword">defer</span> C.rd_kafka_topic_partition_list_destroy(cparts)</span><br><span class="line">cerr := C.rd_kafka_position(c.handle.rk, cparts)</span><br><span class="line"><span class="keyword">if</span> cerr != C.RD_KAFKA_RESP_ERR_NO_ERROR &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, newError(cerr)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> newTopicPartitionsFromCparts(cparts), <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="Pause-partitions-TopicPartition-err-error"><a href="#Pause-partitions-TopicPartition-err-error" class="headerlink" title="Pause(partitions []TopicPartition) (err error)"></a>Pause(partitions []TopicPartition) (err error)</h4><ul><li>根据提供的 partition 暂停消费</li><li>如果设置了<code>go.events.channel.enable</code>，只会受到<code>go.events.channel.size</code>的影响，这个 API 将不会生效</li></ul><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Consumer)</span> <span class="title">Pause</span><span class="params">(partitions []TopicPartition)</span> <span class="params">(err error)</span></span> &#123;</span><br><span class="line">cparts := newCPartsFromTopicPartitions(partitions)</span><br><span class="line"><span class="keyword">defer</span> C.rd_kafka_topic_partition_list_destroy(cparts)</span><br><span class="line">cerr := C.rd_kafka_pause_partitions(c.handle.rk, cparts)</span><br><span class="line"><span class="keyword">if</span> cerr != C.RD_KAFKA_RESP_ERR_NO_ERROR &#123;</span><br><span class="line"><span class="keyword">return</span> newError(cerr)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="Resume-partitions-TopicPartition-err-error"><a href="#Resume-partitions-TopicPartition-err-error" class="headerlink" title="Resume(partitions []TopicPartition) (err error)"></a>Resume(partitions []TopicPartition) (err error)</h4><ul><li>唤醒被暂停的 partition</li></ul><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Consumer)</span> <span class="title">Resume</span><span class="params">(partitions []TopicPartition)</span> <span class="params">(err error)</span></span> &#123;</span><br><span class="line">cparts := newCPartsFromTopicPartitions(partitions)</span><br><span class="line"><span class="keyword">defer</span> C.rd_kafka_topic_partition_list_destroy(cparts)</span><br><span class="line">cerr := C.rd_kafka_resume_partitions(c.handle.rk, cparts)</span><br><span class="line"><span class="keyword">if</span> cerr != C.RD_KAFKA_RESP_ERR_NO_ERROR &#123;</span><br><span class="line"><span class="keyword">return</span> newError(cerr)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="GetConsumerGroupMetadata-ConsumerGroupMetadata-error"><a href="#GetConsumerGroupMetadata-ConsumerGroupMetadata-error" class="headerlink" title="GetConsumerGroupMetadata() (*ConsumerGroupMetadata, error)"></a>GetConsumerGroupMetadata() (*ConsumerGroupMetadata, error)</h4><ul><li>返回当前消费者组的元数据</li><li>这个返回的对象，应该传递给事务生产者的 SendOffsetsToTransaction() API</li></ul><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Consumer)</span> <span class="title">GetConsumerGroupMetadata</span><span class="params">()</span> <span class="params">(*ConsumerGroupMetadata, error)</span></span> &#123;</span><br><span class="line">cgmd := C.rd_kafka_consumer_group_metadata(c.handle.rk)</span><br><span class="line"><span class="keyword">if</span> cgmd == <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, NewError(ErrState, <span class="string">"Consumer group metadata not available"</span>, <span class="literal">false</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">defer</span> C.rd_kafka_consumer_group_metadata_destroy(cgmd)</span><br><span class="line"></span><br><span class="line">serialized, err := serializeConsumerGroupMetadata(cgmd)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> &amp;ConsumerGroupMetadata&#123;serialized&#125;, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;由于我们部门的一些大数据服务是用到 kafka 的，这个时期正值我们对 golang 语言对一个转型阶段，对比了一下开源对 kafka 客户端，决定使用 &lt;code&gt;confluent-kafka-go&lt;/code&gt;, 所以在这里记录一下&lt;code&gt;confluent-kafka-go&lt;/code&gt; 的一些内容&lt;/p&gt;
    
    </summary>
    
    
      <category term="Golang" scheme="http://blog.crazylaw.cn/categories/Golang/"/>
    
    
      <category term="Golang" scheme="http://blog.crazylaw.cn/tags/Golang/"/>
    
      <category term="Kafka" scheme="http://blog.crazylaw.cn/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>【Docker】如何选择hub.docker中的tag标签</title>
    <link href="http://blog.crazylaw.cn/2020/05/25/docker/docker-tags%E7%9A%84%E9%80%89%E6%8B%A9/"/>
    <id>http://blog.crazylaw.cn/2020/05/25/docker/docker-tags%E7%9A%84%E9%80%89%E6%8B%A9/</id>
    <published>2020-05-25T03:28:30.000Z</published>
    <updated>2021-03-20T16:25:01.805Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>很多小伙伴或许还不清楚各个tag的区别，其实最大的一个问题在于，你需要了解各大操作系统的不同。</p><a id="more"></a><h2 id="操作系统"><a href="#操作系统" class="headerlink" title="操作系统"></a>操作系统</h2><ul><li>debian</li><li>ubuntu</li><li>centos</li><li><a href="https://alpineliµnux.org/" target="_blank" rel="noopener">alpine</a></li></ul><h2 id="debian"><a href="#debian" class="headerlink" title="debian"></a>debian</h2><h3 id="Debian-发行版本"><a href="#Debian-发行版本" class="headerlink" title="Debian 发行版本"></a>Debian 发行版本</h3><p>Debian 一直维护着至少三个发行版本：稳定版（stable），测试版（testing）和不稳定版（unstable）。</p><h4 id="稳定版（stable）"><a href="#稳定版（stable）" class="headerlink" title="稳定版（stable）"></a>稳定版（stable）</h4><p>稳定版包含了 Debian 官方最近一次发行的软件包。</p><p>作为 Debian 的正式发行版本，它是我们优先推荐给用户您选用的版本。</p><p>当前 Debian 的稳定版版本号是 10，<code>开发代号为 buster</code>。最初版本为 10，于 2019年07月06日 发布，其更新 10.4 已于 2020年05月09日 发布。</p><h4 id="测试版（testing）"><a href="#测试版（testing）" class="headerlink" title="测试版（testing）"></a>测试版（testing）</h4><p>测试版包含了那些暂时未被收录进入稳定版的软件包，但它们已经进入了候选队列。使用这个版本的最大益处在于它拥有更多版本较新的软件。</p><p>想要了解 什么是测试版以及 如何成为稳定版的更多信息，请看 Debian FAQ。</p><p>当前的测试版版本代号是 <code>bullseye</code>。</p><h4 id="不稳定版（unstable）"><a href="#不稳定版（unstable）" class="headerlink" title="不稳定版（unstable）"></a>不稳定版（unstable）</h4><p>不稳定版存放了 Debian 现行的开发工作。通常，只有开发者和那些喜欢过惊险刺激生活的人选用该版本。推荐使用不稳定版的用户订阅 debian-devel-announce 邮件列表，以接收关于重大变更的通知，比如有可能导致问题的升级。</p><p>不稳定版的版本代号永远都被称为 <code>sid</code>。</p><ul><li>下一代 Debian 正式发行版的代号为 bullseye — 发布时间尚未确定</li><li>Debian 10（buster） — 当前的稳定版（stable）</li><li>Debian 9（stretch） — 旧的稳定版（oldstable）</li><li>Debian 8（jessie） — 更旧的稳定版（oldoldstable）</li><li>Debian 7（wheezy） — 被淘汰的稳定版</li><li>Debian 6.0（squeeze） — 被淘汰的稳定版</li><li>Debian GNU/Linux 5.0（lenny） — 被淘汰的稳定版</li><li>Debian GNU/Linux 4.0（etch） — 被淘汰的稳定版</li><li>Debian GNU/Linux 3.1（sarge） — 被淘汰的稳定版</li><li>Debian GNU/Linux 3.0（woody） — 被淘汰的稳定版</li><li>Debian GNU/Linux 2.2（potato） — 被淘汰的稳定版</li><li>Debian GNU/Linux 2.1（slink） — 被淘汰的稳定版</li><li>Debian GNU/Linux 2.0（hamm） — 被淘汰的稳定版</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;很多小伙伴或许还不清楚各个tag的区别，其实最大的一个问题在于，你需要了解各大操作系统的不同。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Docker" scheme="http://blog.crazylaw.cn/categories/Docker/"/>
    
    
      <category term="Docker" scheme="http://blog.crazylaw.cn/tags/Docker/"/>
    
  </entry>
  
</feed>
